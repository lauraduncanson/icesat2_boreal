{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minus-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host='api.ops.maap-project.org')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-contest",
   "metadata": {},
   "source": [
    "# Launch DPS for mapBoreal.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amazing-provider",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def local_to_s3(url, user='lduncanson'):\n",
    "    ''' A Function to convert local paths to s3 urls'''\n",
    "    return url.replace('/projects/my-private-bucket', f's3://maap-ops-workspace/{user}')\n",
    "def local_to_https(url, user='lduncanson'):\n",
    "    ''' A Function to convert local paths to https urls'''\n",
    "    return url.replace('/projects/my-private-bucket', f'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/{user}')\n",
    "def local_to_https_uswest2(url, user='lduncanson'):\n",
    "    ''' A Function to convert local paths to https us-west-s urls'''\n",
    "    return url.replace('/projects/my-private-bucket', f'https://maap-ops-workspace.s3.us-west-2.amazonaws.com/{user}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neural-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "atl08_filt_tindex_master =   pd.read_csv('s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_filt_tindex_master.csv')\n",
    "topo_tindex_master =         pd.read_csv('s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv')\n",
    "landsat_tindex_master =      pd.read_csv('s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv')\n",
    "\n",
    "# Convert al local_paths to s3\n",
    "#.. for data produced by 'lduncanson' workspace\n",
    "atl08_filt_tindex_master['https'] = [local_to_https_uswest2(local_path, user='lduncanson') for local_path in atl08_filt_tindex_master['local_path']]\n",
    "\n",
    "#.. for data produced by 'nathanmthomas' workspace\n",
    "for tindex_master in [topo_tindex_master, landsat_tindex_master]:\n",
    "    tindex_master['https'] = [local_to_https_uswest2(local_path, user='nathanmthomas') for local_path in tindex_master['local_path']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-celebrity",
   "metadata": {},
   "source": [
    "# Use the ATL08 filtered tindex master list to tell you which tiles you'll run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "following-procurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3968\n",
      "[42, 199, 200, 223, 224, 386, 387, 11, 49, 111, 112, 131]\n",
      "3940\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "INPUT_TILE_NUM_LIST = atl08_filt_tindex_master['tile_num'].values.astype(int).tolist()\n",
    "print(len(INPUT_TILE_NUM_LIST))\n",
    "\n",
    "# Remove duplicate tile nums\n",
    "INPUT_TILE_NUM_LIST = list(set(INPUT_TILE_NUM_LIST))\n",
    "\n",
    "# Filter to only tiles of interest\n",
    "TEST_INPUT_TILE_NUM_LIST = [42, 199, 200, 223, 224, 386, 387, 11, 49, 111, 112, 131]\n",
    "print(TEST_INPUT_TILE_NUM_LIST)\n",
    "print(len(INPUT_TILE_NUM_LIST))\n",
    "\n",
    "#for test, set\n",
    "INPUT_TILE_NUM_LIST = TEST_INPUT_TILE_NUM_LIST\n",
    "print(len(INPUT_TILE_NUM_LIST))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "announced-catch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/lduncanson/dps_output/run_tile_atl08_ubuntu/master/2021/10/11/23/50/48/232289/atl08_004_30m_filt_topo_landsat_20211011_0042.csv\n",
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/nathanmthomas/dps_output/do_topo_stack_3-1-5_ubuntu/ops/2021/09/15/18/18/44/208345/Copernicus_42_covars_cog_topo_stack.tif\n",
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/nathanmthomas/dps_output/do_landsat_stack_3-1-2_ubuntu/ops/2021/09/14/19/10/42/604491/Landsat8_42_comp_cog_2015-2020_dps.tif\n"
     ]
    }
   ],
   "source": [
    "# Check retrieval of s3 path with a tle_num\n",
    "in_atl08_https =  atl08_filt_tindex_master['https'].loc[atl08_filt_tindex_master['tile_num'] == INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "in_topo_https = topo_tindex_master['https'].loc[topo_tindex_master['tile_num'] == INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "in_landsat_https = landsat_tindex_master['https'].loc[landsat_tindex_master['tile_num'] == INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "print(in_atl08_https)\n",
    "print(in_topo_https)\n",
    "print(in_landsat_https)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-roommate",
   "metadata": {},
   "source": [
    "## Get files for boreal biomass models & boreal wide sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "through-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_models_https = 'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/lduncanson/my-private-bucket/bio_models.tar'\n",
    "\n",
    "train_data_https = 'https://maap-ops-workspace.s3.us-west-2.amazonaws.com/lduncanson/boreal_train_data_v3.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-alpha",
   "metadata": {},
   "source": [
    "## Run a DPS job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alpine-landscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPS run num: 1, tile num: 42, job info: {'status': 'success', 'http_status_code': 200, 'job_id': 'c5d82f82-141d-4fdb-bbc3-f3e5d0d4c6b6'}\n",
      "DPS job status: <Response [200]>\n",
      "DPS run num: 5, tile num: 224, job info: {'status': 'success', 'http_status_code': 200, 'job_id': '5f645b14-31f2-40a9-a4e8-63cae2e634fa'}\n",
      "DPS job status: <Response [200]>\n",
      "DPS run num: 10, tile num: 111, job info: {'status': 'success', 'http_status_code': 200, 'job_id': 'bf13176d-28f7-4d77-b542-ad496b930080'}\n",
      "DPS job status: <Response [200]>\n",
      "DPS run num: 12, tile num: 131, job info: {'status': 'success', 'http_status_code': 200, 'job_id': '46a7c255-614e-46a9-86ec-d9776ebc4474'}\n",
      "DPS job status: <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "RUN_DPS  = True\n",
    "\n",
    "if RUN_DPS:\n",
    "    ##################################\n",
    "    #Test DPS submission on a single file\n",
    "    #for i, INPUT_TILE_NUM in enumerate(INPUT_TILE_NUM_LIST):\n",
    "    for i, INPUT_TILE_NUM in enumerate(INPUT_TILE_NUM_LIST):\n",
    "        DPS_num = i+1\n",
    "        \n",
    "        # Get the s3 paths of the corresponding input filenames with an input tile_num\n",
    "        in_atl08_https =  atl08_filt_tindex_master['https'].loc[atl08_filt_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_topo_https = topo_tindex_master['https'].loc[topo_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_landsat_https = landsat_tindex_master['https'].loc[landsat_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        \n",
    "        #print(in_atl08_https) \n",
    "        #print(in_topo_https)\n",
    "        #print(in_landsat_https)\n",
    "        \n",
    "        if True:\n",
    "            in_param_dict = {\n",
    "                                    'in_atl08_fn': f\"input/{os.path.basename(in_atl08_https)}\",\n",
    "                                    'in_topo_fn': f\"input/{os.path.basename(in_topo_https)}\",\n",
    "                                    'in_landsat_fn': f\"input/{os.path.basename(in_landsat_https)}\",\n",
    "                                    'in_atl08_fn_url': in_atl08_https,\n",
    "                                    'in_topo_fn_url': in_topo_https,\n",
    "                                    'in_landsat_fn_url': in_landsat_https,\n",
    "                                    'DO_SLOPE_VALID_MASK': 'TRUE',\n",
    "                                    'in_atl08_sample_fn':f\"input/{os.path.basename(train_data_https)}\",\n",
    "                                    'in_atl08_sample_url':train_data_https,\n",
    "                                    'in_tile_num': INPUT_TILE_NUM,\n",
    "                                    'in_tile_fn_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_grid_albers90k_gpkg.gpkg',\n",
    "                                    'in_tile_fn': 'input/boreal_grid_albers90k_gpkg.gpkg'\n",
    "                }\n",
    "\n",
    "            submit_result = maap.submitJob(\n",
    "                    identifier='run_boreal_biomass',\n",
    "                    algo_id='run_boreal_biomass_ubuntu',\n",
    "                    version='master',\n",
    "                    username='lduncanson', # username needs to be the same as whoever created the workspace\n",
    "                    queue='maap-dps-worker-32gb',\n",
    "                    **in_param_dict\n",
    "                )\n",
    "\n",
    "            #submit_result = 'submit test'\n",
    "            if DPS_num in [1, 5, 10, 15, 20, 25, 50, 75, 100,200, 500,1000,2000, 3000,3500, 4000, len(INPUT_TILE_NUM_LIST)]:\n",
    "                print(f\"DPS run num: {DPS_num}, tile num: {INPUT_TILE_NUM}, job info: {submit_result}\") \n",
    "                print(f\"DPS job status: {maap.getJobStatus(submit_result.get('job_id')) }\" )\n",
    "        else:\n",
    "            print(f\"Tile num: {INPUT_TILE_NUM}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-heritage",
   "metadata": {},
   "source": [
    "## Get other lists just of missing tiles (various ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "scenic-prague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "need_tindex_master = pd.read_csv('/projects/my-public-bucket/DPS_tile_lists/Need_AGB_tindex_master.csv')\n",
    "print(len(need_tindex_master))\n",
    "\n",
    "INPUT_TILE_NUM_LIST = need_tindex_master.tile_num.tolist()\n",
    "\n",
    "# Remove duplicate tile nums\n",
    "INPUT_TILE_NUM_LIST = list(set(INPUT_TILE_NUM_LIST))\n",
    "\n",
    "print(len(INPUT_TILE_NUM_LIST))\n",
    "\n",
    "INPUT_TILE_NUM_LIST_NEED = INPUT_TILE_NUM_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affecting-happening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_tiles = [4304, 4305, 4221, 4220, 1785, 1718, 1720, 1661, 1257, 1318, 1317]\n",
    "werid_tiles = [1255, 1196, 949, 1062, 1063, 1005, 950, 1004]\n",
    "INPUT_TILE_NUM_LIST = weird_tiles\n",
    "len(INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "separate-psychology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "[978, 988, 978, 988, 1263, 1261, 1264, 794, 2932, 3190, 3012, 3014, 3360, 3017, 378, 765, 812, 821, 764, 861, 1302, 1308, 1469, 2894, 2883, 2965, 2976, 3327, 3321, 3510, 3509, 4372, 4440, 4477, 2994, 1406, 2840, 411, 380, 3335, 2495, 4403, 2906, 2907, 2814]\n"
     ]
    }
   ],
   "source": [
    "# Get all boreal tiles\n",
    "ATL08_filt_tindex_master =   pd.read_csv('s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_filt_tindex_master.csv')\n",
    "boreal_tile_index_path = '/projects/shared-buckets/nathanmthomas/boreal_grid_albers90k_gpkg.gpkg'\n",
    "boreal_tile_index = geopandas.read_file(boreal_tile_index_path)\n",
    "boreal_tile_index.rename(columns={\"layer\":\"tile_num\"}, inplace=True)\n",
    "boreal_tile_index[\"tile_num\"] = boreal_tile_index[\"tile_num\"].astype(int)\n",
    "\n",
    "bad_tiles = [3540,3634,3728,3823,3916,4004] #Dropping the tiles near antimeridian that reproject poorly.\n",
    "select_needs = [3360,2994,3190,2840,3012,3014,3017,2932,1261,1263,1264,988,978,794, 380,378,411,821,861,\n",
    "                812,765,764,1308,1302,1469,1406,2495,2883,2965,3321,3509,3510,3327,3335,2976,2906,2907,2894,2814,4253,4293,4403,4440,4408,4372,4477,3986]\n",
    "\n",
    "# Remove bad tiles\n",
    "boreal_tile_index = boreal_tile_index[~boreal_tile_index['tile_num'].isin(bad_tiles)]\n",
    "    \n",
    "#print(boreal_tile_index.head())\n",
    "tile_matches_select_needs = boreal_tile_index.merge(ATL08_filt_tindex_master[ATL08_filt_tindex_master['tile_num'].isin(select_needs)], how='right', on='tile_num')\n",
    "print(len(tile_matches_select_needs))\n",
    "select_needs_filt = print([t for t in tile_matches_select_needs.tile_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "coordinate-psychology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "4\n",
      "[3986 4253 4293 4408]\n",
      "70\n",
      "[  65  106  127  250  361  417  493  566  568  618  741  753  786  798\n",
      "  801  830  848  876  877  927 1273 1287 1318 1402 1438 1439 1440 1604\n",
      " 1848 1850 1852 1936 2425 2426 2952 3124 3355 3540 3542 3634 3728 3823\n",
      " 3845 3846 3895 3916 4004 4007 4018 4083 4176 4204 4250 4256 4307 4325\n",
      " 4342 4395 4417 4444 4450 4451 4463 4467 4468 4483 4499 4510 4527 4534]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select_needs_filt = [978, 988, 978, 988, 1263, 1261, 1264, 794, 2932, 3190, 3012, 3014, 3360, 3017, 378, 765, 812, 821, 764, 861, 1302, 1308, 1469, 2894, 2883, 2965, 2976, 3327, 3321, 3510, 3509, 4372, 4440, 4477, 2994, 1406, 2840, 411, 380, 3335, 2495, 4403, 2906, 2907, 2814]\n",
    "\n",
    "print(len(select_needs_filt))\n",
    "\n",
    "import numpy as np\n",
    "tile_nums_missing_but_wont_run = np.setdiff1d(select_needs, select_needs_filt)\n",
    "print(len(tile_nums_missing_but_wont_run))\n",
    "print(tile_nums_missing_but_wont_run)\n",
    "\n",
    "tile_nums_missing_periphery = np.setdiff1d(INPUT_TILE_NUM_LIST_NEED, select_needs_filt)\n",
    "print(len(tile_nums_missing_periphery))\n",
    "print(tile_nums_missing_periphery)\n",
    "INPUT_TILE_NUM_LIST = tile_nums_missing_periphery\n",
    "len(INPUT_TILE_NUM_LIST)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "muslim-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TILE_NUM_LIST = [4440,4372,4477]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
