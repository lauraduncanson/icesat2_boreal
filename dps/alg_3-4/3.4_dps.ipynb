{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "isolated-retirement",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py==3.1.0 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 3)) (3.1.0)\n",
      "Collecting pandas==1.2.2\n",
      "  Using cached pandas-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "Collecting pygeos==0.12.0\n",
      "  Using cached pygeos-0.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Requirement already satisfied: rtree==0.9.7 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 8)) (0.9.7)\n",
      "Requirement already satisfied: numpy==1.21.0 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 9)) (1.21.0)\n",
      "Requirement already satisfied: geopandas==0.9.0 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: rasterio==1.2.6 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (1.2.6)\n",
      "Collecting importlib_resources\n",
      "  Using cached importlib_resources-5.10.2-py3-none-any.whl (34 kB)\n",
      "Collecting contextily\n",
      "  Using cached contextily-1.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "Collecting s3fs==0.3.4\n",
      "  Using cached s3fs-0.3.4-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: rio-cogeo==2.3.1 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 22)) (2.3.1)\n",
      "Collecting rio-tiler==2.1.4\n",
      "  Using cached rio_tiler-2.1.4-py3-none-any.whl\n",
      "Collecting morecantile==2.1.4\n",
      "  Using cached morecantile-2.1.4-py3-none-any.whl\n",
      "Collecting pystac-client\n",
      "  Using cached pystac_client-0.5.1-py3-none-any.whl (29 kB)\n",
      "Collecting cachetools\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: cached-property in /projects/.local/lib/python3.7/site-packages (from h5py==3.1.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /projects/.local/lib/python3.7/site-packages (from pandas==1.2.2->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 4)) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.2.2->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: shapely>=1.6 in /projects/.local/lib/python3.7/site-packages (from geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (1.7.1)\n",
      "Requirement already satisfied: fiona>=1.8 in /projects/.local/lib/python3.7/site-packages (from geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (1.8.20)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in /projects/.local/lib/python3.7/site-packages (from geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (3.2.1)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (20.3.0)\n",
      "Requirement already satisfied: affine in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (2.3.0)\n",
      "Requirement already satisfied: click-plugins in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (1.1.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (2021.5.30)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (8.0.1)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (1.4.7)\n",
      "Requirement already satisfied: cligj>=0.5 in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (0.7.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (41.4.0)\n",
      "Requirement already satisfied: boto3>=1.9.91 in /opt/conda/lib/python3.7/site-packages/boto3-1.17.72-py3.7.egg (from s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (1.17.72)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.7/site-packages/botocore-1.20.72-py3.7.egg (from s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (1.20.72)\n",
      "Requirement already satisfied: pydantic in /projects/.local/lib/python3.7/site-packages (from rio-cogeo==2.3.1->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 22)) (1.8.2)\n",
      "Collecting pystac>=0.5.3\n",
      "  Using cached pystac-1.5.0-py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: numexpr in /projects/.local/lib/python3.7/site-packages (from rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.7.3)\n",
      "Requirement already satisfied: rio-color in /projects/.local/lib/python3.7/site-packages (from rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (1.0.4)\n",
      "Requirement already satisfied: requests in /projects/.local/lib/python3.7/site-packages (from rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.27.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib_resources->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 14)) (3.4.1)\n",
      "Collecting xyzservices\n",
      "  Using cached xyzservices-2022.9.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages/matplotlib-3.4.2-py3.7-linux-x86_64.egg (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (3.4.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages/Pillow-8.2.0-py3.7-linux-x86_64.egg (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (8.2.0)\n",
      "Requirement already satisfied: geopy in /projects/.local/lib/python3.7/site-packages (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (2.2.0)\n",
      "Requirement already satisfied: mercantile in /projects/.local/lib/python3.7/site-packages (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.2.1)\n",
      "Requirement already satisfied: joblib in /projects/.local/lib/python3.7/site-packages (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.0.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages/jmespath-0.10.0-py3.7.egg (from boto3>=1.9.91->s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages/s3transfer-0.4.2-py3.7.egg (from boto3>=1.9.91->s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (0.4.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /projects/.local/lib/python3.7/site-packages (from botocore>=1.12.91->s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (1.26.7)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click>=4.0->rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (3.7.3)\n",
      "Requirement already satisfied: munch in /projects/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from pystac>=0.5.3->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /projects/.local/lib/python3.7/site-packages (from requests->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.0.10)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (2.4.7)\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in /projects/.local/lib/python3.7/site-packages (from geopy->contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.52)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages/cycler-0.10.0-py3.7.egg (from matplotlib->contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages/kiwisolver-1.3.1-py3.7-linux-x86_64.egg (from matplotlib->contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: rio-mucho in /projects/.local/lib/python3.7/site-packages (from rio-color->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (1.0.0)\n",
      "Installing collected packages: xyzservices, pygeos, importlib_resources, fsspec, cachetools, pystac, pandas, pystac-client, s3fs, morecantile, contextily, rio-tiler\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.4\n",
      "    Uninstalling pandas-1.1.4:\n",
      "      Successfully uninstalled pandas-1.1.4\n",
      "Successfully installed cachetools-5.3.0 contextily-1.2.0 fsspec-2023.1.0 importlib_resources-5.10.2 morecantile-2.1.4 pandas-1.2.2 pygeos-0.12.0 pystac-1.5.0 pystac-client-0.5.1 rio-tiler-2.1.4 s3fs-0.3.4 xyzservices-2022.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host='api.ops.maap-project.org')\n",
    "!pip install -U -r /projects/icesat2_boreal/dps/requirements_main.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-differential",
   "metadata": {},
   "source": [
    "# Launch DPS for mapBoreal.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effective-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "!pip install xmltodict\n",
    "import xmltodict\n",
    "\n",
    "def local_to_s3(url, user='lduncanson'):\n",
    "    ''' A Function to convert local paths to s3 urls'''\n",
    "    return url.replace('/projects/my-private-bucket', f's3://maap-ops-workspace/{user}')\n",
    "def local_to_https(url, user='lduncanson'):\n",
    "    ''' A Function to convert local paths to https urls'''\n",
    "    return url.replace('/projects/my-private-bucket', f'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/{user}')\n",
    "def local_to_https_uswest2(url, user='lduncanson'):\n",
    "    ''' A Function to convert local paths to https us-west-s urls'''\n",
    "    return url.replace('/projects/my-private-bucket', f'https://maap-ops-workspace.s3.us-west-2.amazonaws.com/{user}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "failing-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "#atl08_filt_tindex_master =   pd.read_csv('s3://maap-ops-workspace/lduncanson/dps_output/run_tile_atl08_ubuntu/master/2022/03/ATL08_filt_tindex_master.csv')\n",
    "\n",
    "#atl08_filt_tindex_master =   pd.read_csv('s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/ATL08_filt_tindex_master.csv')\n",
    "#my-public-bucket/DPS_tile_lists/fall2022/ATL08_filt_tindex_master.csv\n",
    "\n",
    "#note - check that the atl08 version matched ground = with_gedi_rh noground = with_atl03_rh\n",
    "atl08_filt_tindex_master =   pd.read_csv('/projects/shared-buckets/lduncanson/DPS_tile_lists/fall2022/with_atl03_rh/ATL08_filt_tindex_master.csv')\n",
    "topo_tindex_master =         pd.read_csv('/projects/shared-buckets/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv')\n",
    "landsat_tindex_master =      pd.read_csv('/projects/shared-buckets/nathanmthomas/DPS_tile_lists/HLS/c2020/HLS_stack_2022_v2/HLS_tindex_master.csv')\n",
    "lc_tindex_master = pd.read_csv('/projects/shared-buckets/nathanmthomas/DPS_tile_lists/LC/LC_tindex_master.csv')\n",
    "\n",
    "\n",
    "# Convert al local_paths to s3\n",
    "#.. for data produced by 'lduncanson' workspace\n",
    "atl08_filt_tindex_master['https'] = [local_to_https_uswest2(local_path, user='lduncanson') for local_path in atl08_filt_tindex_master['local_path']]\n",
    "\n",
    "#.. for data produced by 'nathanmthomas' workspace\n",
    "for tindex_master in [topo_tindex_master, landsat_tindex_master, lc_tindex_master]:\n",
    "    tindex_master['https'] = [local_to_https_uswest2(local_path, user='nathanmthomas') for local_path in tindex_master['local_path']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-waterproof",
   "metadata": {},
   "source": [
    "# Use the ATL08 filtered tindex master list to tell you which tiles you'll run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nervous-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "4880\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NORWAY_TILE_LIST = pd.read_csv('/projects/my-public-bucket/misc_files/norway_tiles.csv').layer.tolist()\n",
    "\n",
    "#NORWAY_TILE_LIST = [42,199,200,223,224,386,387,11,49,111,112,131]\n",
    "\n",
    "DELTA_TILE_LIST = [3459]\n",
    "\n",
    "BONA_TILE_LIST  = [3364,3365]\n",
    "\n",
    "HEALY_TILE_LIST = [3552]\n",
    "\n",
    "INPUT_EXPERIMENT_TILE_NUM_LIST = NORWAY_TILE_LIST + DELTA_TILE_LIST + BONA_TILE_LIST + HEALY_TILE_LIST\n",
    "#INPUT_EXPERIMENT_TILE_NUM_LIST = NORWAY_TILE_LIST\n",
    "#DPS_INPUT_TILE_NUM_LIST = INPUT_EXPERIMENT_TILE_NUM_LIST\n",
    "#len(DPS_INPUT_TILE_NUM_LIST)\n",
    "\n",
    "#ALASKA_TILE_LIST =  list(range(3268,3272+1))+\\\n",
    "#                    list(range(3361,3366+1))+\\\n",
    "#                    list(range(3454,3459+1))+\\\n",
    "#                    list(range(3549,3555+1))+\\\n",
    "#                    list(range(3643,3648+1))\n",
    "\n",
    "print(len(INPUT_EXPERIMENT_TILE_NUM_LIST))\n",
    "DPS_INPUT_TILE_NUM_LIST = INPUT_EXPERIMENT_TILE_NUM_LIST\n",
    "\n",
    "FULL_TILE_LIST = atl08_filt_tindex_master['tile_num']\n",
    "\n",
    "DPS_INPUT_TILE_NUM_LIST = [639, 640, 726, 727, 373, 405]\n",
    "\n",
    "DPS_INPUT_TILE_NUM_LIST = FULL_TILE_LIST\n",
    "DPS_INPUT_TILE_NUM_LIST = [3082,2918, 1854, 1789, 1262,34210, 33975,37,148,169,446,162,629,1761, 2651, 2970, 3604, 4080]\n",
    "DPS_INPUT_TILE_NUM_LIST = [1050, 1105, 34210, 4245, 24959, 23828]\n",
    "DPS_INPUT_TILE_NUM_LIST = INPUT_EXPERIMENT_TILE_NUM_LIST\n",
    "\n",
    "DPS_INPUT_TILE_NUM_LIST = FULL_TILE_LIST\n",
    "\n",
    "\n",
    "print(len(DPS_INPUT_TILE_NUM_LIST))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "invalid-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/lduncanson/dps_output/run_tile_atl08_ubuntu/tile_atl08/2022/12/01/13/59/41/965091/atl08_005_30m_filt_topo_landsat_20221201_1920.csv\n",
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/nathanmthomas/dps_output/do_topo_stack_3-1-5_ubuntu/ops/2021/07/23/22/00/04/859421/Copernicus_1920_covars_cog_topo_stack.tif\n",
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/nathanmthomas/dps_output/do_HLS_stack_3-1-2_ubuntu/HLS_stack_2022_v2/2022/10/18/13/04/49/382281/HLS_1920_06-15_09-01_2019_2021.tif\n",
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/nathanmthomas/dps_output/run_build_stack_ubuntu/master/2022/09/15/16/53/38/462447/esa_worldcover_v100_2020_1920_cog.tif\n"
     ]
    }
   ],
   "source": [
    "# Check retrieval of s3 path with a tle_num\n",
    "in_atl08_https =  atl08_filt_tindex_master['https'].loc[atl08_filt_tindex_master['tile_num'] == DPS_INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "in_topo_https = topo_tindex_master['https'].loc[topo_tindex_master['tile_num'] == DPS_INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "in_landsat_https = landsat_tindex_master['https'].loc[landsat_tindex_master['tile_num'] == DPS_INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "in_lc_https = lc_tindex_master['https'].loc[lc_tindex_master['tile_num'] == DPS_INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "\n",
    "print(in_atl08_https)\n",
    "print(in_topo_https)\n",
    "print(in_landsat_https)\n",
    "print(in_lc_https)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-crisis",
   "metadata": {},
   "source": [
    "## Get files for boreal biomass models & boreal wide sample\n",
    "Note that this no longer sends the bio models to the job - this is embedded in the .sh run script (not in the param dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "painted-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_models_https = 'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/lduncanson/my-private-bucket/bio_models_noground.tar'\n",
    "\n",
    "train_data_https = 'https://maap-ops-workspace.s3.us-west-2.amazonaws.com/lduncanson/boreal_train_data_v10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alert-example",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/lduncanson/my-private-bucket/bio_models_noground.tar'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_models_https"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-simulation",
   "metadata": {},
   "source": [
    "## Test from Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "packed-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put latest test data in a test folder (outside repo)\n",
    "# Terminal:\n",
    "                           \n",
    "# file 1: /projects/testing/input/atl08_004_30m_filt_topo_landsat_20220208_0042.csv\n",
    "# file 2: /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_42_covars_cog_topo_stack.tif\n",
    "# file 3: /projects/shared-buckets/nathanmthomas/alg_34_testing/Landsat8_42_comp_cog_2015-2020_dps.tif\n",
    "#file 4: /projects/my-private-bucket/boreal_train_data_v3.csv\n",
    "#file 5: /projects/my-private-bucket/bio_models.tar\n",
    "#file 6: 42\n",
    "# file 7:\n",
    "\n",
    "# ./run_boreal_biomass.sh file1 file2 file3 file4 file5 file6 file7\n",
    "#ATL08_CSV= /projects/testing/input/atl08_005_30m_filt_topo_landsat_20220322_0042.csv\n",
    "#TOPO_TIF=/projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_42_covars_cog_topo_stack.tif\n",
    "#LANDSAT_TIF= /projects/shared-buckets/nathanmthomas/alg_34_testing/Landsat8_42_comp_cog_2015-2020_dps.tif\n",
    "#DO_SLOPE_VALID_MASK= 'TRUE'\n",
    "#ATL08_SAMPLE_CSV=/projects/my-private-bucket/boreal_train_data_v3.csv\n",
    "#in_tile_num=42\n",
    "#in_tile_fn=/projects/shared-buckets/nathanmthomas/boreal_grid_albers90k_gpkg.gpkg\n",
    "\n",
    "#python /projects/icesat2_boreal/lib/merge_neighbors_atl08.py -in_tile_num 42 -in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg -in_tile_field 'tile_num' -csv_list_fn s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_filt_tindex_master.csv -out_dir /projects/testing/output/\n",
    "#echo $cmd\n",
    "#eval $cmd\n",
    "\n",
    "# Set the output merged CSV name to a var\n",
    "#MERGED_ATL08_CSV=$(ls ${OUTPUTDIR}/atl08_005_30m_filt_merge_neighbors* | head -1)\n",
    "\n",
    "# Run mapBoreal with merged CSV as input\n",
    "#conda activate r-with-gdal\n",
    "\n",
    "\n",
    "#Rscript /projects/icesat2_boreal/lib/mapBoreal.R /projects/testing/output/atl08_004_30m_filt_merge_neighbors_0042.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_42_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_42_06-01_09-15_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v4.csv 2 2 100 300 5 TRUE 50 3000 \n",
    "\n",
    "#test swapping for terra\n",
    "#Rscript /projects/icesat2_boreal/lib/mapBoreal_speedy.R /projects/testing/output/atl08_004_30m_filt_merge_neighbors_0042.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_42_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_42_06-01_09-15_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v4.csv 3 2 100 300 5 TRUE 100 3000 /projects/shared-buckets/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson \n",
    "\n",
    "# of tiles to run: 115\n",
    "# [3585, 4108, 1037, 28176, 4112, 31762, 1050, 2075, 28702, 31263, 29223, 29739, 30252, 3117, 30763, 3120, 2614, 2618, 2619, 1615, 1105, 1617, 2142, 612, 2148, 2149, 2151, 2152, 2153, 1146, 1662, 1159, 1672, 2698, 34443, 26280, 2217, 2223, 2224, 33975, 1721, 3770, 3771, 1214, 1731, 1733, 26827, 2779, 3301, 3302, 27368, 745, 3307, 1273, 2301, 27908, 1798, 1799, 1800, 3861, 28440, 3865, 31513, 28963, 31014, 29481, 2859, 29996, 30508, 1334, 4411, 3393, 3395, 2375, 3399, 1865, 1866, 1867, 2378, 2379, 2380, 2381, 3400, 3951, 3952, 34674, 3954, 2942, 895, 2944, 1933, 1934, 1937, 1938, 2457, 3486, 34210, 931, 1448, 26554, 4034, 4035, 1999, 981, 3029, 3030, 2009, 2010, 2011, 27098, 991, 992, 993, 2533, 27638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "formed-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of tile 1927\n",
    "#ATL08_CSV= /projects/testing/input/atl08_005_30m_filt_topo_landsat_20220418_1927.csv\n",
    "#TOPO_TIF=/projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_1927_covars_cog_topo_stack.tif \n",
    "#LANDSAT_TIF= /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_1927_06-15_09-01_2019_2021.tif\n",
    "#DO_SLOPE_VALID_MASK= 'TRUE'\n",
    "#ATL08_SAMPLE_CSV=/projects/my-private-bucket/boreal_train_data_v4.csv\n",
    "#in_tile_num=42\n",
    "#in_tile_fn=/projects/shared-buckets/nathanmthomas/boreal_grid_albers90k_gpkg.gpkg\n",
    "\n",
    "#python /projects/icesat2_boreal/lib/merge_neighbors_atl08.py -in_tile_num 1927 -in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg -in_tile_field 'tile_num' -csv_list_fn /projects/my-public-bucket/DPS_tile_lists/ATL08_filt_tindex_master.csv -out_dir /projects/testing/output/\n",
    "#echo $cmd\n",
    "#eval $cmd\n",
    "\n",
    "# Set the output merged CSV name to a var\n",
    "#MERGED_ATL08_CSV=$(ls ${OUTPUTDIR}/atl08_005_30m_filt_merge_neighbors* | head -1)\n",
    "\n",
    "# Run mapBoreal with merged CSV as input\n",
    "#conda activate r-with-gdal\n",
    "\n",
    "\n",
    "#Rscript /projects/icesat2_boreal/lib/mapBoreal.R /projects/testing/output/atl08_004_30m_filt_merge_neighbors_0042.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_42_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_726_06-15_09-01_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v5.csv 2 2 100 300 5 TRUE 50 3000 \n",
    "\n",
    "#test swapping for terra\n",
    "#Rscript /projects/icesat2_boreal/lib/mapBoreal_speedy.R /projects/shared-buckets/nathanmthomas/atl08_004_30m_filt_merge_neighbors_3885.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_3585_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_727_06-15_09-01_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v5.csv 2 2 100 300 5 TRUE 100 3000 /projects/shared-buckets/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "coupled-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on tile 727 and 726\n",
    "#python /projects/icesat2_boreal/lib/merge_neighbors_atl08.py -in_tile_num 727 -in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg -in_tile_field 'tile_num' -csv_list_fn s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_filt_tindex_master.csv -out_dir /projects/testing/output/\n",
    "       \n",
    "#python /projects/icesat2_boreal/lib/merge_neighbors_atl08.py -in_tile_num 726 -in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg -in_tile_field 'tile_num' -csv_list_fn s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_filt_tindex_master.csv -out_dir /projects/testing/output/    \n",
    "\n",
    "## Rscript /projects/icesat2_boreal/lib/mapBoreal_speedy.R /projects/testing/output/atl08_004_30m_filt_merge_neighbors_0726.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_726_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_726_06-15_09-01_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v7.csv 2 2 130 250 5 TRUE 50 5000 /projects/shared-buckets/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson \n",
    "\n",
    "## Rscript /projects/icesat2_boreal/lib/mapBoreal_speedy.R /projects/testing/output/atl08_004_30m_filt_merge_neighbors_0727.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_727_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_727_06-15_09-01_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v7.csv 2 2 130 250 5 TRUE 50 5000 /projects/shared-buckets/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-albuquerque",
   "metadata": {},
   "source": [
    "## Commit with Tag for running\n",
    "1) Add version name as a map_boreal_2022_v2 or whatever is appropriate - both to this notebook and algorithm config yaml\n",
    "\n",
    "2) follow git instructions (every time!!):\n",
    "git add changes\n",
    "git commit -m 'message'\n",
    "git tag -f map_boreal_2022_rh_noground_v2\n",
    "\n",
    "git push\n",
    "git push origin -f map_boreal_2022_rh_noground_v2\n",
    "git push dps\n",
    "git push dps -f map_boreal_2022_rh_noground_v2\n",
    "\n",
    "3) if it looks weird check git log to make sure tag is at same place as origin and dps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-greeting",
   "metadata": {},
   "source": [
    "## register via terminal\n",
    "python /projects/register-algorithm.py /projects/icesat2_boreal/dps/alg_3-4/algorithm_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-implement",
   "metadata": {},
   "source": [
    "## Run a DPS job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "clean-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input tiles for DPS: 212\n",
      "DPS run #: 1\t| tile num: 3592\t| submit status: success\t| job id: 1e3f7a73-20dd-4323-a385-65a0a6d458c2\n",
      "DPS run #: 2\t| tile num: 521\t| submit status: success\t| job id: 48e3d1b1-3272-49cb-999e-a4466de51f14\n",
      "DPS run #: 25\t| tile num: 3117\t| submit status: success\t| job id: 4e2ac8eb-fff8-48f2-a972-225fd04d51a5\n",
      "DPS run #: 50\t| tile num: 1119\t| submit status: success\t| job id: ac0b4ea5-6290-476f-95bb-24854cc5ea4d\n",
      "DPS run #: 100\t| tile num: 4307\t| submit status: success\t| job id: 386a5a75-d6a1-42be-b91e-8b006ea09efd\n",
      "Current time:\t202301270356\n",
      "CPU times: user 2.68 s, sys: 156 ms, total: 2.84 s\n",
      "Wall time: 59 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>http_status_code</th>\n",
       "      <th>job_id</th>\n",
       "      <th>dps_num</th>\n",
       "      <th>tile_num</th>\n",
       "      <th>submit_time</th>\n",
       "      <th>dbs_job_hour</th>\n",
       "      <th>algo_id</th>\n",
       "      <th>user</th>\n",
       "      <th>worker_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>1e3f7a73-20dd-4323-a385-65a0a6d458c2</td>\n",
       "      <td>1</td>\n",
       "      <td>3592</td>\n",
       "      <td>2023-01-27 03:55:07.363980</td>\n",
       "      <td>3</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-32gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>48e3d1b1-3272-49cb-999e-a4466de51f14</td>\n",
       "      <td>2</td>\n",
       "      <td>521</td>\n",
       "      <td>2023-01-27 03:55:07.429165</td>\n",
       "      <td>3</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-32gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>b2fd6417-74b8-4969-b4a8-c2e559deb7b9</td>\n",
       "      <td>3</td>\n",
       "      <td>2569</td>\n",
       "      <td>2023-01-27 03:55:07.592840</td>\n",
       "      <td>3</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-32gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>f185310f-b28f-4d2d-b8d3-04c6e61b9ee7</td>\n",
       "      <td>4</td>\n",
       "      <td>2570</td>\n",
       "      <td>2023-01-27 03:55:07.765817</td>\n",
       "      <td>3</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-32gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>128c7c03-6b3b-4270-b0dc-2484350318fe</td>\n",
       "      <td>5</td>\n",
       "      <td>3594</td>\n",
       "      <td>2023-01-27 03:55:08.064855</td>\n",
       "      <td>3</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-32gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>392b0f9d-61d4-4d86-9377-5f8d284f5daf</td>\n",
       "      <td>208</td>\n",
       "      <td>4083</td>\n",
       "      <td>2023-01-27 03:56:05.019606</td>\n",
       "      <td>3</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-32gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>a074c93a-2057-4ffe-8ef1-96d80fcddbfd</td>\n",
       "      <td>209</td>\n",
       "      <td>3574</td>\n",
       "      <td>2023-01-27 03:56:05.252870</td>\n",
       "      <td>3</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-32gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>35b53458-3632-40c5-8e9d-81f088084d05</td>\n",
       "      <td>210</td>\n",
       "      <td>2045</td>\n",
       "      <td>2023-01-27 03:56:05.478843</td>\n",
       "      <td>3</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-32gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>a7e5d515-a898-4c6a-9751-b2f0a9dccda2</td>\n",
       "      <td>211</td>\n",
       "      <td>2558</td>\n",
       "      <td>2023-01-27 03:56:05.705842</td>\n",
       "      <td>3</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-32gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>15b5aa5c-9d90-40df-84ad-fe2085bff55c</td>\n",
       "      <td>212</td>\n",
       "      <td>511</td>\n",
       "      <td>2023-01-27 03:56:05.898172</td>\n",
       "      <td>3</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-32gb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     status  http_status_code                                job_id  dps_num  \\\n",
       "0   success               200  1e3f7a73-20dd-4323-a385-65a0a6d458c2        1   \n",
       "0   success               200  48e3d1b1-3272-49cb-999e-a4466de51f14        2   \n",
       "0   success               200  b2fd6417-74b8-4969-b4a8-c2e559deb7b9        3   \n",
       "0   success               200  f185310f-b28f-4d2d-b8d3-04c6e61b9ee7        4   \n",
       "0   success               200  128c7c03-6b3b-4270-b0dc-2484350318fe        5   \n",
       "..      ...               ...                                   ...      ...   \n",
       "0   success               200  392b0f9d-61d4-4d86-9377-5f8d284f5daf      208   \n",
       "0   success               200  a074c93a-2057-4ffe-8ef1-96d80fcddbfd      209   \n",
       "0   success               200  35b53458-3632-40c5-8e9d-81f088084d05      210   \n",
       "0   success               200  a7e5d515-a898-4c6a-9751-b2f0a9dccda2      211   \n",
       "0   success               200  15b5aa5c-9d90-40df-84ad-fe2085bff55c      212   \n",
       "\n",
       "    tile_num                submit_time  dbs_job_hour  \\\n",
       "0       3592 2023-01-27 03:55:07.363980             3   \n",
       "0        521 2023-01-27 03:55:07.429165             3   \n",
       "0       2569 2023-01-27 03:55:07.592840             3   \n",
       "0       2570 2023-01-27 03:55:07.765817             3   \n",
       "0       3594 2023-01-27 03:55:08.064855             3   \n",
       "..       ...                        ...           ...   \n",
       "0       4083 2023-01-27 03:56:05.019606             3   \n",
       "0       3574 2023-01-27 03:56:05.252870             3   \n",
       "0       2045 2023-01-27 03:56:05.478843             3   \n",
       "0       2558 2023-01-27 03:56:05.705842             3   \n",
       "0        511 2023-01-27 03:56:05.898172             3   \n",
       "\n",
       "                               algo_id        user           worker_type  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-32gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-32gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-32gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-32gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-32gb  \n",
       "..                                 ...         ...                   ...  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-32gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-32gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-32gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-32gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-32gb  \n",
       "\n",
       "[212 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(f\"# of input tiles for DPS: {len_input_list}\")\n",
    "\n",
    "for i, INPUT_TILE_NUM in enumerate(DPS_INPUT_TILE_NUM_LIST):\n",
    "\n",
    "    DPS_num = i+1\n",
    "    IDENTIFIER = 'run_boreal_biomass_quick_v2'\n",
    "    ALGO_ID = f'{IDENTIFIER}_ubuntu'\n",
    "    USER = 'lduncanson'\n",
    "    WORKER_TYPE = 'maap-dps-worker-32gb'\n",
    "    \n",
    "    # Maybe we're using an input tile list that isnt built directly from an ATL08_filt CSV\n",
    "    # so, check if we have filtered ATL08 for this tile:\n",
    "    if INPUT_TILE_NUM in atl08_filt_tindex_master['tile_num'].to_list():\n",
    "        \n",
    "        # Get the s3 paths of the corresponding input filenames with an input tile_num\n",
    "        in_atl08_https =  atl08_filt_tindex_master['https'].loc[atl08_filt_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_topo_https = topo_tindex_master['https'].loc[topo_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_landsat_https = landsat_tindex_master['https'].loc[landsat_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_lc_https = lc_tindex_master['https'].loc[lc_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    " \n",
    "    else:\n",
    "        print(f\"No filtered ATL08 for tile {INPUT_TILE_NUM}. Moving on..\" )\n",
    "        continue\n",
    "    \n",
    "    # iters is the number of iterations of RH model fits; For no uncertainties set iters = 1 for quick mapping / testing.\n",
    "    # anything >1 will be exhaustive until uncertainties constrain or iters becomes 250. Will take a long time.\n",
    "        \n",
    "    # ppside is the number of subtile splits. 2=4 subtiles, 3=9 subtiles\n",
    "    # increasing ppside reduces memory, increases length to completion\n",
    "    \n",
    "    # minDOY is the min possible DOY to search for tile training data. The default is set to May 1\n",
    "    # If sufficient data are found in growing season, minDOY will not be used\n",
    "    \n",
    "    # maxDOY is the max possible DOY to search for tile training data. The default is Sept 30.\n",
    "    \n",
    "    # max_sol_el is the max solar elevation possible to search. The default is 0\n",
    "    \n",
    "    # expand_training as TRUE sets iterative searching for training data up to min_n, starting with solar_el<0 and growing season and then:\n",
    "    # searches solar_el up to max_sol_el then\n",
    "    # searches iteratively one month later after growing season until maxDOY then\n",
    "    # searches iteratively one month earlier before growing season until minDOY\n",
    "    # if expand_training=FALSE the growing season sol_el<0 will be taken, and if none, no data will return\n",
    "    \n",
    "    # min_n is the tile training number desired, that expand_training will search until it is fulfilled (or the max available)\n",
    "    \n",
    "    #local_train_perc is the percent tile training data vs. boreal-wide training data. 100 = all local, 0 = all boreal wide\n",
    "        \n",
    "    in_param_dict = {\n",
    "                                    'in_atl08_fn': 's3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/fall2022/with_atl03_rh/ATL08_filt_tindex_master.csv',\n",
    "                                    #'in_atl08_fn': f\"input/{os.path.basename(in_atl08_https)}\",\n",
    "                                    'in_topo_fn': f\"input/{os.path.basename(in_topo_https)}\",\n",
    "                                    'in_landsat_fn': f\"input/{os.path.basename(in_landsat_https)}\",\n",
    "                                    'in_lc_fn': f\"input/{os.path.basename(in_lc_https)}\",\n",
    "                                    'in_atl08_fn_url': in_atl08_https,\n",
    "                                    'in_topo_fn_url': in_topo_https,\n",
    "                                    'in_landsat_fn_url': in_landsat_https,\n",
    "                                    'in_lc_fn_url': in_lc_https,\n",
    "                                    'DO_SLOPE_VALID_MASK': 'TRUE',\n",
    "                                    'in_atl08_sample_fn':f\"input/{os.path.basename(train_data_https)}\",\n",
    "                                    'in_atl08_sample_url':train_data_https,\n",
    "                                    'in_tile_num': INPUT_TILE_NUM,\n",
    "                                    'in_tile_fn_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "                                    'in_tile_fn': 'input/boreal_tiles_v003.gpkg',\n",
    "                                    'in_tile_field': 'tile_num',\n",
    "                                    'iters': 30,\n",
    "                                    'ppside': 2,\n",
    "                                    'minDOY':130,\n",
    "                                    'maxDOY':250,\n",
    "                                    'max_sol_el':5,\n",
    "                                    'expand_training': 'TRUE',\n",
    "                                    'local_train_perc': 100,\n",
    "                                    'min_n': 5000, \n",
    "                                    'boreal_vect': 'https://maap-ops-workspace.s3.us-west-2.amazonaws.com/shared/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson',\n",
    "                                    'boreal_vect_fn': 'input/wwf_circumboreal_Dissolve_reprj.geojson',\n",
    "                                    'predict_var': 'AGB'\n",
    "                }\n",
    "    \n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version='map_boreal_2022_rh_noground_v2',#, ht_boreal_2022_rh_noground_v1 ,'map_boreal_2022_rh_ground_v1','map_boreal_2022_rh_noground_v1',\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_param_dict\n",
    "        )\n",
    "\n",
    "    # Build a dataframe of submission details\n",
    "    submit_result['dps_num'] = DPS_num\n",
    "    submit_result['tile_num'] = INPUT_TILE_NUM\n",
    "    submit_result['submit_time'] = datetime.datetime.now()\n",
    "    submit_result['dbs_job_hour'] =datetime.datetime.now().hour\n",
    "    submit_result['algo_id'] = ALGO_ID\n",
    "    submit_result['user'] = USER\n",
    "    submit_result['worker_type'] = WORKER_TYPE\n",
    "\n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(pd.DataFrame([submit_result]))\n",
    "    \n",
    "    if DPS_num in [1, 2, 25, 50, 100, 500, 1000, 1500, 2000, 3000, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| tile num: {INPUT_TILE_NUM}\\t| submit status: {submit_result['status']}\\t| job id: {submit_result['job_id']}\") \n",
    "        #local_path\n",
    "\n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "print(f\"Current time:\\t{nowtime}\")\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/DPS_{IDENTIFIER}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-evidence",
   "metadata": {},
   "source": [
    "## Assess DPS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "directed-economy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count total jobs:\t212\n",
      "Count pending jobs:\t0\n",
      "Count running jobs:\t31\n",
      "Count succeeded jobs:\t130\n",
      "Count failed jobs:\t51\n",
      "% of failed jobs:\t28.18\n",
      "CPU times: user 2.23 s, sys: 114 ms, total: 2.34 s\n",
      "Wall time: 5.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#submit_results_df = pd.read_csv('/projects/my-public-bucket/DPS_run_boreal_biomass_quick_v2_submission_results_232_202301260950.csv')\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "\n",
    "def BUILD_TABLE_JOBSTATUS(submit_results_df):\n",
    "    import xmltodict\n",
    "    df = pd.concat([pd.DataFrame(xmltodict.parse(maap.getJobStatus(job_id).content)).transpose() for job_id in submit_results_df.job_id.to_list()])\n",
    "    return df\n",
    "\n",
    "job_status_df = BUILD_TABLE_JOBSTATUS(submit_results_df)\n",
    "#print(job_status_df.head())\n",
    "\n",
    "num_jobs = submit_results_df.shape[0]\n",
    "z = submit_results_df.merge(job_status_df, how='left', left_on='job_id',  right_on='wps:JobID')\n",
    "\n",
    "print(f'Count total jobs:\\t{num_jobs}')\n",
    "print(f\"Count pending jobs:\\t{z[z['wps:Status'] =='Accepted'].shape[0]}\")\n",
    "print(f\"Count running jobs:\\t{z[z['wps:Status'] =='Running'].shape[0]}\")\n",
    "print(f\"Count succeeded jobs:\\t{z[z['wps:Status'] =='Succeeded'].shape[0]}\")\n",
    "print(f\"Count failed jobs:\\t{z[z['wps:Status'] =='Failed'].shape[0]}\")\n",
    "print(f\"% of failed jobs:\\t{round(z[z['wps:Status'] =='Failed'].shape[0] / ( z[z['wps:Status'] =='Failed'].shape[0] + z[z['wps:Status'] =='Succeeded'].shape[0] ), 4) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "varying-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4881 entries, 0 to 0\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   status            4881 non-null   object        \n",
      " 1   http_status_code  4881 non-null   int64         \n",
      " 2   job_id            4881 non-null   object        \n",
      " 3   dps_num           4881 non-null   int64         \n",
      " 4   tile_num          4881 non-null   int64         \n",
      " 5   submit_time       4881 non-null   datetime64[ns]\n",
      " 6   dbs_job_hour      4881 non-null   int64         \n",
      " 7   algo_id           4881 non-null   object        \n",
      " 8   user              4881 non-null   object        \n",
      " 9   worker_type       4881 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(4), object(5)\n",
      "memory usage: 419.5+ KB\n"
     ]
    }
   ],
   "source": [
    "submit_results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "structural-battery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wps:Result': {'@xmlns:ows': 'http://www.opengis.net/ows/2.0',\n",
       "  '@xmlns:schemaLocation': 'http://schemas.opengis.net/wps/2.0/wps.xsd',\n",
       "  '@xmlns:wps': 'http://www.opengis.net/wps/2.0',\n",
       "  '@xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
       "  'wps:JobID': '9b9e1479-faf2-4093-b1ca-03fb2dbe1bed',\n",
       "  'wps:Output': {'@id': 'traceback',\n",
       "   'wps:Data': 'Traceback (most recent call last):\\n  File \"/home/ops/verdi/ops/hysds-0.3.11/hysds/job_worker.py\", line 1126, in run_job\\n    raise RuntimeError(\"Got non-zero exit code: {}\".format(status))\\nRuntimeError: Got non-zero exit code: 255'}}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Failed'].iloc[0].job_id).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "outer-antibody",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wps:Result': {'@xmlns:ows': 'http://www.opengis.net/ows/2.0',\n",
       "  '@xmlns:schemaLocation': 'http://schemas.opengis.net/wps/2.0/wps.xsd',\n",
       "  '@xmlns:wps': 'http://www.opengis.net/wps/2.0',\n",
       "  '@xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
       "  'wps:JobID': '167d1e36-6d6e-4d4c-9549-b02dae4d038e',\n",
       "  'wps:Output': {'@id': 'traceback',\n",
       "   'wps:Data': 'Traceback (most recent call last):\\n  File \"/home/ops/verdi/ops/hysds-0.3.11/hysds/job_worker.py\", line 1085, in run_job\\n    monitoredRunner.join()\\n  File \"/home/ops/verdi/lib/python3.7/site-packages/billiard/process.py\", line 148, in join\\n    res = self._popen.wait(timeout)\\n  File \"/home/ops/verdi/lib/python3.7/site-packages/billiard/popen_fork.py\", line 57, in wait\\n    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\\n  File \"/home/ops/verdi/lib/python3.7/site-packages/billiard/popen_fork.py\", line 33, in poll\\n    pid, sts = os.waitpid(self.pid, flag)\\n  File \"/home/ops/verdi/lib/python3.7/site-packages/billiard/pool.py\", line 229, in soft_timeout_sighandler\\n    raise SoftTimeLimitExceeded()\\nbilliard.exceptions.SoftTimeLimitExceeded: SoftTimeLimitExceeded()'}}}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Failed'].iloc[20].job_id).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-earth",
   "metadata": {},
   "source": [
    "## After successful DPS run, create AGB tindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "living-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "Building a list of tiles:\n",
      "MAAP version:\t\tmap_boreal_2022_rh_noground_v2\n",
      "Type:\t\tAGB\n",
      "Year:\t\t2023\n",
      "Month:\t\t['01']\n",
      "Days:\t\t1-31\n",
      "\n",
      "Output dir:  /projects/my-public-bucket/DPS_tile_lists/2023/\n",
      "                                             s3_path  ...                          file\n",
      "0  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  boreal_agb_20230124_0004.tif\n",
      "1  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  boreal_agb_20230124_0201.tif\n",
      "2  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  boreal_agb_20230124_0225.tif\n",
      "3  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  boreal_agb_20230124_0016.tif\n",
      "4  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  boreal_agb_20230124_0013.tif\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1597: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "/projects/icesat2_boreal/lib/build_tindex_master.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tile_num'] = df['tile_num'].astype(str).astype(int)\n",
      "# of duplicate tiles: 130\n",
      "Final # of tiles: 4668\n",
      "df shape :                                               s3_path  ... tile_num\n",
      "1   s3://maap-ops-workspace/lduncanson/dps_output/...  ...     3043\n",
      "0   s3://maap-ops-workspace/lduncanson/dps_output/...  ...      137\n",
      "25  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     2499\n",
      "24  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     1555\n",
      "23  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     2402\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "Writing tindex master csv: /projects/my-public-bucket/DPS_tile_lists/2023/AGB_tindex_master.csv\n",
      "Building tindex master json and mosaic json...\n",
      "Building /projects/my-public-bucket/DPS_tile_lists/2023/AGB_tindex_master_mosaic.json\n",
      "/projects/.local/lib/python3.7/site-packages/mercantile/__init__.py:79: FutureWarning: Mercantile 2.0 will require tile x and y to be within the range (0, 2 ** zoom)\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "## create AGB tindex \n",
    "month_dir_str = '01'\n",
    "alg_name_str = 'run_boreal_biomass_quick_v2_ubuntu'\n",
    "maap_version_str = 'map_boreal_2022_rh_noground_v2'\n",
    "#index_out_dir = os.path.join('/projects/my-private-bucket/dps_output/run_boreal_biomass_qiuck_v2_ubuntu/map_boreal_2022_v3/2022/', month_dir_str)\n",
    "index_out_dir = '/projects/my-public-bucket/DPS_tile_lists/2023/'\n",
    "!python /projects/icesat2_boreal/lib/build_tindex_master.py -t 'AGB' -alg_name $alg_name_str --maap_version $maap_version_str -y 2023 -m $month_dir_str -o $index_out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "experienced-personal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tiles to run: 212\n",
      " [3592, 521, 2569, 2570, 3594, 2061, 2573, 2575, 31758, 28178, 2070, 24, 35352, 31260, 35357, 3615, 2079, 4128, 1058, 548, 2086, 39464, 557, 2605, 3117, 2608, 2097, 2610, 4144, 1078, 3128, 3129, 4157, 3134, 42558, 2112, 65, 1604, 2119, 3145, 2559, 2125, 3661, 4176, 3670, 2648, 3675, 41052, 2142, 1119, 613, 2150, 2151, 106, 1643, 4204, 621, 1661, 3709, 127, 3714, 3716, 136, 2187, 34446, 4242, 3224, 4250, 669, 159, 3232, 4256, 3234, 2725, 3237, 2218, 173, 26285, 23218, 3252, 183, 186, 4283, 25790, 191, 192, 1216, 3778, 2753, 2757, 1224, 2248, 2250, 203, 2252, 2255, 208, 2257, 2258, 4307, 2263, 216, 728, 23551, 1246, 2270, 27364, 741, 230, 26341, 3821, 27381, 27384, 42745, 3321, 4344, 2300, 256, 3840, 1798, 1287, 4360, 33033, 777, 27911, 780, 27917, 2833, 786, 2322, 2324, 3346, 3862, 3348, 23829, 37147, 28444, 3871, 2339, 4388, 2853, 28965, 4396, 813, 3373, 3889, 35125, 3895, 1850, 4411, 834, 2375, 2887, 2894, 2896, 3923, 3412, 346, 1882, 348, 2394, 2907, 1892, 2405, 3941, 2408, 1898, 876, 4464, 34673, 3447, 3453, 2444, 3469, 2446, 45970, 2451, 3476, 36243, 2972, 4510, 26015, 419, 1956, 4007, 25512, 3495, 1963, 4012, 3506, 2997, 4534, 2999, 3000, 26554, 959, 967, 4040, 2000, 41437, 2526, 2528, 485, 27114, 493, 2032, 27635, 4083, 3574, 2045, 2558, 511]\n"
     ]
    }
   ],
   "source": [
    "TEST_DPS  = True\n",
    "\n",
    "if TEST_DPS:\n",
    "    \n",
    "    if True:\n",
    "        #!python /projects/icesat2_boreal/lib/build_tindex_master_v2.py -t AGB -y 2022 -m $month_dir_str -o $index_out_dir\n",
    "        t = pd.read_csv(os.path.join(index_out_dir,'AGB_tindex_master.csv'))\n",
    "        COMPLETED_TILES = t.tile_num.to_list()\n",
    "        NEED_TILES = list(set(DPS_INPUT_TILE_NUM_LIST) - set(COMPLETED_TILES))\n",
    "\n",
    "        #print(NEED_TILES)\n",
    "        DPS_INPUT_TILE_NUM_LIST = NEED_TILES\n",
    "    \n",
    "else:\n",
    "    DPS_INPUT_TILE_NUM_LIST = INPUT_TILE_NUM_LIST\n",
    "   \n",
    "\n",
    "#DPS_INPUT_TILE_NUM_LIST = [248, 273, 272, 271, 324]\n",
    "print(f\"# of tiles to run: {len(DPS_INPUT_TILE_NUM_LIST)}\\n\", DPS_INPUT_TILE_NUM_LIST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-diabetes",
   "metadata": {},
   "source": [
    "## Re-run Failed Tiles in DPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "casual-civilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input tiles for DPS: 219\n",
      "DPS run #: 1\t| tile num: 1539\t| submit status: success\t| job id: 24a8117e-1e63-4762-8007-a71545ae36e2\n",
      "DPS run #: 2\t| tile num: 3078\t| submit status: success\t| job id: ba3efa1c-f4c9-4f86-80a0-e17b5a1d2c07\n",
      "DPS run #: 25\t| tile num: 3633\t| submit status: success\t| job id: a5a7b136-c7bc-4ddd-a28f-aca8a01f96b9\n",
      "DPS run #: 50\t| tile num: 3178\t| submit status: success\t| job id: 9a772e83-2864-495b-8758-f79780c04abf\n",
      "DPS run #: 100\t| tile num: 2265\t| submit status: success\t| job id: 695d01d8-d6dd-4bab-922f-f0dd2e15e30f\n",
      "Current time:\t202211210208\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CompressionOptions' from 'pandas._typing' (/opt/conda/lib/python3.7/site-packages/pandas/_typing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[1;32m   3143\u001b[0m         \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mRender\u001b[0m \u001b[0ma\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0man\u001b[0m \u001b[0mHTML\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3145\u001b[0;31m         \u001b[0mExamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3146\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3147\u001b[0m         >>> df = pd.DataFrame(dict(name=['Raphael', 'Donatello'],\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwriters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlibwriters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mCompressionOptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mFilePathOrBuffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CompressionOptions' from 'pandas._typing' (/opt/conda/lib/python3.7/site-packages/pandas/_typing.py)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(f\"# of input tiles for DPS: {len_input_list}\")\n",
    "\n",
    "for i, INPUT_TILE_NUM in enumerate(DPS_INPUT_TILE_NUM_LIST):\n",
    "\n",
    "    DPS_num = i+1\n",
    "    IDENTIFIER = 'run_boreal_biomass_quick_v2'\n",
    "    ALGO_ID = f'{IDENTIFIER}_ubuntu'\n",
    "    USER = 'lduncanson'\n",
    "    WORKER_TYPE = 'maap-dps-worker-16gb'\n",
    "    \n",
    "    # Maybe we're using an input tile list that isnt built directly from an ATL08_filt CSV\n",
    "    # so, check if we have filtered ATL08 for this tile:\n",
    "    if INPUT_TILE_NUM in atl08_filt_tindex_master['tile_num'].to_list():\n",
    "        \n",
    "        # Get the s3 paths of the corresponding input filenames with an input tile_num\n",
    "        in_atl08_https =  atl08_filt_tindex_master['https'].loc[atl08_filt_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_topo_https = topo_tindex_master['https'].loc[topo_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_landsat_https = landsat_tindex_master['https'].loc[landsat_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_lc_https = lc_tindex_master['https'].loc[lc_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    " \n",
    "    else:\n",
    "        print(f\"No filtered ATL08 for tile {INPUT_TILE_NUM}. Moving on..\" )\n",
    "        continue\n",
    "    \n",
    "    # iters is the number of iterations of RH model fits; set to low (2-5) to test, 30-50 for production\n",
    "    # 30 yields run times ~4 hours\n",
    "        \n",
    "    # ppside is the number of subtile splits. 2=4 subtiles, 3=9 subtiles\n",
    "    # increasing ppside reduces memory, increases length to completion\n",
    "    \n",
    "    # minDOY is the min possible DOY to search for tile training data. The default is set to May 1\n",
    "    # If sufficient data are found in growing season, minDOY will not be used\n",
    "    \n",
    "    # maxDOY is the max possible DOY to search for tile training data. The default is Sept 30.\n",
    "    \n",
    "    # max_sol_el is the max solar elevation possible to search. The default is 0\n",
    "    \n",
    "    # expand_training as TRUE sets iterative searching for training data up to min_n, starting with solar_el<0 and growing season and then:\n",
    "    # searches solar_el up to max_sol_el then\n",
    "    # searches iteratively one month later after growing season until maxDOY then\n",
    "    # searches iteratively one month earlier before growing season until minDOY\n",
    "    # if expand_training=FALSE the growing season sol_el<0 will be taken, and if none, no data will return\n",
    "    \n",
    "    # min_n is the tile training number desired, that expand_training will search until it is fulfilled (or the max available)\n",
    "    \n",
    "    #local_train_perc is the percent tile training data vs. boreal-wide training data. 100 = all local, 0 = all boreal wide\n",
    "    \n",
    "    \n",
    "    in_param_dict = {\n",
    "                                    'in_atl08_fn': 's3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/fall2022/ATL08_filt_tindex_master.csv',\n",
    "                                    #'in_atl08_fn': f\"input/{os.path.basename(in_atl08_https)}\",\n",
    "                                    'in_topo_fn': f\"input/{os.path.basename(in_topo_https)}\",\n",
    "                                    'in_landsat_fn': f\"input/{os.path.basename(in_landsat_https)}\",\n",
    "                                    'in_lc_fn': f\"input/{os.path.basename(in_lc_https)}\",\n",
    "                                    'in_atl08_fn_url': in_atl08_https,\n",
    "                                    'in_topo_fn_url': in_topo_https,\n",
    "                                    'in_landsat_fn_url': in_landsat_https,\n",
    "                                    'in_lc_fn_url': in_lc_https,\n",
    "                                    'DO_SLOPE_VALID_MASK': 'TRUE',\n",
    "                                    'in_atl08_sample_fn':f\"input/{os.path.basename(train_data_https)}\",\n",
    "                                    'in_atl08_sample_url':train_data_https,\n",
    "                                    'in_tile_num': INPUT_TILE_NUM,\n",
    "                                    'in_tile_fn_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "                                    'in_tile_fn': 'input/boreal_tiles_v003.gpkg',\n",
    "                                    'in_tile_field': 'tile_num',\n",
    "                                    'iters': 1,\n",
    "                                    'ppside': 2,\n",
    "                                    'minDOY':130,\n",
    "                                    'maxDOY':250,\n",
    "                                    'max_sol_el':5,\n",
    "                                    'expand_training': 'TRUE',\n",
    "                                    'local_train_perc': 100,\n",
    "                                    'min_n': 5000, \n",
    "                                    'boreal_vect': 'https://maap-ops-workspace.s3.us-west-2.amazonaws.com/shared/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson',\n",
    "                                    'boreal_vect_fn': 'input/wwf_circumboreal_Dissolve_reprj.geojson'\n",
    "                }\n",
    "    \n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version='map_boreal_2022_v3',\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_param_dict\n",
    "        )\n",
    "\n",
    "    # Build a dataframe of submission details\n",
    "    submit_result['dps_num'] = DPS_num\n",
    "    submit_result['tile_num'] = INPUT_TILE_NUM\n",
    "    submit_result['submit_time'] = datetime.datetime.now()\n",
    "    submit_result['dbs_job_hour'] =datetime.datetime.now().hour\n",
    "    submit_result['algo_id'] = ALGO_ID\n",
    "    submit_result['user'] = USER\n",
    "    submit_result['worker_type'] = WORKER_TYPE\n",
    "\n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(pd.DataFrame([submit_result]))\n",
    "    \n",
    "    if DPS_num in [1, 2, 25, 50, 100, 500, 1000, 1500, 2000, 3000, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| tile num: {INPUT_TILE_NUM}\\t| submit status: {submit_result['status']}\\t| job id: {submit_result['job_id']}\") \n",
    "        #local_path\n",
    "\n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "print(f\"Current time:\\t{nowtime}\")\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/DPS_{IDENTIFIER}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-adams",
   "metadata": {},
   "source": [
    "## Re-Assess Re-submission\n",
    "Continue with last few steps until all jobs successful: create tindex, create new tile list, re-run missing tiles in DPS, re-assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dietary-platinum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count total jobs:\t219\n",
      "Count pending jobs:\t0\n",
      "Count running jobs:\t70\n",
      "Count succeeded jobs:\t15\n",
      "Count failed jobs:\t134\n",
      "% of failed jobs:\t89.92999999999999\n",
      "CPU times: user 2.24 s, sys: 197 ms, total: 2.44 s\n",
      "Wall time: 5.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#submit_results_df = pd.read_csv('/projects/my-public-bucket/DPS_run_boreal_biomass_quick_v2_submission_results_2494_202211132259.csv')\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "\n",
    "def BUILD_TABLE_JOBSTATUS(submit_results_df):\n",
    "    import xmltodict\n",
    "    df = pd.concat([pd.DataFrame(xmltodict.parse(maap.getJobStatus(job_id).content)).transpose() for job_id in submit_results_df.job_id.to_list()])\n",
    "    return df\n",
    "\n",
    "job_status_df = BUILD_TABLE_JOBSTATUS(submit_results_df)\n",
    "#print(job_status_df.head())\n",
    "\n",
    "num_jobs = submit_results_df.shape[0]\n",
    "z = submit_results_df.merge(job_status_df, how='left', left_on='job_id',  right_on='wps:JobID')\n",
    "\n",
    "print(f'Count total jobs:\\t{num_jobs}')\n",
    "print(f\"Count pending jobs:\\t{z[z['wps:Status'] =='Accepted'].shape[0]}\")\n",
    "print(f\"Count running jobs:\\t{z[z['wps:Status'] =='Running'].shape[0]}\")\n",
    "print(f\"Count succeeded jobs:\\t{z[z['wps:Status'] =='Succeeded'].shape[0]}\")\n",
    "print(f\"Count failed jobs:\\t{z[z['wps:Status'] =='Failed'].shape[0]}\")\n",
    "print(f\"% of failed jobs:\\t{round(z[z['wps:Status'] =='Failed'].shape[0] / ( z[z['wps:Status'] =='Failed'].shape[0] + z[z['wps:Status'] =='Succeeded'].shape[0] ), 4) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-conditions",
   "metadata": {},
   "source": [
    "## Get other lists just of missing tiles (various ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cooperative-drive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "need_tindex_master = pd.read_csv('/projects/my-public-bucket/DPS_tile_lists/Need_AGB_tindex_master.csv')\n",
    "print(len(need_tindex_master))\n",
    "\n",
    "INPUT_TILE_NUM_LIST = need_tindex_master.tile_num.tolist()\n",
    "\n",
    "# Remove duplicate tile nums\n",
    "INPUT_TILE_NUM_LIST = list(set(INPUT_TILE_NUM_LIST))\n",
    "\n",
    "print(len(INPUT_TILE_NUM_LIST))\n",
    "\n",
    "INPUT_TILE_NUM_LIST_NEED = INPUT_TILE_NUM_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pleased-disposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_tiles = [4304, 4305, 4221, 4220, 1785, 1718, 1720, 1661, 1257, 1318, 1317]\n",
    "werid_tiles = [1255, 1196, 949, 1062, 1063, 1005, 950, 1004]\n",
    "INPUT_TILE_NUM_LIST = weird_tiles\n",
    "len(INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accessory-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "[978, 988, 978, 988, 1263, 1261, 1264, 794, 2932, 3190, 3012, 3014, 3360, 3017, 378, 765, 812, 821, 764, 861, 1302, 1308, 1469, 2894, 2883, 2965, 2976, 3327, 3321, 3510, 3509, 4372, 4440, 4477, 2994, 1406, 2840, 411, 380, 3335, 2495, 4403, 2906, 2907, 2814]\n"
     ]
    }
   ],
   "source": [
    "# Get all boreal tiles\n",
    "ATL08_filt_tindex_master =   pd.read_csv('s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_filt_tindex_master.csv')\n",
    "boreal_tile_index_path = '/projects/shared-buckets/nathanmthomas/boreal_grid_albers90k_gpkg.gpkg'\n",
    "boreal_tile_index = geopandas.read_file(boreal_tile_index_path)\n",
    "boreal_tile_index.rename(columns={\"layer\":\"tile_num\"}, inplace=True)\n",
    "boreal_tile_index[\"tile_num\"] = boreal_tile_index[\"tile_num\"].astype(int)\n",
    "\n",
    "bad_tiles = [3540,3634,3728,3823,3916,4004] #Dropping the tiles near antimeridian that reproject poorly.\n",
    "select_needs = [3360,2994,3190,2840,3012,3014,3017,2932,1261,1263,1264,988,978,794, 380,378,411,821,861,\n",
    "                812,765,764,1308,1302,1469,1406,2495,2883,2965,3321,3509,3510,3327,3335,2976,2906,2907,2894,2814,4253,4293,4403,4440,4408,4372,4477,3986]\n",
    "\n",
    "# Remove bad tiles\n",
    "boreal_tile_index = boreal_tile_index[~boreal_tile_index['tile_num'].isin(bad_tiles)]\n",
    "    \n",
    "#print(boreal_tile_index.head())\n",
    "tile_matches_select_needs = boreal_tile_index.merge(ATL08_filt_tindex_master[ATL08_filt_tindex_master['tile_num'].isin(select_needs)], how='right', on='tile_num')\n",
    "print(len(tile_matches_select_needs))\n",
    "select_needs_filt = print([t for t in tile_matches_select_needs.tile_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "light-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "4\n",
      "[3986 4253 4293 4408]\n",
      "70\n",
      "[  65  106  127  250  361  417  493  566  568  618  741  753  786  798\n",
      "  801  830  848  876  877  927 1273 1287 1318 1402 1438 1439 1440 1604\n",
      " 1848 1850 1852 1936 2425 2426 2952 3124 3355 3540 3542 3634 3728 3823\n",
      " 3845 3846 3895 3916 4004 4007 4018 4083 4176 4204 4250 4256 4307 4325\n",
      " 4342 4395 4417 4444 4450 4451 4463 4467 4468 4483 4499 4510 4527 4534]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select_needs_filt = [978, 988, 978, 988, 1263, 1261, 1264, 794, 2932, 3190, 3012, 3014, 3360, 3017, 378, 765, 812, 821, 764, 861, 1302, 1308, 1469, 2894, 2883, 2965, 2976, 3327, 3321, 3510, 3509, 4372, 4440, 4477, 2994, 1406, 2840, 411, 380, 3335, 2495, 4403, 2906, 2907, 2814]\n",
    "\n",
    "print(len(select_needs_filt))\n",
    "\n",
    "import numpy as np\n",
    "tile_nums_missing_but_wont_run = np.setdiff1d(select_needs, select_needs_filt)\n",
    "print(len(tile_nums_missing_but_wont_run))\n",
    "print(tile_nums_missing_but_wont_run)\n",
    "\n",
    "tile_nums_missing_periphery = np.setdiff1d(INPUT_TILE_NUM_LIST_NEED, select_needs_filt)\n",
    "print(len(tile_nums_missing_periphery))\n",
    "print(tile_nums_missing_periphery)\n",
    "INPUT_TILE_NUM_LIST = tile_nums_missing_periphery\n",
    "len(INPUT_TILE_NUM_LIST)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "united-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TILE_NUM_LIST = [4440,4372,4477]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-performer",
   "metadata": {},
   "source": [
    "## Send output cog.tif tiles somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pharmaceutical-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "out_files = glob.glob(\"/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/**/*cog.tif\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "revised-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/02/23/49/35/272239/boreal_agb_20220202_0200_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/01/42/37/290043/boreal_agb_20220203_0224_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/01/52/46/430279/boreal_agb_20220203_0387_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/02/21/50/348844/boreal_agb_20220203_0386_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/03/01/42/237812/boreal_agb_20220203_0199_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/03/06/47/336472/boreal_agb_20220203_0223_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/03/08/52/152954/boreal_agb_20220203_0042_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/06/47/53/128079/boreal_agb_20220203_0111_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/06/59/17/037689/boreal_agb_20220203_0112_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/07/25/06/085526/boreal_agb_20220203_0049_cog.tif']\n"
     ]
    }
   ],
   "source": [
    "print(out_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUBDIR = 'add_solar'\n",
    "OUTDIR = f'/projects/my-private-bucket/dps_output/run_boreal_biomass_v2_ubuntu/master/2022/{TEST_SUBDIR}'\n",
    "!python /projects/icesat2_boreal/lib/build_tindex_master.py -y 2022 -m $TEST_SUBDIR -o $OUTDIR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
