{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sunset-benchmark",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py==3.1.0 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: pandas==1.2.2 in /opt/conda/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: pygeos==0.12.0 in /opt/conda/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 7)) (0.12.0)\n",
      "Requirement already satisfied: rtree==0.9.7 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 8)) (0.9.7)\n",
      "Requirement already satisfied: numpy==1.21.0 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 9)) (1.21.0)\n",
      "Requirement already satisfied: geopandas==0.9.0 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: rasterio==1.2.6 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (1.2.6)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 14)) (5.10.2)\n",
      "Requirement already satisfied: contextily in /opt/conda/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.2.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 16)) (2023.1.0)\n",
      "Requirement already satisfied: s3fs==0.3.4 in /opt/conda/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (0.3.4)\n",
      "Requirement already satisfied: rio-cogeo==2.3.1 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 22)) (2.3.1)\n",
      "Requirement already satisfied: rio-tiler==2.1.4 in /opt/conda/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.1.4)\n",
      "Requirement already satisfied: morecantile==2.1.4 in /opt/conda/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 25)) (2.1.4)\n",
      "Requirement already satisfied: pystac-client in /opt/conda/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 26)) (0.5.1)\n",
      "Requirement already satisfied: cachetools in /opt/conda/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 48)) (5.2.1)\n",
      "Requirement already satisfied: cached-property in /projects/.local/lib/python3.7/site-packages (from h5py==3.1.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /projects/.local/lib/python3.7/site-packages (from pandas==1.2.2->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 4)) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.2.2->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: shapely>=1.6 in /projects/.local/lib/python3.7/site-packages (from geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (1.7.1)\n",
      "Requirement already satisfied: fiona>=1.8 in /projects/.local/lib/python3.7/site-packages (from geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (1.8.20)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in /projects/.local/lib/python3.7/site-packages (from geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (3.2.1)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (1.4.7)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (20.3.0)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (8.0.1)\n",
      "Requirement already satisfied: affine in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (2.3.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (0.7.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (2021.5.30)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (41.4.0)\n",
      "Requirement already satisfied: click-plugins in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (1.1.1)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.7/site-packages/botocore-1.20.72-py3.7.egg (from s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (1.20.72)\n",
      "Requirement already satisfied: boto3>=1.9.91 in /opt/conda/lib/python3.7/site-packages/boto3-1.17.72-py3.7.egg (from s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (1.17.72)\n",
      "Requirement already satisfied: pydantic in /projects/.local/lib/python3.7/site-packages (from rio-cogeo==2.3.1->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 22)) (1.8.2)\n",
      "Requirement already satisfied: requests in /projects/.local/lib/python3.7/site-packages (from rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.27.1)\n",
      "Requirement already satisfied: rio-color in /projects/.local/lib/python3.7/site-packages (from rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (1.0.4)\n",
      "Requirement already satisfied: numexpr in /projects/.local/lib/python3.7/site-packages (from rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.7.3)\n",
      "Requirement already satisfied: pystac>=0.5.3 in /opt/conda/lib/python3.7/site-packages (from rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (1.5.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib_resources->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 14)) (3.4.1)\n",
      "Requirement already satisfied: geopy in /projects/.local/lib/python3.7/site-packages (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (2.2.0)\n",
      "Requirement already satisfied: joblib in /projects/.local/lib/python3.7/site-packages (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.0.1)\n",
      "Requirement already satisfied: xyzservices in /opt/conda/lib/python3.7/site-packages (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (2022.9.0)\n",
      "Requirement already satisfied: mercantile in /projects/.local/lib/python3.7/site-packages (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.2.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages/matplotlib-3.4.2-py3.7-linux-x86_64.egg (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (3.4.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages/Pillow-8.2.0-py3.7-linux-x86_64.egg (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (8.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages/jmespath-0.10.0-py3.7.egg (from boto3>=1.9.91->s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages/s3transfer-0.4.2-py3.7.egg (from boto3>=1.9.91->s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (0.4.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /projects/.local/lib/python3.7/site-packages (from botocore>=1.12.91->s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (1.26.7)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click>=4.0->rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (3.7.3)\n",
      "Requirement already satisfied: munch in /projects/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from pystac>=0.5.3->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /projects/.local/lib/python3.7/site-packages (from requests->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.0.10)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (2.4.7)\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in /projects/.local/lib/python3.7/site-packages (from geopy->contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.52)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages/cycler-0.10.0-py3.7.egg (from matplotlib->contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages/kiwisolver-1.3.1-py3.7-linux-x86_64.egg (from matplotlib->contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: rio-mucho in /projects/.local/lib/python3.7/site-packages (from rio-color->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host='api.ops.maap-project.org')\n",
    "!pip install -U -r /projects/icesat2_boreal/dps/requirements_main.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-collector",
   "metadata": {},
   "source": [
    "# Launch DPS for mapBoreal.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moral-decision",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "!pip install xmltodict\n",
    "import xmltodict\n",
    "\n",
    "def local_to_s3(url, user='lduncanson'):\n",
    "    ''' A Function to convert local paths to s3 urls'''\n",
    "    return url.replace('/projects/my-private-bucket', f's3://maap-ops-workspace/{user}')\n",
    "def local_to_https(url, user='lduncanson'):\n",
    "    ''' A Function to convert local paths to https urls'''\n",
    "    return url.replace('/projects/my-private-bucket', f'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/{user}')\n",
    "def local_to_https_uswest2(url, user='lduncanson'):\n",
    "    ''' A Function to convert local paths to https us-west-s urls'''\n",
    "    return url.replace('/projects/my-private-bucket', f'https://maap-ops-workspace.s3.us-west-2.amazonaws.com/{user}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ready-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "#atl08_filt_tindex_master =   pd.read_csv('s3://maap-ops-workspace/lduncanson/dps_output/run_tile_atl08_ubuntu/master/2022/03/ATL08_filt_tindex_master.csv')\n",
    "\n",
    "#atl08_filt_tindex_master =   pd.read_csv('s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/ATL08_filt_tindex_master.csv')\n",
    "#my-public-bucket/DPS_tile_lists/fall2022/ATL08_filt_tindex_master.csv\n",
    "\n",
    "#note - check that the atl08 version matched ground = with_gedi_rh noground = with_atl03_rh\n",
    "atl08_filt_tindex_master =   pd.read_csv('/projects/shared-buckets/lduncanson/DPS_tile_lists/fall2022/with_gedi_rh/ATL08_filt_tindex_master.csv')\n",
    "topo_tindex_master =         pd.read_csv('/projects/shared-buckets/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv')\n",
    "landsat_tindex_master =      pd.read_csv('/projects/shared-buckets/nathanmthomas/DPS_tile_lists/HLS/c2020/HLS_stack_2022_v2/HLS_tindex_master.csv')\n",
    "lc_tindex_master = pd.read_csv('/projects/shared-buckets/nathanmthomas/DPS_tile_lists/LC/LC_tindex_master.csv')\n",
    "\n",
    "\n",
    "# Convert al local_paths to s3\n",
    "#.. for data produced by 'lduncanson' workspace\n",
    "atl08_filt_tindex_master['https'] = [local_to_https_uswest2(local_path, user='lduncanson') for local_path in atl08_filt_tindex_master['local_path']]\n",
    "\n",
    "#.. for data produced by 'nathanmthomas' workspace\n",
    "for tindex_master in [topo_tindex_master, landsat_tindex_master, lc_tindex_master]:\n",
    "    tindex_master['https'] = [local_to_https_uswest2(local_path, user='nathanmthomas') for local_path in tindex_master['local_path']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-graphics",
   "metadata": {},
   "source": [
    "# Use the ATL08 filtered tindex master list to tell you which tiles you'll run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "included-compression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NORWAY_TILE_LIST = pd.read_csv('/projects/my-public-bucket/misc_files/norway_tiles.csv').layer.tolist()\n",
    "\n",
    "#NORWAY_TILE_LIST = [42,199,200,223,224,386,387,11,49,111,112,131]\n",
    "\n",
    "DELTA_TILE_LIST = [3459]\n",
    "\n",
    "BONA_TILE_LIST  = [3364,3365]\n",
    "\n",
    "HEALY_TILE_LIST = [3552]\n",
    "\n",
    "INPUT_EXPERIMENT_TILE_NUM_LIST = NORWAY_TILE_LIST + DELTA_TILE_LIST + BONA_TILE_LIST + HEALY_TILE_LIST\n",
    "#INPUT_EXPERIMENT_TILE_NUM_LIST = NORWAY_TILE_LIST\n",
    "#DPS_INPUT_TILE_NUM_LIST = INPUT_EXPERIMENT_TILE_NUM_LIST\n",
    "#len(DPS_INPUT_TILE_NUM_LIST)\n",
    "\n",
    "#ALASKA_TILE_LIST =  list(range(3268,3272+1))+\\\n",
    "#                    list(range(3361,3366+1))+\\\n",
    "#                    list(range(3454,3459+1))+\\\n",
    "#                    list(range(3549,3555+1))+\\\n",
    "#                    list(range(3643,3648+1))\n",
    "\n",
    "print(len(INPUT_EXPERIMENT_TILE_NUM_LIST))\n",
    "DPS_INPUT_TILE_NUM_LIST = INPUT_EXPERIMENT_TILE_NUM_LIST\n",
    "\n",
    "FULL_TILE_LIST = atl08_filt_tindex_master['tile_num']\n",
    "\n",
    "DPS_INPUT_TILE_NUM_LIST = [639, 640, 726, 727, 373, 405]\n",
    "\n",
    "DPS_INPUT_TILE_NUM_LIST = FULL_TILE_LIST\n",
    "DPS_INPUT_TILE_NUM_LIST = [3082,2918, 1854, 1789, 1262,34210, 33975,37,148,169,446,162,629,1761, 2651, 2970, 3604, 4080]\n",
    "DPS_INPUT_TILE_NUM_LIST = [1050, 1105, 34210, 4245, 24959, 23828]\n",
    "DPS_INPUT_TILE_NUM_LIST = INPUT_EXPERIMENT_TILE_NUM_LIST\n",
    "\n",
    "DPS_INPUT_TILE_NUM_LIST = INPUT_EXPERIMENT_TILE_NUM_LIST\n",
    "\n",
    "\n",
    "print(len(DPS_INPUT_TILE_NUM_LIST))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "leading-edmonton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/lduncanson/dps_output/run_tile_atl08_ubuntu/tile_atl08/2022/10/27/14/11/37/788374/atl08_005_30m_filt_topo_landsat_20221027_0131.csv\n",
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/nathanmthomas/dps_output/do_topo_stack_3-1-5_ubuntu/ops/2021/09/15/18/37/26/218643/Copernicus_131_covars_cog_topo_stack.tif\n",
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/nathanmthomas/dps_output/do_HLS_stack_3-1-2_ubuntu/HLS_stack_2022_v2/2022/10/12/20/45/20/047641/HLS_131_06-01_09-15_2019_2021.tif\n",
      "https://maap-ops-workspace.s3.us-west-2.amazonaws.com/nathanmthomas/dps_output/run_build_stack_ubuntu/master/2022/09/15/17/23/42/788257/esa_worldcover_v100_2020_131_cog.tif\n"
     ]
    }
   ],
   "source": [
    "# Check retrieval of s3 path with a tle_num\n",
    "in_atl08_https =  atl08_filt_tindex_master['https'].loc[atl08_filt_tindex_master['tile_num'] == DPS_INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "in_topo_https = topo_tindex_master['https'].loc[topo_tindex_master['tile_num'] == DPS_INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "in_landsat_https = landsat_tindex_master['https'].loc[landsat_tindex_master['tile_num'] == DPS_INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "in_lc_https = lc_tindex_master['https'].loc[lc_tindex_master['tile_num'] == DPS_INPUT_TILE_NUM_LIST[0]].tolist()[0]\n",
    "\n",
    "print(in_atl08_https)\n",
    "print(in_topo_https)\n",
    "print(in_landsat_https)\n",
    "print(in_lc_https)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-gentleman",
   "metadata": {},
   "source": [
    "## Get files for boreal biomass models & boreal wide sample\n",
    "Note that this no longer sends the bio models to the job - this is embedded in the .sh run script (not in the param dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alpine-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_models_https = 'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/lduncanson/my-private-bucket/bio_models_ground.tar'\n",
    "\n",
    "train_data_https = 'https://maap-ops-workspace.s3.us-west-2.amazonaws.com/lduncanson/boreal_train_data_v10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "engaging-digest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/lduncanson/my-private-bucket/bio_models_ground.tar'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_models_https"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-royalty",
   "metadata": {},
   "source": [
    "## Test from Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "relevant-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put latest test data in a test folder (outside repo)\n",
    "# Terminal:\n",
    "                           \n",
    "# file 1: /projects/testing/input/atl08_004_30m_filt_topo_landsat_20220208_0042.csv\n",
    "# file 2: /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_42_covars_cog_topo_stack.tif\n",
    "# file 3: /projects/shared-buckets/nathanmthomas/alg_34_testing/Landsat8_42_comp_cog_2015-2020_dps.tif\n",
    "#file 4: /projects/my-private-bucket/boreal_train_data_v3.csv\n",
    "#file 5: /projects/my-private-bucket/bio_models.tar\n",
    "#file 6: 42\n",
    "# file 7:\n",
    "\n",
    "# ./run_boreal_biomass.sh file1 file2 file3 file4 file5 file6 file7\n",
    "#ATL08_CSV= /projects/testing/input/atl08_005_30m_filt_topo_landsat_20220322_0042.csv\n",
    "#TOPO_TIF=/projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_42_covars_cog_topo_stack.tif\n",
    "#LANDSAT_TIF= /projects/shared-buckets/nathanmthomas/alg_34_testing/Landsat8_42_comp_cog_2015-2020_dps.tif\n",
    "#DO_SLOPE_VALID_MASK= 'TRUE'\n",
    "#ATL08_SAMPLE_CSV=/projects/my-private-bucket/boreal_train_data_v3.csv\n",
    "#in_tile_num=42\n",
    "#in_tile_fn=/projects/shared-buckets/nathanmthomas/boreal_grid_albers90k_gpkg.gpkg\n",
    "\n",
    "#python /projects/icesat2_boreal/lib/merge_neighbors_atl08.py -in_tile_num 42 -in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg -in_tile_field 'tile_num' -csv_list_fn s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_filt_tindex_master.csv -out_dir /projects/testing/output/\n",
    "#echo $cmd\n",
    "#eval $cmd\n",
    "\n",
    "# Set the output merged CSV name to a var\n",
    "#MERGED_ATL08_CSV=$(ls ${OUTPUTDIR}/atl08_005_30m_filt_merge_neighbors* | head -1)\n",
    "\n",
    "# Run mapBoreal with merged CSV as input\n",
    "#conda activate r-with-gdal\n",
    "\n",
    "\n",
    "#Rscript /projects/icesat2_boreal/lib/mapBoreal.R /projects/testing/output/atl08_004_30m_filt_merge_neighbors_0042.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_42_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_42_06-01_09-15_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v4.csv 2 2 100 300 5 TRUE 50 3000 \n",
    "\n",
    "#test swapping for terra\n",
    "#Rscript /projects/icesat2_boreal/lib/mapBoreal_speedy.R /projects/testing/output/atl08_004_30m_filt_merge_neighbors_0042.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_42_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_42_06-01_09-15_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v4.csv 3 2 100 300 5 TRUE 100 3000 /projects/shared-buckets/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson \n",
    "\n",
    "# of tiles to run: 115\n",
    "# [3585, 4108, 1037, 28176, 4112, 31762, 1050, 2075, 28702, 31263, 29223, 29739, 30252, 3117, 30763, 3120, 2614, 2618, 2619, 1615, 1105, 1617, 2142, 612, 2148, 2149, 2151, 2152, 2153, 1146, 1662, 1159, 1672, 2698, 34443, 26280, 2217, 2223, 2224, 33975, 1721, 3770, 3771, 1214, 1731, 1733, 26827, 2779, 3301, 3302, 27368, 745, 3307, 1273, 2301, 27908, 1798, 1799, 1800, 3861, 28440, 3865, 31513, 28963, 31014, 29481, 2859, 29996, 30508, 1334, 4411, 3393, 3395, 2375, 3399, 1865, 1866, 1867, 2378, 2379, 2380, 2381, 3400, 3951, 3952, 34674, 3954, 2942, 895, 2944, 1933, 1934, 1937, 1938, 2457, 3486, 34210, 931, 1448, 26554, 4034, 4035, 1999, 981, 3029, 3030, 2009, 2010, 2011, 27098, 991, 992, 993, 2533, 27638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "designing-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of tile 1927\n",
    "#ATL08_CSV= /projects/testing/input/atl08_005_30m_filt_topo_landsat_20220418_1927.csv\n",
    "#TOPO_TIF=/projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_1927_covars_cog_topo_stack.tif \n",
    "#LANDSAT_TIF= /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_1927_06-15_09-01_2019_2021.tif\n",
    "#DO_SLOPE_VALID_MASK= 'TRUE'\n",
    "#ATL08_SAMPLE_CSV=/projects/my-private-bucket/boreal_train_data_v4.csv\n",
    "#in_tile_num=42\n",
    "#in_tile_fn=/projects/shared-buckets/nathanmthomas/boreal_grid_albers90k_gpkg.gpkg\n",
    "\n",
    "#python /projects/icesat2_boreal/lib/merge_neighbors_atl08.py -in_tile_num 1927 -in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg -in_tile_field 'tile_num' -csv_list_fn /projects/my-public-bucket/DPS_tile_lists/ATL08_filt_tindex_master.csv -out_dir /projects/testing/output/\n",
    "#echo $cmd\n",
    "#eval $cmd\n",
    "\n",
    "# Set the output merged CSV name to a var\n",
    "#MERGED_ATL08_CSV=$(ls ${OUTPUTDIR}/atl08_005_30m_filt_merge_neighbors* | head -1)\n",
    "\n",
    "# Run mapBoreal with merged CSV as input\n",
    "#conda activate r-with-gdal\n",
    "\n",
    "\n",
    "#Rscript /projects/icesat2_boreal/lib/mapBoreal.R /projects/testing/output/atl08_004_30m_filt_merge_neighbors_0042.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_42_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_726_06-15_09-01_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v5.csv 2 2 100 300 5 TRUE 50 3000 \n",
    "\n",
    "#test swapping for terra\n",
    "#Rscript /projects/icesat2_boreal/lib/mapBoreal_speedy.R /projects/shared-buckets/nathanmthomas/atl08_004_30m_filt_merge_neighbors_3885.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_3585_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_727_06-15_09-01_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v5.csv 2 2 100 300 5 TRUE 100 3000 /projects/shared-buckets/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "macro-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on tile 727 and 726\n",
    "#python /projects/icesat2_boreal/lib/merge_neighbors_atl08.py -in_tile_num 727 -in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg -in_tile_field 'tile_num' -csv_list_fn s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_filt_tindex_master.csv -out_dir /projects/testing/output/\n",
    "       \n",
    "#python /projects/icesat2_boreal/lib/merge_neighbors_atl08.py -in_tile_num 726 -in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg -in_tile_field 'tile_num' -csv_list_fn s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_filt_tindex_master.csv -out_dir /projects/testing/output/    \n",
    "\n",
    "## Rscript /projects/icesat2_boreal/lib/mapBoreal_speedy.R /projects/testing/output/atl08_004_30m_filt_merge_neighbors_0726.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_726_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_726_06-15_09-01_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v7.csv 2 2 130 250 5 TRUE 50 5000 /projects/shared-buckets/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson \n",
    "\n",
    "## Rscript /projects/icesat2_boreal/lib/mapBoreal_speedy.R /projects/testing/output/atl08_004_30m_filt_merge_neighbors_0727.csv /projects/shared-buckets/nathanmthomas/alg_34_testing/Copernicus_727_covars_cog_topo_stack.tif /projects/shared-buckets/nathanmthomas/alg_34_testing/HLS_727_06-15_09-01_2019_2021.tif 'TRUE' /projects/my-private-bucket/boreal_train_data_v7.csv 2 2 130 250 5 TRUE 50 5000 /projects/shared-buckets/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-integer",
   "metadata": {},
   "source": [
    "## Commit with Tag for running\n",
    "1) Add version name as a map_boreal_2022_v2 or whatever is appropriate - both to this notebook and algorithm config yaml\n",
    "\n",
    "2) follow git instructions (every time!!):\n",
    "git add changes\n",
    "git commit -m 'message'\n",
    "git tag -f map_boreal_2022_rh_noground_v2\n",
    "\n",
    "git push\n",
    "git push origin -f map_boreal_2022_rh_noground_v2\n",
    "git push dps\n",
    "git push dps -f map_boreal_2022_rh_ground_v1\n",
    "\n",
    "3) if it looks weird check git log to make sure tag is at same place as origin and dps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-disabled",
   "metadata": {},
   "source": [
    "## register via terminal\n",
    "python /projects/register-algorithm.py /projects/icesat2_boreal/dps/alg_3-4/algorithm_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-dimension",
   "metadata": {},
   "source": [
    "## Run a DPS job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "comparative-publisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input tiles for DPS: 127\n",
      "DPS run #: 1\t| tile num: 131\t| submit status: success\t| job id: 068de509-dc89-44ed-881b-48c8772fc565\n",
      "DPS run #: 2\t| tile num: 132\t| submit status: success\t| job id: 567b902f-ceb8-438f-a218-8670829589a0\n",
      "No filtered ATL08 for tile 5. Moving on..\n",
      "No filtered ATL08 for tile 6. Moving on..\n",
      "No filtered ATL08 for tile 7. Moving on..\n",
      "No filtered ATL08 for tile 10. Moving on..\n",
      "No filtered ATL08 for tile 275. Moving on..\n",
      "No filtered ATL08 for tile 14. Moving on..\n",
      "DPS run #: 25\t| tile num: 151\t| submit status: success\t| job id: 2225d777-68f0-4847-a41f-81cbe233e349\n",
      "No filtered ATL08 for tile 21. Moving on..\n",
      "No filtered ATL08 for tile 301. Moving on..\n",
      "No filtered ATL08 for tile 178. Moving on..\n",
      "No filtered ATL08 for tile 328. Moving on..\n",
      "DPS run #: 100\t| tile num: 354\t| submit status: success\t| job id: 88486371-1524-4320-bdf3-cea14753fa92\n",
      "No filtered ATL08 for tile 357. Moving on..\n",
      "No filtered ATL08 for tile 250. Moving on..\n",
      "Current time:\t202301130521\n",
      "CPU times: user 1.47 s, sys: 87 ms, total: 1.55 s\n",
      "Wall time: 28.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>http_status_code</th>\n",
       "      <th>job_id</th>\n",
       "      <th>dps_num</th>\n",
       "      <th>tile_num</th>\n",
       "      <th>submit_time</th>\n",
       "      <th>dbs_job_hour</th>\n",
       "      <th>algo_id</th>\n",
       "      <th>user</th>\n",
       "      <th>worker_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>068de509-dc89-44ed-881b-48c8772fc565</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-01-13 05:21:29.221859</td>\n",
       "      <td>5</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>567b902f-ceb8-438f-a218-8670829589a0</td>\n",
       "      <td>2</td>\n",
       "      <td>132</td>\n",
       "      <td>2023-01-13 05:21:29.298634</td>\n",
       "      <td>5</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>cf91c3e3-3e9d-4ce2-8a69-146ce8cd8a76</td>\n",
       "      <td>3</td>\n",
       "      <td>133</td>\n",
       "      <td>2023-01-13 05:21:29.401913</td>\n",
       "      <td>5</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>65e6c0f9-1f4b-43a2-977d-c9bf190b5c06</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-13 05:21:29.497318</td>\n",
       "      <td>5</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>43563534-20a6-4413-a7e0-3ccc0ffd7b11</td>\n",
       "      <td>8</td>\n",
       "      <td>270</td>\n",
       "      <td>2023-01-13 05:21:29.683664</td>\n",
       "      <td>5</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>ed9b2d4b-5da5-49cd-993a-4a16fa5b7e3c</td>\n",
       "      <td>123</td>\n",
       "      <td>130</td>\n",
       "      <td>2023-01-13 05:21:56.563516</td>\n",
       "      <td>5</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>bfb6d71e-7bc7-4d30-96f5-c0ec7cec16ca</td>\n",
       "      <td>124</td>\n",
       "      <td>3459</td>\n",
       "      <td>2023-01-13 05:21:56.725112</td>\n",
       "      <td>5</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>bf12f195-275d-4efe-841c-419c5d7e719a</td>\n",
       "      <td>125</td>\n",
       "      <td>3364</td>\n",
       "      <td>2023-01-13 05:21:56.887252</td>\n",
       "      <td>5</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>27eef1cd-c57c-450a-a70f-f7243f40d2d8</td>\n",
       "      <td>126</td>\n",
       "      <td>3365</td>\n",
       "      <td>2023-01-13 05:21:57.095524</td>\n",
       "      <td>5</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>6c06dab5-d7b7-44ed-9ad1-5aaae0923e81</td>\n",
       "      <td>127</td>\n",
       "      <td>3552</td>\n",
       "      <td>2023-01-13 05:21:57.277024</td>\n",
       "      <td>5</td>\n",
       "      <td>run_boreal_biomass_quick_v2_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     status  http_status_code                                job_id  dps_num  \\\n",
       "0   success               200  068de509-dc89-44ed-881b-48c8772fc565        1   \n",
       "0   success               200  567b902f-ceb8-438f-a218-8670829589a0        2   \n",
       "0   success               200  cf91c3e3-3e9d-4ce2-8a69-146ce8cd8a76        3   \n",
       "0   success               200  65e6c0f9-1f4b-43a2-977d-c9bf190b5c06        4   \n",
       "0   success               200  43563534-20a6-4413-a7e0-3ccc0ffd7b11        8   \n",
       "..      ...               ...                                   ...      ...   \n",
       "0   success               200  ed9b2d4b-5da5-49cd-993a-4a16fa5b7e3c      123   \n",
       "0   success               200  bfb6d71e-7bc7-4d30-96f5-c0ec7cec16ca      124   \n",
       "0   success               200  bf12f195-275d-4efe-841c-419c5d7e719a      125   \n",
       "0   success               200  27eef1cd-c57c-450a-a70f-f7243f40d2d8      126   \n",
       "0   success               200  6c06dab5-d7b7-44ed-9ad1-5aaae0923e81      127   \n",
       "\n",
       "    tile_num                submit_time  dbs_job_hour  \\\n",
       "0        131 2023-01-13 05:21:29.221859             5   \n",
       "0        132 2023-01-13 05:21:29.298634             5   \n",
       "0        133 2023-01-13 05:21:29.401913             5   \n",
       "0          4 2023-01-13 05:21:29.497318             5   \n",
       "0        270 2023-01-13 05:21:29.683664             5   \n",
       "..       ...                        ...           ...   \n",
       "0        130 2023-01-13 05:21:56.563516             5   \n",
       "0       3459 2023-01-13 05:21:56.725112             5   \n",
       "0       3364 2023-01-13 05:21:56.887252             5   \n",
       "0       3365 2023-01-13 05:21:57.095524             5   \n",
       "0       3552 2023-01-13 05:21:57.277024             5   \n",
       "\n",
       "                               algo_id        user           worker_type  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-16gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-16gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-16gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-16gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-16gb  \n",
       "..                                 ...         ...                   ...  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-16gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-16gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-16gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-16gb  \n",
       "0   run_boreal_biomass_quick_v2_ubuntu  lduncanson  maap-dps-worker-16gb  \n",
       "\n",
       "[115 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(f\"# of input tiles for DPS: {len_input_list}\")\n",
    "\n",
    "for i, INPUT_TILE_NUM in enumerate(DPS_INPUT_TILE_NUM_LIST):\n",
    "\n",
    "    DPS_num = i+1\n",
    "    IDENTIFIER = 'run_boreal_biomass_quick_v2'\n",
    "    ALGO_ID = f'{IDENTIFIER}_ubuntu'\n",
    "    USER = 'lduncanson'\n",
    "    WORKER_TYPE = 'maap-dps-worker-16gb'\n",
    "    \n",
    "    # Maybe we're using an input tile list that isnt built directly from an ATL08_filt CSV\n",
    "    # so, check if we have filtered ATL08 for this tile:\n",
    "    if INPUT_TILE_NUM in atl08_filt_tindex_master['tile_num'].to_list():\n",
    "        \n",
    "        # Get the s3 paths of the corresponding input filenames with an input tile_num\n",
    "        in_atl08_https =  atl08_filt_tindex_master['https'].loc[atl08_filt_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_topo_https = topo_tindex_master['https'].loc[topo_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_landsat_https = landsat_tindex_master['https'].loc[landsat_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_lc_https = lc_tindex_master['https'].loc[lc_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    " \n",
    "    else:\n",
    "        print(f\"No filtered ATL08 for tile {INPUT_TILE_NUM}. Moving on..\" )\n",
    "        continue\n",
    "    \n",
    "    # iters is the number of iterations of RH model fits; For no uncertainties set iters = 1 for quick mapping / testing.\n",
    "    # anything >1 will be exhaustive until uncertainties constrain or iters becomes 250. Will take a long time.\n",
    "        \n",
    "    # ppside is the number of subtile splits. 2=4 subtiles, 3=9 subtiles\n",
    "    # increasing ppside reduces memory, increases length to completion\n",
    "    \n",
    "    # minDOY is the min possible DOY to search for tile training data. The default is set to May 1\n",
    "    # If sufficient data are found in growing season, minDOY will not be used\n",
    "    \n",
    "    # maxDOY is the max possible DOY to search for tile training data. The default is Sept 30.\n",
    "    \n",
    "    # max_sol_el is the max solar elevation possible to search. The default is 0\n",
    "    \n",
    "    # expand_training as TRUE sets iterative searching for training data up to min_n, starting with solar_el<0 and growing season and then:\n",
    "    # searches solar_el up to max_sol_el then\n",
    "    # searches iteratively one month later after growing season until maxDOY then\n",
    "    # searches iteratively one month earlier before growing season until minDOY\n",
    "    # if expand_training=FALSE the growing season sol_el<0 will be taken, and if none, no data will return\n",
    "    \n",
    "    # min_n is the tile training number desired, that expand_training will search until it is fulfilled (or the max available)\n",
    "    \n",
    "    #local_train_perc is the percent tile training data vs. boreal-wide training data. 100 = all local, 0 = all boreal wide\n",
    "        \n",
    "    in_param_dict = {\n",
    "                                    'in_atl08_fn': 's3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/fall2022/with_gedi_rh/ATL08_filt_tindex_master.csv',\n",
    "                                    #'in_atl08_fn': f\"input/{os.path.basename(in_atl08_https)}\",\n",
    "                                    'in_topo_fn': f\"input/{os.path.basename(in_topo_https)}\",\n",
    "                                    'in_landsat_fn': f\"input/{os.path.basename(in_landsat_https)}\",\n",
    "                                    'in_lc_fn': f\"input/{os.path.basename(in_lc_https)}\",\n",
    "                                    'in_atl08_fn_url': in_atl08_https,\n",
    "                                    'in_topo_fn_url': in_topo_https,\n",
    "                                    'in_landsat_fn_url': in_landsat_https,\n",
    "                                    'in_lc_fn_url': in_lc_https,\n",
    "                                    'DO_SLOPE_VALID_MASK': 'TRUE',\n",
    "                                    'in_atl08_sample_fn':f\"input/{os.path.basename(train_data_https)}\",\n",
    "                                    'in_atl08_sample_url':train_data_https,\n",
    "                                    'in_tile_num': INPUT_TILE_NUM,\n",
    "                                    'in_tile_fn_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "                                    'in_tile_fn': 'input/boreal_tiles_v003.gpkg',\n",
    "                                    'in_tile_field': 'tile_num',\n",
    "                                    'iters': 1,\n",
    "                                    'ppside': 2,\n",
    "                                    'minDOY':130,\n",
    "                                    'maxDOY':250,\n",
    "                                    'max_sol_el':5,\n",
    "                                    'expand_training': 'TRUE',\n",
    "                                    'local_train_perc': 100,\n",
    "                                    'min_n': 5000, \n",
    "                                    'boreal_vect': 'https://maap-ops-workspace.s3.us-west-2.amazonaws.com/shared/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson',\n",
    "                                    'boreal_vect_fn': 'input/wwf_circumboreal_Dissolve_reprj.geojson',\n",
    "                                    'predict_var': 'Ht'\n",
    "                }\n",
    "    \n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version='map_boreal_2022_rh_ground_v1',#, ht_boreal_2022_rh_noground_v1 ,'map_boreal_2022_rh_ground_v1','map_boreal_2022_rh_noground_v1',\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_param_dict\n",
    "        )\n",
    "\n",
    "    # Build a dataframe of submission details\n",
    "    submit_result['dps_num'] = DPS_num\n",
    "    submit_result['tile_num'] = INPUT_TILE_NUM\n",
    "    submit_result['submit_time'] = datetime.datetime.now()\n",
    "    submit_result['dbs_job_hour'] =datetime.datetime.now().hour\n",
    "    submit_result['algo_id'] = ALGO_ID\n",
    "    submit_result['user'] = USER\n",
    "    submit_result['worker_type'] = WORKER_TYPE\n",
    "\n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(pd.DataFrame([submit_result]))\n",
    "    \n",
    "    if DPS_num in [1, 2, 25, 50, 100, 500, 1000, 1500, 2000, 3000, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| tile num: {INPUT_TILE_NUM}\\t| submit status: {submit_result['status']}\\t| job id: {submit_result['job_id']}\") \n",
    "        #local_path\n",
    "\n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "print(f\"Current time:\\t{nowtime}\")\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/DPS_{IDENTIFIER}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-printing",
   "metadata": {},
   "source": [
    "## Assess DPS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#submit_results_df = pd.read_csv('/projects/my-public-bucket/DPS_run_boreal_biomass_quick_v2_submission_results_7_202212050255.csv')\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "\n",
    "def BUILD_TABLE_JOBSTATUS(submit_results_df):\n",
    "    import xmltodict\n",
    "    df = pd.concat([pd.DataFrame(xmltodict.parse(maap.getJobStatus(job_id).content)).transpose() for job_id in submit_results_df.job_id.to_list()])\n",
    "    return df\n",
    "\n",
    "job_status_df = BUILD_TABLE_JOBSTATUS(submit_results_df)\n",
    "#print(job_status_df.head())\n",
    "\n",
    "num_jobs = submit_results_df.shape[0]\n",
    "z = submit_results_df.merge(job_status_df, how='left', left_on='job_id',  right_on='wps:JobID')\n",
    "\n",
    "print(f'Count total jobs:\\t{num_jobs}')\n",
    "print(f\"Count pending jobs:\\t{z[z['wps:Status'] =='Accepted'].shape[0]}\")\n",
    "print(f\"Count running jobs:\\t{z[z['wps:Status'] =='Running'].shape[0]}\")\n",
    "print(f\"Count succeeded jobs:\\t{z[z['wps:Status'] =='Succeeded'].shape[0]}\")\n",
    "print(f\"Count failed jobs:\\t{z[z['wps:Status'] =='Failed'].shape[0]}\")\n",
    "print(f\"% of failed jobs:\\t{round(z[z['wps:Status'] =='Failed'].shape[0] / ( z[z['wps:Status'] =='Failed'].shape[0] + z[z['wps:Status'] =='Succeeded'].shape[0] ), 4) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "resident-architecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4881 entries, 0 to 0\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   status            4881 non-null   object        \n",
      " 1   http_status_code  4881 non-null   int64         \n",
      " 2   job_id            4881 non-null   object        \n",
      " 3   dps_num           4881 non-null   int64         \n",
      " 4   tile_num          4881 non-null   int64         \n",
      " 5   submit_time       4881 non-null   datetime64[ns]\n",
      " 6   dbs_job_hour      4881 non-null   int64         \n",
      " 7   algo_id           4881 non-null   object        \n",
      " 8   user              4881 non-null   object        \n",
      " 9   worker_type       4881 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(4), object(5)\n",
      "memory usage: 419.5+ KB\n"
     ]
    }
   ],
   "source": [
    "submit_results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "american-assignment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('wps:Result',\n",
       "              OrderedDict([('@xmlns:ows', 'http://www.opengis.net/ows/2.0'),\n",
       "                           ('@xmlns:schemaLocation',\n",
       "                            'http://schemas.opengis.net/wps/2.0/wps.xsd'),\n",
       "                           ('@xmlns:wps', 'http://www.opengis.net/wps/2.0'),\n",
       "                           ('@xmlns:xsi',\n",
       "                            'http://www.w3.org/2001/XMLSchema-instance'),\n",
       "                           ('wps:JobID',\n",
       "                            '5e4fd861-5d92-4e5d-b5b8-1d4e8e6dd463'),\n",
       "                           ('wps:Output',\n",
       "                            OrderedDict([('@id',\n",
       "                                          'output-2022-05-06T21:13:23.330599'),\n",
       "                                         ('wps:Data',\n",
       "                                          ['http://maap-ops-workspace.s3-website-us-west-2.amazonaws.com/lduncanson/dps_output/run_boreal_biomass_v4_ubuntu/master/2022/05/06/21/13/23/330599',\n",
       "                                           's3://s3.us-west-2.amazonaws.com:80/maap-ops-workspace/lduncanson/dps_output/run_boreal_biomass_v4_ubuntu/master/2022/05/06/21/13/23/330599',\n",
       "                                           'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/lduncanson/dps_output/run_boreal_biomass_v4_ubuntu/master/2022/05/06/21/13/23/330599/?region=us-east-1&tab=overview'])]))]))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Succeeded'].iloc[0].job_id).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "missing-pillow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wps:Result': {'@xmlns:ows': 'http://www.opengis.net/ows/2.0',\n",
       "  '@xmlns:schemaLocation': 'http://schemas.opengis.net/wps/2.0/wps.xsd',\n",
       "  '@xmlns:wps': 'http://www.opengis.net/wps/2.0',\n",
       "  '@xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
       "  'wps:JobID': '167d1e36-6d6e-4d4c-9549-b02dae4d038e',\n",
       "  'wps:Output': {'@id': 'traceback',\n",
       "   'wps:Data': 'Traceback (most recent call last):\\n  File \"/home/ops/verdi/ops/hysds-0.3.11/hysds/job_worker.py\", line 1085, in run_job\\n    monitoredRunner.join()\\n  File \"/home/ops/verdi/lib/python3.7/site-packages/billiard/process.py\", line 148, in join\\n    res = self._popen.wait(timeout)\\n  File \"/home/ops/verdi/lib/python3.7/site-packages/billiard/popen_fork.py\", line 57, in wait\\n    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\\n  File \"/home/ops/verdi/lib/python3.7/site-packages/billiard/popen_fork.py\", line 33, in poll\\n    pid, sts = os.waitpid(self.pid, flag)\\n  File \"/home/ops/verdi/lib/python3.7/site-packages/billiard/pool.py\", line 229, in soft_timeout_sighandler\\n    raise SoftTimeLimitExceeded()\\nbilliard.exceptions.SoftTimeLimitExceeded: SoftTimeLimitExceeded()'}}}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Failed'].iloc[20].job_id).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-library",
   "metadata": {},
   "source": [
    "## After successful DPS run, create AGB tindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "aging-poverty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "Building a list of tiles:\n",
      "MAAP version:\t\tht_boreal_2022_rh_noground_v1\n",
      "Type:\t\tAGB\n",
      "Year:\t\t2022\n",
      "Month:\t\t['12']\n",
      "Days:\t\t1-31\n",
      "\n",
      "Output dir:  /projects/my-public-bucket/DPS_tile_lists/fall2022/\n",
      "                                             s3_path  ...                                   file\n",
      "0  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  boreal_ht_202212071670436347_0726.tif\n",
      "1  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  boreal_ht_202212071670436654_0639.tif\n",
      "2  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  boreal_ht_202212071670436766_0727.tif\n",
      "3  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  boreal_ht_202212071670436807_0405.tif\n",
      "4  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  boreal_ht_202212071670436846_0373.tif\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1597: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "/projects/icesat2_boreal/lib/build_tindex_master.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tile_num'] = df['tile_num'].astype(str).astype(int)\n",
      "# of duplicate tiles: 6\n",
      "Final # of tiles: 4767\n",
      "df shape :                                                 s3_path  ... tile_num\n",
      "4772  s3://maap-ops-workspace/lduncanson/dps_output/...  ...      196\n",
      "4771  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     1963\n",
      "4770  s3://maap-ops-workspace/lduncanson/dps_output/...  ...      169\n",
      "4769  s3://maap-ops-workspace/lduncanson/dps_output/...  ...      435\n",
      "4768  s3://maap-ops-workspace/lduncanson/dps_output/...  ...       97\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "Writing tindex master csv: /projects/my-public-bucket/DPS_tile_lists/fall2022/AGB_tindex_master.csv\n"
     ]
    }
   ],
   "source": [
    "## create AGB tindex \n",
    "month_dir_str = '12'\n",
    "alg_name_str = 'run_boreal_biomass_quick_v2_ubuntu'\n",
    "maap_version_str = 'ht_boreal_2022_rh_noground_v1'\n",
    "#index_out_dir = os.path.join('/projects/my-private-bucket/dps_output/run_boreal_biomass_qiuck_v2_ubuntu/map_boreal_2022_v3/2022/', month_dir_str)\n",
    "index_out_dir = '/projects/my-public-bucket/DPS_tile_lists/fall2022/'\n",
    "!python /projects/icesat2_boreal/lib/build_tindex_master.py -t 'AGB' -alg_name $alg_name_str --maap_version $maap_version_str -y 2022 -m $month_dir_str -o $index_out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "matched-proportion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tiles to run: 113\n",
      " [2059, 2060, 2571, 3598, 1560, 1050, 35357, 2102, 59, 65, 1604, 2629, 3146, 4176, 1105, 2129, 34899, 34900, 2644, 4183, 41052, 2652, 2146, 2149, 106, 4204, 621, 36468, 3708, 124, 127, 34446, 3737, 4250, 159, 4256, 2214, 1707, 174, 3246, 36020, 3252, 4280, 186, 3262, 25790, 1216, 2243, 2250, 2251, 2771, 4307, 2260, 26840, 3802, 3294, 27364, 741, 230, 3305, 27374, 1270, 27384, 42745, 4344, 4346, 3840, 33033, 1295, 786, 2834, 3361, 4396, 3889, 3895, 1850, 4411, 2882, 2887, 2895, 2907, 348, 2912, 3941, 876, 4464, 34673, 3454, 3460, 1930, 4510, 34210, 419, 4007, 25512, 2483, 4534, 3027, 471, 3547, 41437, 2527, 2529, 2018, 3043, 485, 27114, 493, 2032, 27635, 4083, 505, 23551]\n"
     ]
    }
   ],
   "source": [
    "TEST_DPS  = True\n",
    "\n",
    "if TEST_DPS:\n",
    "    \n",
    "    if True:\n",
    "        #!python /projects/icesat2_boreal/lib/build_tindex_master_v2.py -t AGB -y 2022 -m $month_dir_str -o $index_out_dir\n",
    "        t = pd.read_csv(os.path.join(index_out_dir,'AGB_tindex_master.csv'))\n",
    "        COMPLETED_TILES = t.tile_num.to_list()\n",
    "        NEED_TILES = list(set(DPS_INPUT_TILE_NUM_LIST) - set(COMPLETED_TILES))\n",
    "\n",
    "        #print(NEED_TILES)\n",
    "        DPS_INPUT_TILE_NUM_LIST = NEED_TILES\n",
    "    \n",
    "else:\n",
    "    DPS_INPUT_TILE_NUM_LIST = INPUT_TILE_NUM_LIST\n",
    "   \n",
    "\n",
    "#DPS_INPUT_TILE_NUM_LIST = [248, 273, 272, 271, 324]\n",
    "print(f\"# of tiles to run: {len(DPS_INPUT_TILE_NUM_LIST)}\\n\", DPS_INPUT_TILE_NUM_LIST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-shaft",
   "metadata": {},
   "source": [
    "## Re-run Failed Tiles in DPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exclusive-worship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input tiles for DPS: 219\n",
      "DPS run #: 1\t| tile num: 1539\t| submit status: success\t| job id: 24a8117e-1e63-4762-8007-a71545ae36e2\n",
      "DPS run #: 2\t| tile num: 3078\t| submit status: success\t| job id: ba3efa1c-f4c9-4f86-80a0-e17b5a1d2c07\n",
      "DPS run #: 25\t| tile num: 3633\t| submit status: success\t| job id: a5a7b136-c7bc-4ddd-a28f-aca8a01f96b9\n",
      "DPS run #: 50\t| tile num: 3178\t| submit status: success\t| job id: 9a772e83-2864-495b-8758-f79780c04abf\n",
      "DPS run #: 100\t| tile num: 2265\t| submit status: success\t| job id: 695d01d8-d6dd-4bab-922f-f0dd2e15e30f\n",
      "Current time:\t202211210208\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CompressionOptions' from 'pandas._typing' (/opt/conda/lib/python3.7/site-packages/pandas/_typing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[1;32m   3143\u001b[0m         \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mRender\u001b[0m \u001b[0ma\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0man\u001b[0m \u001b[0mHTML\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3145\u001b[0;31m         \u001b[0mExamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3146\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3147\u001b[0m         >>> df = pd.DataFrame(dict(name=['Raphael', 'Donatello'],\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwriters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlibwriters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mCompressionOptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mFilePathOrBuffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CompressionOptions' from 'pandas._typing' (/opt/conda/lib/python3.7/site-packages/pandas/_typing.py)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(f\"# of input tiles for DPS: {len_input_list}\")\n",
    "\n",
    "for i, INPUT_TILE_NUM in enumerate(DPS_INPUT_TILE_NUM_LIST):\n",
    "\n",
    "    DPS_num = i+1\n",
    "    IDENTIFIER = 'run_boreal_biomass_quick_v2'\n",
    "    ALGO_ID = f'{IDENTIFIER}_ubuntu'\n",
    "    USER = 'lduncanson'\n",
    "    WORKER_TYPE = 'maap-dps-worker-16gb'\n",
    "    \n",
    "    # Maybe we're using an input tile list that isnt built directly from an ATL08_filt CSV\n",
    "    # so, check if we have filtered ATL08 for this tile:\n",
    "    if INPUT_TILE_NUM in atl08_filt_tindex_master['tile_num'].to_list():\n",
    "        \n",
    "        # Get the s3 paths of the corresponding input filenames with an input tile_num\n",
    "        in_atl08_https =  atl08_filt_tindex_master['https'].loc[atl08_filt_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_topo_https = topo_tindex_master['https'].loc[topo_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_landsat_https = landsat_tindex_master['https'].loc[landsat_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    "        in_lc_https = lc_tindex_master['https'].loc[lc_tindex_master['tile_num'] == INPUT_TILE_NUM].tolist()[0]\n",
    " \n",
    "    else:\n",
    "        print(f\"No filtered ATL08 for tile {INPUT_TILE_NUM}. Moving on..\" )\n",
    "        continue\n",
    "    \n",
    "    # iters is the number of iterations of RH model fits; set to low (2-5) to test, 30-50 for production\n",
    "    # 30 yields run times ~4 hours\n",
    "        \n",
    "    # ppside is the number of subtile splits. 2=4 subtiles, 3=9 subtiles\n",
    "    # increasing ppside reduces memory, increases length to completion\n",
    "    \n",
    "    # minDOY is the min possible DOY to search for tile training data. The default is set to May 1\n",
    "    # If sufficient data are found in growing season, minDOY will not be used\n",
    "    \n",
    "    # maxDOY is the max possible DOY to search for tile training data. The default is Sept 30.\n",
    "    \n",
    "    # max_sol_el is the max solar elevation possible to search. The default is 0\n",
    "    \n",
    "    # expand_training as TRUE sets iterative searching for training data up to min_n, starting with solar_el<0 and growing season and then:\n",
    "    # searches solar_el up to max_sol_el then\n",
    "    # searches iteratively one month later after growing season until maxDOY then\n",
    "    # searches iteratively one month earlier before growing season until minDOY\n",
    "    # if expand_training=FALSE the growing season sol_el<0 will be taken, and if none, no data will return\n",
    "    \n",
    "    # min_n is the tile training number desired, that expand_training will search until it is fulfilled (or the max available)\n",
    "    \n",
    "    #local_train_perc is the percent tile training data vs. boreal-wide training data. 100 = all local, 0 = all boreal wide\n",
    "    \n",
    "    \n",
    "    in_param_dict = {\n",
    "                                    'in_atl08_fn': 's3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/fall2022/ATL08_filt_tindex_master.csv',\n",
    "                                    #'in_atl08_fn': f\"input/{os.path.basename(in_atl08_https)}\",\n",
    "                                    'in_topo_fn': f\"input/{os.path.basename(in_topo_https)}\",\n",
    "                                    'in_landsat_fn': f\"input/{os.path.basename(in_landsat_https)}\",\n",
    "                                    'in_lc_fn': f\"input/{os.path.basename(in_lc_https)}\",\n",
    "                                    'in_atl08_fn_url': in_atl08_https,\n",
    "                                    'in_topo_fn_url': in_topo_https,\n",
    "                                    'in_landsat_fn_url': in_landsat_https,\n",
    "                                    'in_lc_fn_url': in_lc_https,\n",
    "                                    'DO_SLOPE_VALID_MASK': 'TRUE',\n",
    "                                    'in_atl08_sample_fn':f\"input/{os.path.basename(train_data_https)}\",\n",
    "                                    'in_atl08_sample_url':train_data_https,\n",
    "                                    'in_tile_num': INPUT_TILE_NUM,\n",
    "                                    'in_tile_fn_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "                                    'in_tile_fn': 'input/boreal_tiles_v003.gpkg',\n",
    "                                    'in_tile_field': 'tile_num',\n",
    "                                    'iters': 1,\n",
    "                                    'ppside': 2,\n",
    "                                    'minDOY':130,\n",
    "                                    'maxDOY':250,\n",
    "                                    'max_sol_el':5,\n",
    "                                    'expand_training': 'TRUE',\n",
    "                                    'local_train_perc': 100,\n",
    "                                    'min_n': 5000, \n",
    "                                    'boreal_vect': 'https://maap-ops-workspace.s3.us-west-2.amazonaws.com/shared/nathanmthomas/analyze_agb/input_zones/wwf_circumboreal_Dissolve_reprj.geojson',\n",
    "                                    'boreal_vect_fn': 'input/wwf_circumboreal_Dissolve_reprj.geojson'\n",
    "                }\n",
    "    \n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version='map_boreal_2022_v3',\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_param_dict\n",
    "        )\n",
    "\n",
    "    # Build a dataframe of submission details\n",
    "    submit_result['dps_num'] = DPS_num\n",
    "    submit_result['tile_num'] = INPUT_TILE_NUM\n",
    "    submit_result['submit_time'] = datetime.datetime.now()\n",
    "    submit_result['dbs_job_hour'] =datetime.datetime.now().hour\n",
    "    submit_result['algo_id'] = ALGO_ID\n",
    "    submit_result['user'] = USER\n",
    "    submit_result['worker_type'] = WORKER_TYPE\n",
    "\n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(pd.DataFrame([submit_result]))\n",
    "    \n",
    "    if DPS_num in [1, 2, 25, 50, 100, 500, 1000, 1500, 2000, 3000, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| tile num: {INPUT_TILE_NUM}\\t| submit status: {submit_result['status']}\\t| job id: {submit_result['job_id']}\") \n",
    "        #local_path\n",
    "\n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "print(f\"Current time:\\t{nowtime}\")\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/DPS_{IDENTIFIER}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-municipality",
   "metadata": {},
   "source": [
    "## Re-Assess Re-submission\n",
    "Continue with last few steps until all jobs successful: create tindex, create new tile list, re-run missing tiles in DPS, re-assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "promising-beads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count total jobs:\t219\n",
      "Count pending jobs:\t0\n",
      "Count running jobs:\t70\n",
      "Count succeeded jobs:\t15\n",
      "Count failed jobs:\t134\n",
      "% of failed jobs:\t89.92999999999999\n",
      "CPU times: user 2.24 s, sys: 197 ms, total: 2.44 s\n",
      "Wall time: 5.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#submit_results_df = pd.read_csv('/projects/my-public-bucket/DPS_run_boreal_biomass_quick_v2_submission_results_2494_202211132259.csv')\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "\n",
    "def BUILD_TABLE_JOBSTATUS(submit_results_df):\n",
    "    import xmltodict\n",
    "    df = pd.concat([pd.DataFrame(xmltodict.parse(maap.getJobStatus(job_id).content)).transpose() for job_id in submit_results_df.job_id.to_list()])\n",
    "    return df\n",
    "\n",
    "job_status_df = BUILD_TABLE_JOBSTATUS(submit_results_df)\n",
    "#print(job_status_df.head())\n",
    "\n",
    "num_jobs = submit_results_df.shape[0]\n",
    "z = submit_results_df.merge(job_status_df, how='left', left_on='job_id',  right_on='wps:JobID')\n",
    "\n",
    "print(f'Count total jobs:\\t{num_jobs}')\n",
    "print(f\"Count pending jobs:\\t{z[z['wps:Status'] =='Accepted'].shape[0]}\")\n",
    "print(f\"Count running jobs:\\t{z[z['wps:Status'] =='Running'].shape[0]}\")\n",
    "print(f\"Count succeeded jobs:\\t{z[z['wps:Status'] =='Succeeded'].shape[0]}\")\n",
    "print(f\"Count failed jobs:\\t{z[z['wps:Status'] =='Failed'].shape[0]}\")\n",
    "print(f\"% of failed jobs:\\t{round(z[z['wps:Status'] =='Failed'].shape[0] / ( z[z['wps:Status'] =='Failed'].shape[0] + z[z['wps:Status'] =='Succeeded'].shape[0] ), 4) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-sixth",
   "metadata": {},
   "source": [
    "## Get other lists just of missing tiles (various ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "patent-alberta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "need_tindex_master = pd.read_csv('/projects/my-public-bucket/DPS_tile_lists/Need_AGB_tindex_master.csv')\n",
    "print(len(need_tindex_master))\n",
    "\n",
    "INPUT_TILE_NUM_LIST = need_tindex_master.tile_num.tolist()\n",
    "\n",
    "# Remove duplicate tile nums\n",
    "INPUT_TILE_NUM_LIST = list(set(INPUT_TILE_NUM_LIST))\n",
    "\n",
    "print(len(INPUT_TILE_NUM_LIST))\n",
    "\n",
    "INPUT_TILE_NUM_LIST_NEED = INPUT_TILE_NUM_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "impressive-minutes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_tiles = [4304, 4305, 4221, 4220, 1785, 1718, 1720, 1661, 1257, 1318, 1317]\n",
    "werid_tiles = [1255, 1196, 949, 1062, 1063, 1005, 950, 1004]\n",
    "INPUT_TILE_NUM_LIST = weird_tiles\n",
    "len(INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strong-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "[978, 988, 978, 988, 1263, 1261, 1264, 794, 2932, 3190, 3012, 3014, 3360, 3017, 378, 765, 812, 821, 764, 861, 1302, 1308, 1469, 2894, 2883, 2965, 2976, 3327, 3321, 3510, 3509, 4372, 4440, 4477, 2994, 1406, 2840, 411, 380, 3335, 2495, 4403, 2906, 2907, 2814]\n"
     ]
    }
   ],
   "source": [
    "# Get all boreal tiles\n",
    "ATL08_filt_tindex_master =   pd.read_csv('s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_filt_tindex_master.csv')\n",
    "boreal_tile_index_path = '/projects/shared-buckets/nathanmthomas/boreal_grid_albers90k_gpkg.gpkg'\n",
    "boreal_tile_index = geopandas.read_file(boreal_tile_index_path)\n",
    "boreal_tile_index.rename(columns={\"layer\":\"tile_num\"}, inplace=True)\n",
    "boreal_tile_index[\"tile_num\"] = boreal_tile_index[\"tile_num\"].astype(int)\n",
    "\n",
    "bad_tiles = [3540,3634,3728,3823,3916,4004] #Dropping the tiles near antimeridian that reproject poorly.\n",
    "select_needs = [3360,2994,3190,2840,3012,3014,3017,2932,1261,1263,1264,988,978,794, 380,378,411,821,861,\n",
    "                812,765,764,1308,1302,1469,1406,2495,2883,2965,3321,3509,3510,3327,3335,2976,2906,2907,2894,2814,4253,4293,4403,4440,4408,4372,4477,3986]\n",
    "\n",
    "# Remove bad tiles\n",
    "boreal_tile_index = boreal_tile_index[~boreal_tile_index['tile_num'].isin(bad_tiles)]\n",
    "    \n",
    "#print(boreal_tile_index.head())\n",
    "tile_matches_select_needs = boreal_tile_index.merge(ATL08_filt_tindex_master[ATL08_filt_tindex_master['tile_num'].isin(select_needs)], how='right', on='tile_num')\n",
    "print(len(tile_matches_select_needs))\n",
    "select_needs_filt = print([t for t in tile_matches_select_needs.tile_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "local-majority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "4\n",
      "[3986 4253 4293 4408]\n",
      "70\n",
      "[  65  106  127  250  361  417  493  566  568  618  741  753  786  798\n",
      "  801  830  848  876  877  927 1273 1287 1318 1402 1438 1439 1440 1604\n",
      " 1848 1850 1852 1936 2425 2426 2952 3124 3355 3540 3542 3634 3728 3823\n",
      " 3845 3846 3895 3916 4004 4007 4018 4083 4176 4204 4250 4256 4307 4325\n",
      " 4342 4395 4417 4444 4450 4451 4463 4467 4468 4483 4499 4510 4527 4534]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select_needs_filt = [978, 988, 978, 988, 1263, 1261, 1264, 794, 2932, 3190, 3012, 3014, 3360, 3017, 378, 765, 812, 821, 764, 861, 1302, 1308, 1469, 2894, 2883, 2965, 2976, 3327, 3321, 3510, 3509, 4372, 4440, 4477, 2994, 1406, 2840, 411, 380, 3335, 2495, 4403, 2906, 2907, 2814]\n",
    "\n",
    "print(len(select_needs_filt))\n",
    "\n",
    "import numpy as np\n",
    "tile_nums_missing_but_wont_run = np.setdiff1d(select_needs, select_needs_filt)\n",
    "print(len(tile_nums_missing_but_wont_run))\n",
    "print(tile_nums_missing_but_wont_run)\n",
    "\n",
    "tile_nums_missing_periphery = np.setdiff1d(INPUT_TILE_NUM_LIST_NEED, select_needs_filt)\n",
    "print(len(tile_nums_missing_periphery))\n",
    "print(tile_nums_missing_periphery)\n",
    "INPUT_TILE_NUM_LIST = tile_nums_missing_periphery\n",
    "len(INPUT_TILE_NUM_LIST)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "balanced-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TILE_NUM_LIST = [4440,4372,4477]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-bandwidth",
   "metadata": {},
   "source": [
    "## Send output cog.tif tiles somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "controversial-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "out_files = glob.glob(\"/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/**/*cog.tif\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "civic-costume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/02/23/49/35/272239/boreal_agb_20220202_0200_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/01/42/37/290043/boreal_agb_20220203_0224_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/01/52/46/430279/boreal_agb_20220203_0387_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/02/21/50/348844/boreal_agb_20220203_0386_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/03/01/42/237812/boreal_agb_20220203_0199_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/03/06/47/336472/boreal_agb_20220203_0223_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/03/08/52/152954/boreal_agb_20220203_0042_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/06/47/53/128079/boreal_agb_20220203_0111_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/06/59/17/037689/boreal_agb_20220203_0112_cog.tif', '/projects/my-private-bucket/dps_output/run_boreal_biomass_ubuntu/master/2022/02/baseline_run/03/07/25/06/085526/boreal_agb_20220203_0049_cog.tif']\n"
     ]
    }
   ],
   "source": [
    "print(out_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUBDIR = 'add_solar'\n",
    "OUTDIR = f'/projects/my-private-bucket/dps_output/run_boreal_biomass_v2_ubuntu/master/2022/{TEST_SUBDIR}'\n",
    "!python /projects/icesat2_boreal/lib/build_tindex_master.py -y 2022 -m $TEST_SUBDIR -o $OUTDIR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
