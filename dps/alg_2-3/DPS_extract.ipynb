{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "square-rescue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.7/site-packages (0.12.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host='api.ops.maap-project.org')\n",
    "!pip install xmltodict\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-petersburg",
   "metadata": {},
   "source": [
    "# Launch DPS for extract_filter_atl08.py\n",
    "using the rebinned 30m h5 granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cloudy-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import os, glob\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fifteen-journalism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/icesat2_boreal/dps/alg_2-3\n"
     ]
    }
   ],
   "source": [
    "curr_dir = wk_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "print(curr_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-source",
   "metadata": {},
   "source": [
    "## Test extract_filter_atl08.py code on Test Data\n",
    "We are running extract_filter_atl08.py (but *FILTERING* is turned off):\n",
    "\n",
    "python /projects/icesat2_boreal/lib/extract_filter_atl08.py -o \"/projects/test_data/test_data_30m\" -i \"/projects/my-private-bucket/dps_output/run_rebinning_ubuntu/master/2022/02/25/18/51/17/542274/ATL08_30m_20190721220156_03640403_005_01.h5\" --no-filter-qual --do_30m\n",
    "\n",
    "python /projects/icesat2_boreal/lib/extract_filter_atl08.py -o \"/projects/test_data/test_data_30m\" -i \"https://maap-ops-workspace.s3.amazonaws.com.com/lduncanson/dps_output/run_rebinning_ubuntu/master/2022/02/25/18/51/17/542274/ATL08_30m_20190721220156_03640403_005_01.h5\" --no-filter-qual --do_30m\n",
    "\n",
    "python lib/extract_filter_atl08.py -i \"path/to/h5file\" -o \"path/of/out/dir\" --no-filter-qual --do_30m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-fifth",
   "metadata": {},
   "source": [
    "### Build the input DPS list of granules - s3fs implementation\n",
    "by finding h5 granules that dont yet have csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "color-oklahoma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching extracted csv to ATL08 30m h5 granules..\n",
      "Finding the null matches (the granules that still need extracted csvs...\n",
      "# of extractions still needed:\t12283\n",
      "\n",
      "CPU times: user 1min 5s, sys: 653 ms, total: 1min 6s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem()\n",
    "bucket = \"s3://maap-ops-workspace\"\n",
    "searchkey_h5_list = [f'lduncanson/dps_output/run_rebinning_ubuntu/master/2022/03/{DAY}/**/*.h5' for DAY in ['03','04','05']]\n",
    "searchkey_csv_list = [f'lduncanson/dps_output/run_extract_filter_atl08_ubuntu/master/2022/03/{DAY}/**/*.csv' for DAY in ['06','07','08', '09', '10','11']]\n",
    "\n",
    "# Concat list of lists to data frame\n",
    "atl0830m_h5_path_df = pd.concat([pd.DataFrame(s3.glob(os.path.join(bucket, searchkey)), columns=['maap_path']) for searchkey in searchkey_h5_list])\n",
    "atl0830m_csv_path_df = pd.concat([pd.DataFrame(s3.glob(os.path.join(bucket, searchkey)), columns=['maap_path']) for searchkey in searchkey_csv_list])\n",
    "\n",
    "#Convert data frame to list\n",
    "ATL08_h5_GRANULE_LIST = [i.replace(\"maap-ops-workspace\", \"https://maap-ops-workspace.s3.amazonaws.com\") for i in atl0830m_h5_path_df.maap_path.to_list()]\n",
    "ATL08_csv_GRANULE_LIST  = [i.replace(\"maap-ops-workspace\", \"https://maap-ops-workspace.s3.amazonaws.com\") for i in atl0830m_csv_path_df.maap_path.to_list()]\n",
    "\n",
    "grans = [os.path.split(i)[-1].replace('.h5','') for i in atl0830m_h5_path_df.maap_path.to_list()]\n",
    "ATL08_h5_GRANULE_df = pd.DataFrame({'h5_path': ATL08_h5_GRANULE_LIST, 'granule_name': grans})\n",
    "\n",
    "csvs  = [os.path.split(i)[-1].replace('_30m.csv','') for i in atl0830m_csv_path_df.maap_path.to_list()]\n",
    "ATL08_csv_GRANULE_df = pd.DataFrame({'csv_path': ATL08_csv_GRANULE_LIST, 'granule_name': csvs})\n",
    "\n",
    "print('Matching extracted csv to ATL08 30m h5 granules..')\n",
    "merged = ATL08_h5_GRANULE_df.merge(ATL08_csv_GRANULE_df, how='left', on='granule_name')\n",
    "\n",
    "print('Finding the null matches (the granules that still need extracted csvs...')\n",
    "INPUT_ATL08_GRANULE_LIST = merged[merged.csv_path.isnull()].h5_path.to_list()\n",
    "print(f'# of extractions still needed:\\t{len(INPUT_ATL08_GRANULE_LIST)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "egyptian-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DPS on the 12283 SUBSET list of input\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://maap-ops-workspace.s3.amazonaws.com/lduncanson/dps_output/run_rebinning_ubuntu/master/2022/03/05/04/47/47/547100/ATL08_30m_20210715133745_03351202_005_01.h5'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DPS  = False\n",
    "\n",
    "if TEST_DPS:\n",
    "    print('Running DPS on a SUBSET list of input')\n",
    "    DPS_INPUT_ATL08_GRANULE_LIST = INPUT_ATL08_GRANULE_LIST[-100:]\n",
    "else:\n",
    "    LIST_SIZE = 12283\n",
    "    print(f'Running DPS on the {LIST_SIZE} SUBSET list of input')\n",
    "    DPS_INPUT_ATL08_GRANULE_LIST = INPUT_ATL08_GRANULE_LIST[0:LIST_SIZE]\n",
    "    \n",
    "DPS_INPUT_ATL08_GRANULE_LIST[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-worthy",
   "metadata": {},
   "source": [
    "## Run a DPS job across the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "civil-sterling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPS run #: 1\t| granule name: ATL08_30m_20181014051811_02380106_005_01.h5\t| submit status: success\t| job id: 14300080-dbb9-48f6-b09a-b2da43dd71ef\n",
      "DPS run #: 100\t| granule name: ATL08_30m_20181206074142_10490105_005_01.h5\t| submit status: success\t| job id: 5ca5820c-fee4-40bd-a0bc-8d007707db27\n",
      "CPU times: user 1.2 s, sys: 102 ms, total: 1.31 s\n",
      "Wall time: 32.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>http_status_code</th>\n",
       "      <th>job_id</th>\n",
       "      <th>dps_num</th>\n",
       "      <th>tile_num</th>\n",
       "      <th>submit_time</th>\n",
       "      <th>dbs_job_hour</th>\n",
       "      <th>algo_id</th>\n",
       "      <th>user</th>\n",
       "      <th>worker_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>14300080-dbb9-48f6-b09a-b2da43dd71ef</td>\n",
       "      <td>1</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-03-09 23:44:41.138078</td>\n",
       "      <td>23</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>cc3a1c14-5814-4b7f-8767-02c45976debf</td>\n",
       "      <td>2</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-03-09 23:44:41.303006</td>\n",
       "      <td>23</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>dc77ede4-ceee-4991-8830-a525cb3fa648</td>\n",
       "      <td>3</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-03-09 23:44:49.258201</td>\n",
       "      <td>23</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>7d3be3a7-cc97-41dd-8bd5-0476571558c0</td>\n",
       "      <td>4</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-03-09 23:44:49.335834</td>\n",
       "      <td>23</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>aa0c915b-f434-4c66-9a53-33a1dda2679d</td>\n",
       "      <td>5</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-03-09 23:44:49.456240</td>\n",
       "      <td>23</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>1e134e59-1341-4858-98eb-7656434010fb</td>\n",
       "      <td>96</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-03-09 23:45:12.320036</td>\n",
       "      <td>23</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>d9348359-a4cd-4775-8402-eb9eac1750e9</td>\n",
       "      <td>97</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-03-09 23:45:12.509542</td>\n",
       "      <td>23</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>1c42e394-d1a0-467b-a82b-b172deb7acf7</td>\n",
       "      <td>98</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-03-09 23:45:12.760632</td>\n",
       "      <td>23</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>a2f41b23-50b7-4384-8404-84f3926c0b60</td>\n",
       "      <td>99</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-03-09 23:45:13.022425</td>\n",
       "      <td>23</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>5ca5820c-fee4-40bd-a0bc-8d007707db27</td>\n",
       "      <td>100</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-03-09 23:45:13.268402</td>\n",
       "      <td>23</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     status  http_status_code                                job_id  dps_num  \\\n",
       "0   success               200  14300080-dbb9-48f6-b09a-b2da43dd71ef        1   \n",
       "0   success               200  cc3a1c14-5814-4b7f-8767-02c45976debf        2   \n",
       "0   success               200  dc77ede4-ceee-4991-8830-a525cb3fa648        3   \n",
       "0   success               200  7d3be3a7-cc97-41dd-8bd5-0476571558c0        4   \n",
       "0   success               200  aa0c915b-f434-4c66-9a53-33a1dda2679d        5   \n",
       "..      ...               ...                                   ...      ...   \n",
       "0   success               200  1e134e59-1341-4858-98eb-7656434010fb       96   \n",
       "0   success               200  d9348359-a4cd-4775-8402-eb9eac1750e9       97   \n",
       "0   success               200  1c42e394-d1a0-467b-a82b-b172deb7acf7       98   \n",
       "0   success               200  a2f41b23-50b7-4384-8404-84f3926c0b60       99   \n",
       "0   success               200  5ca5820c-fee4-40bd-a0bc-8d007707db27      100   \n",
       "\n",
       "                                             tile_num  \\\n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "..                                                ...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "\n",
       "                  submit_time  dbs_job_hour                          algo_id  \\\n",
       "0  2022-03-09 23:44:41.138078            23  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-03-09 23:44:41.303006            23  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-03-09 23:44:49.258201            23  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-03-09 23:44:49.335834            23  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-03-09 23:44:49.456240            23  run_extract_filter_atl08_ubuntu   \n",
       "..                        ...           ...                              ...   \n",
       "0  2022-03-09 23:45:12.320036            23  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-03-09 23:45:12.509542            23  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-03-09 23:45:12.760632            23  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-03-09 23:45:13.022425            23  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-03-09 23:45:13.268402            23  run_extract_filter_atl08_ubuntu   \n",
       "\n",
       "          user          worker_type  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "..         ...                  ...  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_ATL08_GRANULE_LIST)\n",
    "\n",
    "for i, INPUT_ATL08_GRANULE in enumerate(DPS_INPUT_ATL08_GRANULE_LIST):\n",
    "\n",
    "    DPS_num = i+1\n",
    "    IDENTIFIER = 'run_extract_filter_atl08'\n",
    "    ALGO_ID = f'{IDENTIFIER}_ubuntu'\n",
    "    USER = 'lduncanson'\n",
    "    WORKER_TYPE = 'maap-dps-worker-8gb'\n",
    "    \n",
    "    in_param_dict = {\n",
    "                        'input_file': INPUT_ATL08_GRANULE\n",
    "                    }\n",
    "\n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version='master',\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_param_dict\n",
    "        )\n",
    "    \n",
    "    # Build a dataframe of submission details\n",
    "    submit_result['dps_num'] = DPS_num\n",
    "    submit_result['tile_num'] = INPUT_ATL08_GRANULE\n",
    "    submit_result['submit_time'] = datetime.datetime.now()\n",
    "    submit_result['dbs_job_hour'] = datetime.datetime.now().hour\n",
    "    submit_result['algo_id'] = ALGO_ID\n",
    "    submit_result['user'] = USER\n",
    "    submit_result['worker_type'] = WORKER_TYPE\n",
    "    \n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(pd.DataFrame([submit_result]))\n",
    "    \n",
    "    if DPS_num in [1, 100, 500, 1000, 1500, 2000, 3000, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000, len_input_list]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| granule name: {os.path.basename(INPUT_ATL08_GRANULE)}\\t| submit status: {submit_result['status']}\\t| job id: {submit_result['job_id']}\") \n",
    "\n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/DPS_{IDENTIFIER}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-territory",
   "metadata": {},
   "source": [
    "After almost any DPS job, you have to assess what succeeded and failed. This involves:\n",
    "1. building a table of job status based on job ids captured in the job_results_df from the DPS run chunk (this takes 40 mins for ~47k jobs) --> this tells you how many jobs failed\n",
    "2. merging the job status table with the job results df --> this tells you which specific granules (or tile nums) failed\n",
    "3. building another input list of granules for a follow-up DPS\n",
    "## Assess DPS results\n",
    "Build a table of job status based on job id - how many jobs failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "continued-invitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count total jobs:\t100\n",
      "Count pending jobs:\t0\n",
      "Count running jobs:\t39\n",
      "Count succeeded jobs:\t43\n",
      "Count failed jobs:\t18\n",
      "% of failed jobs:\t29.509999999999998\n",
      "CPU times: user 1.07 s, sys: 67.9 ms, total: 1.13 s\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def BUILD_TABLE_JOBSTATUS(submit_results_df):\n",
    "    import xmltodict\n",
    "    df = pd.concat([pd.DataFrame(xmltodict.parse(maap.getJobStatus(job_id).content)).transpose() for job_id in submit_results_df.job_id.to_list()])\n",
    "    return df\n",
    "\n",
    "job_status_df = BUILD_TABLE_JOBSTATUS(submit_results_df)\n",
    "#print(job_status_df.head())\n",
    "\n",
    "num_jobs = submit_results_df.shape[0]\n",
    "z = submit_results_df.merge(job_status_df, how='left', left_on='job_id', right_on='wps:JobID')\n",
    "\n",
    "print(f'Count total jobs:\\t{num_jobs}')\n",
    "print(f\"Count pending jobs:\\t{z[z['wps:Status'] =='Accepted'].shape[0]}\")\n",
    "print(f\"Count running jobs:\\t{z[z['wps:Status'] =='Running'].shape[0]}\")\n",
    "print(f\"Count succeeded jobs:\\t{z[z['wps:Status'] =='Succeeded'].shape[0]}\")\n",
    "print(f\"Count failed jobs:\\t{z[z['wps:Status'] =='Failed'].shape[0]}\")\n",
    "print(f\"% of failed jobs:\\t{round(z[z['wps:Status'] =='Failed'].shape[0] / ( z[z['wps:Status'] =='Failed'].shape[0] + z[z['wps:Status'] =='Succeeded'].shape[0] ), 4) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "prepared-burke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('wps:Result',\n",
       "              OrderedDict([('@xmlns:ows', 'http://www.opengis.net/ows/2.0'),\n",
       "                           ('@xmlns:schemaLocation',\n",
       "                            'http://schemas.opengis.net/wps/2.0/wps.xsd'),\n",
       "                           ('@xmlns:wps', 'http://www.opengis.net/wps/2.0'),\n",
       "                           ('@xmlns:xsi',\n",
       "                            'http://www.w3.org/2001/XMLSchema-instance'),\n",
       "                           ('wps:JobID',\n",
       "                            'f45e0863-6de1-4d69-95cb-971ff74cf9b9'),\n",
       "                           ('wps:Output',\n",
       "                            OrderedDict([('@id', 'traceback'),\n",
       "                                         ('wps:Data',\n",
       "                                          'Not a conda environment: /app/icesat2_boreal/dps/alg_2-3/run_extract_filter_atl08.sh\\n+ /app/icesat2_boreal/dps/alg_2-3/run_extract_filter_atl08.sh\\nERROR: Exception:\\nTraceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\\n    yield\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 519, in read\\n    data = self._fp.read(amt) if not fp_closed else b\"\"\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\\n    data = self.__fp.read(amt)\\n  File \"/opt/conda/lib/python3.9/http/client.py\", line 463, in read\\n    n = self.readinto(b)\\n  File \"/opt/conda/lib/python3.9/http/client.py\", line 507, in readinto\\n    n = self.fp.readinto(b)\\n  File \"/opt/conda/lib/python3.9/socket.py\", line 704, in readinto\\n    return self._sock.recv_into(b)\\n  File \"/opt/conda/lib/python3.9/ssl.py\", line 1241, in recv_into\\n    return self.read(nbytes, buffer)\\n  File \"/opt/conda/lib/python3.9/ssl.py\", line 1099, in read\\n    return self._sslobj.read(len, buffer)\\nsocket.timeout: The read operation timed out\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\\n    status = run_func(*args)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 205, in wrapper\\n    return func(self, options, args)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 339, in run\\n    requirement_set = resolver.resolve(\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 94, in resolve\\n    result = self._result = resolver.resolve(\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\\n    state = resolution.resolve(requirements, max_rounds=max_rounds)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 348, in resolve\\n    self._add_to_criteria(self.state.criteria, r, parent=None)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\\n    if not criterion.candidates:\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\\n    return bool(self._sequence)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\\n    return any(self)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\\n    return (c for c in iterator if id(c) not in self._incompatible_ids)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\\n    candidate = func()\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 215, in _make_candidate_from_link\\n    self._link_candidate_cache[link] = LinkCandidate(\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 288, in __init__\\n    super().__init__(\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\\n    self.dist = self._prepare()\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 227, in _prepare\\n    dist = self._prepare_distribution()\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 299, in _prepare_distribution\\n    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 487, in prepare_linked_requirement\\n    return self._prepare_linked_requirement(req, parallel_builds)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 532, in _prepare_linked_requirement\\n    local_file = unpack_url(\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 214, in unpack_url\\n    file = get_http_url(\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 94, in get_http_url\\n    from_path, content_type = download(link, temp_dir.path)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/network/download.py\", line 146, in __call__\\n    for chunk in chunks:\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/progress_bars.py\", line 304, in _rich_progress_bar\\n    for chunk in iterable:\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\\n    for chunk in response.raw.stream(\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 576, in stream\\n    data = self.read(amt=amt, decode_content=decode_content)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 541, in read\\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\\n  File \"/opt/conda/lib/python3.9/contextlib.py\", line 137, in __exit__\\n    self.gen.throw(typ, value, traceback)\\n  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\\n    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\\npip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host=\\'files.pythonhosted.org\\', port=443): Read timed out.\\nTraceback (most recent call last):\\n  File \"/app/icesat2_boreal/dps/alg_2-3/extract_filter_atl08.py\", line 11, in <module>\\n    import pandas as pd\\nModuleNotFoundError: No module named \\'pandas\\'\\n+ cp _stderr.txt _alt_traceback.txt')]))]))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Failed'].iloc[15].job_id).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-brooks",
   "metadata": {},
   "source": [
    "##### Pick a job and manually test - does it produce output as expected?\n",
    "if so, then AWS had some issue during the DPS: contact MAAP dev about this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "revolutionary-climate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Written by:\n",
      "\tNathan Thomas\t| @Nmt28\n",
      "\tPaul Montesano\t| paul.m.montesano@nasa.gov\n",
      "\n",
      "Min lat: 45.0\n",
      "Max lat: 75.0\n",
      "Min lon: -180.0\n",
      "Max lon: 180.0\n",
      "Month range: 6-9\n",
      "\n",
      "ATL08 granule name: \tATL08_30m_20210714235338_03261206_005_01\n",
      "Input dir: \t\t/projects/my-private-bucket/dps_output/run_rebinning_ubuntu/master/2022/03/05/04/51/06/341661\n",
      "\n",
      "Segment length: 30m\n",
      "Find src nodata value using max of h_can: \tnan\n",
      "\n",
      "Building pandas dataframe...\n",
      "Setting pandas df nodata values to np.nan for some basic eval.\n",
      "# of ATL08 obs: \t\t211006\n",
      "# of ATL08 obs (can pho.>=0): \t77997\n",
      "# of ATL08 obs (toc pho.>=0): \t77997\n",
      "# of ATL08 obs (h_can>=0): \t77905\n",
      "# of ATL08 obs (h_can<0): \t92\n",
      "Setting out pandas df nodata values: \t3.4028234663852886e+38\n",
      "Quality Filtering: \t[OFF] (do downstream)\n",
      "Geographic Filtering: \t[ON] xmin = -180.0, xmax = 180.0, ymin = 45.0, ymax = 75.0\n",
      "Creating CSV: \t\t/projects/my-public-bucket/test_output/ATL08_30m_20210714235338_03261206_005_01_30m.csv\n"
     ]
    }
   ],
   "source": [
    "f = fails_df.iloc[1].tile_num.replace('https://maap-ops-workspace.s3.amazonaws.com/lduncanson','/projects/my-private-bucket')\n",
    "!python /projects/icesat2_boreal/dps/alg_2-3/extract_filter_atl08.py --i $f --no-filter-qual --do_30m -o /projects/my-public-bucket/test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-oracle",
   "metadata": {},
   "source": [
    "### Compile granules of the fails for another DPS batch (Round2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "pursuant-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "laden-desert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000 5000 5000 280\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "DPS_INPUT_ATL08_GRANULE_LIST_1, DPS_INPUT_ATL08_GRANULE_LIST_2, DPS_INPUT_ATL08_GRANULE_LIST_3, DPS_INPUT_ATL08_GRANULE_LIST_4, DPS_INPUT_ATL08_GRANULE_LIST_5 = list(chunks(DPS_INPUT_ATL08_GRANULE_LIST, 5000))\n",
    "print(\n",
    "    len(DPS_INPUT_ATL08_GRANULE_LIST_1), len(DPS_INPUT_ATL08_GRANULE_LIST_2), len(DPS_INPUT_ATL08_GRANULE_LIST_3), len(DPS_INPUT_ATL08_GRANULE_LIST_4), len(DPS_INPUT_ATL08_GRANULE_LIST_5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-feelings",
   "metadata": {},
   "source": [
    "### Examine output of DPS - but replace glob.glob with s3.glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "failing-contributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For DPS job that returned results in hour 20, # granules that ran: 10\n"
     ]
    }
   ],
   "source": [
    "#print(f\"The data frame show you submitted {len(job_results_df)} jobs. Check the returned results to see if the total returned = total submitted...\")\n",
    "for JOB_HOUR in range(20,21):\n",
    "    returned_results_list = glob.glob(f\"/projects/my-private-bucket/dps_output/run_extract_filter_atl08_ubuntu/master/2022/03/04/{JOB_HOUR}/**/*.csv\", recursive=True)\n",
    "    print(f\"For DPS job that returned results in hour {JOB_HOUR}, # granules that ran: {len(returned_results_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "martial-effects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pandas data frame...\n"
     ]
    }
   ],
   "source": [
    "# Merge all files in the list\n",
    "print(\"Creating pandas data frame...\")\n",
    "atl08_gdf = pd.concat((pd.read_csv(f) for f in returned_results_list ), sort=False, ignore_index=True) # <--generator is (), list is []\n",
    "atl08_gdf = gpd.GeoDataFrame(atl08_gdf, geometry=gpd.points_from_xy(atl08_gdf.lon, atl08_gdf.lat), crs='epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "elder-wilderness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fid</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>dt</th>\n",
       "      <th>orb_orient</th>\n",
       "      <th>orb_num</th>\n",
       "      <th>rgt</th>\n",
       "      <th>gt</th>\n",
       "      <th>segid_beg</th>\n",
       "      <th>segid_end</th>\n",
       "      <th>...</th>\n",
       "      <th>asr</th>\n",
       "      <th>h_dif_ref</th>\n",
       "      <th>ter_flg</th>\n",
       "      <th>ph_rem_flg</th>\n",
       "      <th>dem_rem_flg</th>\n",
       "      <th>seg_wmask</th>\n",
       "      <th>lyr_flg</th>\n",
       "      <th>seg_cover</th>\n",
       "      <th>granule_name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14887</td>\n",
       "      <td>75.620664</td>\n",
       "      <td>45.000490</td>\n",
       "      <td>b'2019-04-08T02:04:27.000000Z'</td>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>150</td>\n",
       "      <td>b'gt1r'</td>\n",
       "      <td>249813.842915</td>\n",
       "      <td>249817.842915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158199</td>\n",
       "      <td>-1.342682</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>ATL08_30m_20190408015557_01500302_005_01.h5</td>\n",
       "      <td>POINT (75.62066 45.00049)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14888</td>\n",
       "      <td>75.620628</td>\n",
       "      <td>45.000758</td>\n",
       "      <td>b'2019-04-08T02:04:27.000000Z'</td>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>150</td>\n",
       "      <td>b'gt1r'</td>\n",
       "      <td>249815.336919</td>\n",
       "      <td>249819.336919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158199</td>\n",
       "      <td>-1.342682</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>ATL08_30m_20190408015557_01500302_005_01.h5</td>\n",
       "      <td>POINT (75.62063 45.00076)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14889</td>\n",
       "      <td>75.620401</td>\n",
       "      <td>45.002371</td>\n",
       "      <td>b'2019-04-08T02:04:27.000000Z'</td>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>150</td>\n",
       "      <td>b'gt1r'</td>\n",
       "      <td>249824.331298</td>\n",
       "      <td>249828.331298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158199</td>\n",
       "      <td>-1.073212</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>ATL08_30m_20190408015557_01500302_005_01.h5</td>\n",
       "      <td>POINT (75.62040 45.00237)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14890</td>\n",
       "      <td>75.620364</td>\n",
       "      <td>45.002640</td>\n",
       "      <td>b'2019-04-08T02:04:27.000000Z'</td>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>150</td>\n",
       "      <td>b'gt1r'</td>\n",
       "      <td>249825.830880</td>\n",
       "      <td>249829.830880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158199</td>\n",
       "      <td>-1.073212</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>ATL08_30m_20190408015557_01500302_005_01.h5</td>\n",
       "      <td>POINT (75.62036 45.00264)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14891</td>\n",
       "      <td>75.620291</td>\n",
       "      <td>45.003178</td>\n",
       "      <td>b'2019-04-08T02:04:27.000000Z'</td>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>150</td>\n",
       "      <td>b'gt1r'</td>\n",
       "      <td>249828.830880</td>\n",
       "      <td>249832.830880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>-1.743439</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>ATL08_30m_20190408015557_01500302_005_01.h5</td>\n",
       "      <td>POINT (75.62029 45.00318)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fid        lon        lat                              dt  orb_orient  \\\n",
       "0  14887  75.620664  45.000490  b'2019-04-08T02:04:27.000000Z'           0   \n",
       "1  14888  75.620628  45.000758  b'2019-04-08T02:04:27.000000Z'           0   \n",
       "2  14889  75.620401  45.002371  b'2019-04-08T02:04:27.000000Z'           0   \n",
       "3  14890  75.620364  45.002640  b'2019-04-08T02:04:27.000000Z'           0   \n",
       "4  14891  75.620291  45.003178  b'2019-04-08T02:04:27.000000Z'           0   \n",
       "\n",
       "   orb_num  rgt       gt      segid_beg      segid_end  ...       asr  \\\n",
       "0     3125  150  b'gt1r'  249813.842915  249817.842915  ...  0.158199   \n",
       "1     3125  150  b'gt1r'  249815.336919  249819.336919  ...  0.158199   \n",
       "2     3125  150  b'gt1r'  249824.331298  249828.331298  ...  0.158199   \n",
       "3     3125  150  b'gt1r'  249825.830880  249829.830880  ...  0.158199   \n",
       "4     3125  150  b'gt1r'  249828.830880  249832.830880  ...  0.129356   \n",
       "\n",
       "   h_dif_ref  ter_flg  ph_rem_flg  dem_rem_flg  seg_wmask  lyr_flg  seg_cover  \\\n",
       "0  -1.342682        1           0            0          0        0         48   \n",
       "1  -1.342682        1           0            0          0        0         48   \n",
       "2  -1.073212        1           0            0          0        0         42   \n",
       "3  -1.073212        1           0            0          0        0         42   \n",
       "4  -1.743439        1           0            0          0        0         24   \n",
       "\n",
       "                                  granule_name                   geometry  \n",
       "0  ATL08_30m_20190408015557_01500302_005_01.h5  POINT (75.62066 45.00049)  \n",
       "1  ATL08_30m_20190408015557_01500302_005_01.h5  POINT (75.62063 45.00076)  \n",
       "2  ATL08_30m_20190408015557_01500302_005_01.h5  POINT (75.62040 45.00237)  \n",
       "3  ATL08_30m_20190408015557_01500302_005_01.h5  POINT (75.62036 45.00264)  \n",
       "4  ATL08_30m_20190408015557_01500302_005_01.h5  POINT (75.62029 45.00318)  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atl08_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "representative-wellington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system( \"python /projects/icesat2_boreal/lib/build_tindex_master.py -t ATL08 -dps_year 2022 -m 3 --start_day '04' --outdir /projects/my-public-bucket/test_output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
