{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "innovative-placement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host='api.ops.maap-project.org')\n",
    "!pip install xmltodict\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-accountability",
   "metadata": {},
   "source": [
    "# Launch DPS for extract_filter_atl08.py\n",
    "using the rebinned 30m h5 granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amino-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py==3.1.0 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 3)) (3.1.0)\n",
      "Collecting pandas==1.2.2\n",
      "  Using cached pandas-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "Collecting pygeos==0.12.0\n",
      "  Using cached pygeos-0.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Requirement already satisfied: rtree==0.9.7 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 8)) (0.9.7)\n",
      "Requirement already satisfied: numpy==1.21.0 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 9)) (1.21.0)\n",
      "Requirement already satisfied: geopandas==0.9.0 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: rasterio==1.2.6 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (1.2.6)\n",
      "Collecting importlib_resources\n",
      "  Using cached importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
      "Collecting contextily\n",
      "  Using cached contextily-1.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "Collecting s3fs==0.3.4\n",
      "  Using cached s3fs-0.3.4-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: rio-cogeo==2.3.1 in /projects/.local/lib/python3.7/site-packages (from -r /projects/icesat2_boreal/dps/requirements_main.txt (line 22)) (2.3.1)\n",
      "Collecting rio-tiler==2.1.4\n",
      "  Using cached rio_tiler-2.1.4-py3-none-any.whl\n",
      "Collecting morecantile==2.1.4\n",
      "  Using cached morecantile-2.1.4-py3-none-any.whl\n",
      "Collecting pystac-client\n",
      "  Using cached pystac_client-0.5.1-py3-none-any.whl (29 kB)\n",
      "Collecting cachetools\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: cached-property in /projects/.local/lib/python3.7/site-packages (from h5py==3.1.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /projects/.local/lib/python3.7/site-packages (from pandas==1.2.2->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 4)) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.2.2->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: shapely>=1.6 in /projects/.local/lib/python3.7/site-packages (from geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (1.7.1)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in /projects/.local/lib/python3.7/site-packages (from geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (3.2.1)\n",
      "Requirement already satisfied: fiona>=1.8 in /projects/.local/lib/python3.7/site-packages (from geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (1.8.20)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (2021.5.30)\n",
      "Requirement already satisfied: click-plugins in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (1.1.1)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (8.0.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (0.7.2)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (1.4.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (41.4.0)\n",
      "Requirement already satisfied: affine in /projects/.local/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (2.3.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (20.3.0)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.7/site-packages/botocore-1.20.72-py3.7.egg (from s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (1.20.72)\n",
      "Requirement already satisfied: boto3>=1.9.91 in /opt/conda/lib/python3.7/site-packages/boto3-1.17.72-py3.7.egg (from s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (1.17.72)\n",
      "Requirement already satisfied: pydantic in /projects/.local/lib/python3.7/site-packages (from rio-cogeo==2.3.1->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 22)) (1.8.2)\n",
      "Collecting pystac>=0.5.3\n",
      "  Using cached pystac-1.5.0-py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: numexpr in /projects/.local/lib/python3.7/site-packages (from rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.7.3)\n",
      "Requirement already satisfied: requests in /projects/.local/lib/python3.7/site-packages (from rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.27.1)\n",
      "Requirement already satisfied: rio-color in /projects/.local/lib/python3.7/site-packages (from rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (1.0.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib_resources->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 14)) (3.4.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages/matplotlib-3.4.2-py3.7-linux-x86_64.egg (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (3.4.2)\n",
      "Requirement already satisfied: mercantile in /projects/.local/lib/python3.7/site-packages (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.2.1)\n",
      "Collecting xyzservices\n",
      "  Using cached xyzservices-2022.9.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: joblib in /projects/.local/lib/python3.7/site-packages (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.0.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages/Pillow-8.2.0-py3.7-linux-x86_64.egg (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (8.2.0)\n",
      "Requirement already satisfied: geopy in /projects/.local/lib/python3.7/site-packages (from contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (2.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages/jmespath-0.10.0-py3.7.egg (from boto3>=1.9.91->s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages/s3transfer-0.4.2-py3.7.egg (from boto3>=1.9.91->s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (0.4.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /projects/.local/lib/python3.7/site-packages (from botocore>=1.12.91->s3fs==0.3.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 17)) (1.26.7)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click>=4.0->rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (3.7.3)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (1.12.0)\n",
      "Requirement already satisfied: munch in /projects/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas==0.9.0->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from pystac>=0.5.3->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (3.7.4.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /projects/.local/lib/python3.7/site-packages (from requests->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (2.8)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio==1.2.6->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 13)) (2.4.7)\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in /projects/.local/lib/python3.7/site-packages (from geopy->contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.52)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages/cycler-0.10.0-py3.7.egg (from matplotlib->contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages/kiwisolver-1.3.1-py3.7-linux-x86_64.egg (from matplotlib->contextily->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: rio-mucho in /projects/.local/lib/python3.7/site-packages (from rio-color->rio-tiler==2.1.4->-r /projects/icesat2_boreal/dps/requirements_main.txt (line 23)) (1.0.0)\n",
      "Installing collected packages: xyzservices, pygeos, importlib_resources, fsspec, cachetools, pystac, pandas, pystac-client, s3fs, morecantile, contextily, rio-tiler\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.4\n",
      "    Uninstalling pandas-1.1.4:\n",
      "      Successfully uninstalled pandas-1.1.4\n",
      "Successfully installed cachetools-5.2.0 contextily-1.2.0 fsspec-2022.11.0 importlib_resources-5.10.0 morecantile-2.1.4 pandas-1.2.2 pygeos-0.12.0 pystac-1.5.0 pystac-client-0.5.1 rio-tiler-2.1.4 s3fs-0.3.4 xyzservices-2022.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -r /projects/icesat2_boreal/dps/requirements_main.txt\n",
    "from os import path\n",
    "import os, glob\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "# !pip install s3fs \n",
    "#!pip install pystac\n",
    "import s3fs\n",
    "import sys\n",
    "sys.path.append('/projects/icesat2_boreal/lib')\n",
    "import ExtractUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worth-general",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S3File',\n",
       " 'S3FileSystem',\n",
       " 'S3Map',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_version',\n",
       " 'core',\n",
       " 'errors',\n",
       " 'mapping',\n",
       " 'utils']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(s3fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "middle-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/icesat2_boreal/dps/alg_2-3\n"
     ]
    }
   ],
   "source": [
    "curr_dir = wk_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "print(curr_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-median",
   "metadata": {},
   "source": [
    "## Test extract_filter_atl08.py code on Test Data\n",
    "We are running extract_filter_atl08.py (but *FILTERING* is turned off):\n",
    "\n",
    "python /projects/icesat2_boreal/lib/extract_filter_atl08.py -o \"/projects/test_data/test_data_30m\" -i \"/projects/my-private-bucket/dps_output/run_rebinning_ubuntu/master/2022/02/25/18/51/17/542274/ATL08_30m_20190721220156_03640403_005_01.h5\" --no-filter-qual --do_30m\n",
    "\n",
    "python /projects/icesat2_boreal/lib/extract_filter_atl08.py -o \"/projects/test_data/test_data_30m\" -i \"https://maap-ops-workspace.s3.amazonaws.com.com/lduncanson/dps_output/run_rebinning_ubuntu/master/2022/02/25/18/51/17/542274/ATL08_30m_20190721220156_03640403_005_01.h5\" --no-filter-qual --do_30m\n",
    "\n",
    "python lib/extract_filter_atl08.py -i \"path/to/h5file\" -o \"path/of/out/dir\" --no-filter-qual --do_30m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mineral-boost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Written by:\n",
      "\tNathan Thomas\t| @Nmt28\n",
      "\tPaul Montesano\t| paul.m.montesano@nasa.gov\n",
      "\n",
      "Min lat: 30.0\n",
      "Max lat: 80.0\n",
      "Min lon: -180.0\n",
      "Max lon: 180.0\n",
      "Month range: 6-9\n",
      "\n",
      "ATL08 granule name: \tATL08_30m_20181016054510_02690103_005_01\n",
      "Input dir: \t\t/projects/my-private-bucket/dps_output/run_rebinning_ubuntu/master/2022/09/30/20/54/59/961352\n",
      "\n",
      "Segment length: 30m\n",
      "Find src nodata value using max of h_can: \tnan\n",
      "\n",
      "Building pandas dataframe...\n",
      "Setting pandas df nodata values to np.nan for some basic eval.\n",
      "# of ATL08 obs: \t\t100931\n",
      "# of ATL08 obs (can pho.>=0): \t72415\n",
      "# of ATL08 obs (toc pho.>=0): \t72415\n",
      "# of ATL08 obs (h_can>=0): \t72415\n",
      "# of ATL08 obs (h_can<0): \t0\n",
      "Setting out pandas df nodata values: \t3.4028234663852886e+38\n",
      "Quality Filtering: \t\t[ON]\n",
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "Apply the aggressive land-cover based (v3) filters updated in Jan/Feb 2022\n",
      "\n",
      "Filtering by quality\n",
      "\n",
      "Pre-filter data cleaning...\n",
      "Pandas version: 1.2.2\n",
      "\tGet beam type from orbit orientation and ground track: ['Strong' 'Weak']\n",
      "\tCast some columns to:\n",
      "\t\ttype float: ['lat', 'lon', 'h_can', 'h_te_best', 'ter_slp']\n",
      "\t\ttype integer: ['n_ca_ph', 'n_seg_ph', 'n_toc_ph', 'msw_flg', 'seg_snow']\n",
      "\tBefore quality filtering: \t\t100931 observations in the input dataframe.\n",
      "\tAfter msw_flg=0: \t\t39610 observations in the dataframe.\n",
      "\tAfter beam_type=Strong: \t\t20176 observations in the dataframe.\n",
      "\tAfter seg_snow=1: \t\t1904 observations in the dataframe.\n",
      "\tLand cover threshold dictionary: \n",
      "{0: 0, 111: 60, 113: 60, 112: 60, 114: 60, 115: 60, 116: 60, 121: 50, 123: 50, 122: 50, 124: 50, 125: 50, 126: 50, 20: 20, 30: 10, 90: 10, 100: 5, 60: 5, 40: 0, 50: 0, 70: 0, 80: 0, 200: 0}\n",
      "\tAfter land-cover specific h_can thresholds: \t\t1597 observations in the dataframe.\n",
      "\tAfter h_can_unc <5, seg_cover<32767, sol_el<5, sig_topo<2.5, h_dif_ref<25: \t\t0 observations in the dataframe.\n",
      "\tAfter month filters: 6-9\n",
      "\tAfter all quality filtering: \t\t0 observations in the output dataframe.\n",
      "\tReturning a pandas data frame.\n",
      "\tFiltered obs. for columns: ['lon', 'lat', 'rh25', 'rh50', 'rh60', 'rh70', 'rh75', 'rh80', 'rh90', 'h_can', 'h_max_can', 'h_te_best', 'granule_name', 'rh_type', 'seg_landcov', 'seg_cover', 'sol_el', 'y', 'm', 'doy']\n",
      "\tData frame shape: (0, 20) \n",
      "Geographic Filtering: \t[ON] xmin = -180.0, xmax = 180.0, ymin = 30.0, ymax = 80.0\n",
      "File is empty.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    ## NOTE: this ATL08 needs to be v005 not v004\n",
    "    #h5_fn = '/projects/test_data/test_extract_filter_atl08_30m/ATL08_30m_20181014001049_02350102_005_01.h5'\n",
    "    pn = 'my-private-bucket/dps_output/run_rebinning_ubuntu/master/2022/09/30/20/54/59/961352/ATL08_30m_20181016054510_02690103_005_01.h5'\n",
    "    #h5_fn = 'https://maap-ops-workspace.s3.amazonaws.com/lduncanson/dps_output/run_rebinning_ubuntu/master/2022/09/30/20/44/57/535236/ATL08_30m_20181015233835_02650105_005_01.h5'\n",
    "    #h5_fn = h5_fn.replace('https://maap-ops-workspace.s3.amazonaws.com/lduncanson', '/projects/my-private-bucket')\n",
    "    h5_fn = pn.replace('my-private-bucket','/projects/my-private-bucket')\n",
    "    #!python /projects/icesat2_boreal/lib/extract_filter_atl08.py -o \"/projects/test_data/test_extract_filter_atl08_30m\" -i $h5_fn --no-filter-qual --do_30m --get_gedi_rh\n",
    "    !python /projects/icesat2_boreal/lib/extract_filter_atl08.py -o \"/projects/test_data/test_extract_filter_atl08_30m\" -i $h5_fn --no-filter-qual --seg_length 30 --rh_type atl03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-sheet",
   "metadata": {},
   "source": [
    "# Register DPS algorithm\n",
    "We need to register a DPS algorithm called `run_extract_filter_atl08_ubuntu` before proceeding to the chunks below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "innovative-parker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"algorithm_name\": \"run_extract_filter_atl08\",\n",
      "  \"code_version\": \"master\",\n",
      "  \"environment_name\": \"ubuntu\",\n",
      "  \"repo_url\": \"https://repo.ops.maap-project.org/icesat2_boreal/icesat2_boreal.git\",\n",
      "  \"docker_container_url\": \"mas.maap-project.org:5000/root/ade-base-images/r:latest\",\n",
      "  \"queue\": \"maap-dps-worker-8gb\",\n",
      "  \"algorithm_description\": \"DPS to extract a csv of fields from ATL08 h5 granules\",\n",
      "  \"build_command\": \"icesat2_boreal/dps/build_command_main.sh\",\n",
      "  \"script_command\": \"icesat2_boreal/dps/alg_2-3/run_extract_filter_atl08.sh\",\n",
      "  \"disk_space\": \"10GB\",\n",
      "  \"algorithm_params\": [\n",
      "    {\n",
      "      \"field\": \"input_file\",\n",
      "      \"download\": true\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\"code\": 200, \"message\": {\"id\": \"945d4de04d1c6888733c7e32bc0ce7ea28f42ab5\", \"short_id\": \"945d4de0\", \"created_at\": \"2022-10-11T15:06:55.000+00:00\", \"parent_ids\": [\"a56bf989c0614e9cb23fd1afbb2da131de922ca2\"], \"title\": \"Registering algorithm: run_extract_filter_atl08_ubuntu\", \"message\": \"Registering algorithm: run_extract_filter_atl08_ubuntu\", \"author_name\": \"www-data\", \"author_email\": \"www-data@ip-10-46-1-136\", \"authored_date\": \"2022-10-11T15:06:55.000+00:00\", \"committer_name\": \"www-data\", \"committer_email\": \"www-data@ip-10-46-1-136\", \"committed_date\": \"2022-10-11T15:06:55.000+00:00\", \"trailers\": {}, \"web_url\": \"https://repo.ops.maap-project.org/root/register-job/-/commit/945d4de04d1c6888733c7e32bc0ce7ea28f42ab5\", \"stats\": {\"additions\": 5, \"deletions\": 5, \"total\": 10}, \"status\": \"pending\", \"project_id\": 2, \"last_pipeline\": {\"id\": 737, \"iid\": 627, \"project_id\": 2, \"sha\": \"945d4de04d1c6888733c7e32bc0ce7ea28f42ab5\", \"ref\": \"master\", \"status\": \"pending\", \"source\": \"push\", \"created_at\": \"2022-10-11T15:06:56.046Z\", \"updated_at\": \"2022-10-11T15:06:56.413Z\", \"web_url\": \"https://repo.ops.maap-project.org/root/register-job/-/pipelines/737\"}, \"job_web_url\": \"https://repo.ops.maap-project.org/root/register-job/-/jobs/884\", \"job_log_url\": \"https://repo.ops.maap-project.org/root/register-job/-/jobs/884/raw\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python /projects/register-algorithm.py /projects/icesat2_boreal/dps/alg_2-3/algorithm_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-version",
   "metadata": {},
   "source": [
    "### Build the input DPS list of granules - s3fs implementation\n",
    "Find all the processed h5 granules that Eric ran  \n",
    "Find all the extracted CSV failes that Paul ran  \n",
    "Match h5 granules with corresponding CSVs  \n",
    "Find h5 granules that dont yet have csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3 = s3fs.S3FileSystem()\n",
    "YEAR = \"2022\"\n",
    "MONTH_LIST = ['09','10']\n",
    "bucket = \"s3://maap-ops-workspace\"\n",
    "\n",
    "# Match DPS output from 2 algorithms\n",
    "# These are the 2 DPS algs that produced the files we're searching for\n",
    "DPS_ALG_LIST = ['run_rebinning_ubuntu', 'run_extract_filter_atl08_ubuntu']\n",
    "# These are the 2 file extensions we're searching for\n",
    "FILE_EXT_LIST = ['.h5', '_30m.csv']\n",
    "\n",
    "for i, SEARCH_EXT_TYPE in enumerate(FILE_EXT_LIST):\n",
    "    \n",
    "    searchkey_type_list = [f\"lduncanson/dps_output/{DPS_ALG_LIST[i]}/master/{YEAR}/{MONTH}/{str('%02d' % DAY)}/**/*{SEARCH_EXT_TYPE}\" for DAY in range(1,32) for MONTH in MONTH_LIST]\n",
    "    # Concat list of lists to data frame\n",
    "    atl0830m_type_path_df = pd.concat([pd.DataFrame(s3.glob(os.path.join(bucket, searchkey)), columns=['maap_path']) for searchkey in searchkey_type_list])\n",
    "    #Convert data frame to list\n",
    "    ATL08_type_GRANULE_LIST = [i.replace(\"maap-ops-workspace\", \"https://maap-ops-workspace.s3.amazonaws.com\") for i in atl0830m_type_path_df.maap_path.to_list()]\n",
    "    \n",
    "    grans = [os.path.split(i)[-1].replace(SEARCH_EXT_TYPE,'') for i in atl0830m_type_path_df.maap_path.to_list()]\n",
    "    \n",
    "    if '.h5' in SEARCH_EXT_TYPE:\n",
    "        ATL08_h5_GRANULE_df = pd.DataFrame({'h5_path': ATL08_type_GRANULE_LIST, 'granule_name': grans})\n",
    "    else:\n",
    "        ATL08_csv_GRANULE_df = pd.DataFrame({'csv_path': ATL08_type_GRANULE_LIST, 'granule_name': grans})\n",
    "      \n",
    "print(f\"Count of H5 granules processed in {YEAR} for months {MONTH_LIST}: {len(ATL08_h5_GRANULE_df)}\")    \n",
    "print('Matching extracted csv to ATL08 30m h5 granules..')\n",
    "merged = ATL08_h5_GRANULE_df.merge(ATL08_csv_GRANULE_df, how='left', on='granule_name')\n",
    "\n",
    "print('Finding the null matches (the granules that still need extracted csvs...')\n",
    "INPUT_ATL08_GRANULE_LIST = merged[merged.csv_path.isnull()].h5_path.to_list()\n",
    "print(f'# of CSV extractions still needed:\\t{len(INPUT_ATL08_GRANULE_LIST)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "favorite-bibliography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ATL08_30m_20181024100254_03940102_005_01'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'ATL08_30m_20200609215801_11510705_005_01_30m.csv' not found\n",
    "ATL08_csv_GRANULE_df.iloc[2].granule_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "traditional-presence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DPS on the FULL list of input\n",
      "310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://maap-ops-workspace.s3.amazonaws.com/lduncanson/dps_output/run_rebinning_ubuntu/master/2022/09/30/22/47/49/955241/ATL08_30m_20181021160243_03520102_005_01.h5'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DPS  = False\n",
    "\n",
    "if TEST_DPS:\n",
    "    print('Running DPS on a SUBSET list of input')\n",
    "    DPS_INPUT_ATL08_GRANULE_LIST = INPUT_ATL08_GRANULE_LIST[-100:]\n",
    "else:\n",
    "    if False:\n",
    "        LIST_SIZE = 15000\n",
    "        print(f'Running DPS on the {LIST_SIZE} SUBSET list of input')\n",
    "        DPS_INPUT_ATL08_GRANULE_LIST = INPUT_ATL08_GRANULE_LIST[0:LIST_SIZE]\n",
    "    else:\n",
    "        print(f'Running DPS on the FULL list of input')\n",
    "        DPS_INPUT_ATL08_GRANULE_LIST = INPUT_ATL08_GRANULE_LIST\n",
    "print(len(DPS_INPUT_ATL08_GRANULE_LIST))\n",
    "DPS_INPUT_ATL08_GRANULE_LIST[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-gallery",
   "metadata": {},
   "source": [
    "## Run a DPS job across the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "behind-chain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPS run #: 1\t| granule name: ATL08_30m_20181102142803_05340106_005_01.h5\t| submit status: success\t| job id: 3ccd9f7a-f082-48fd-b3fd-34116e979f2c\n",
      "DPS run #: 100\t| granule name: ATL08_30m_20191109213440_06720502_005_01.h5\t| submit status: success\t| job id: 6242113c-ad03-4aeb-a0f5-9f5997ac286e\n",
      "DPS run #: 310\t| granule name: ATL08_30m_20181021160243_03520102_005_01.h5\t| submit status: success\t| job id: 37f898e1-9fae-4726-8800-7f823d0e1c4b\n",
      "CPU times: user 4.12 s, sys: 266 ms, total: 4.38 s\n",
      "Wall time: 1min 16s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>http_status_code</th>\n",
       "      <th>job_id</th>\n",
       "      <th>dps_num</th>\n",
       "      <th>tile_num</th>\n",
       "      <th>submit_time</th>\n",
       "      <th>dbs_job_hour</th>\n",
       "      <th>algo_id</th>\n",
       "      <th>user</th>\n",
       "      <th>worker_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>3ccd9f7a-f082-48fd-b3fd-34116e979f2c</td>\n",
       "      <td>1</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-10-12 12:07:53.285507</td>\n",
       "      <td>12</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>0151c4ac-095e-4af9-bd18-afa3039354e3</td>\n",
       "      <td>2</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-10-12 12:07:53.348068</td>\n",
       "      <td>12</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>b65ab991-9ba7-4129-b644-bb7dcfa1b540</td>\n",
       "      <td>3</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-10-12 12:07:53.435756</td>\n",
       "      <td>12</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>5eb98c8b-7634-476f-8cb5-58fef71d9190</td>\n",
       "      <td>4</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-10-12 12:07:53.537544</td>\n",
       "      <td>12</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>10dce289-b7b1-4450-a9a1-28411d40a120</td>\n",
       "      <td>5</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-10-12 12:07:53.749112</td>\n",
       "      <td>12</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>4fbd75d7-04ac-4e09-b470-e4fdbdc1db6c</td>\n",
       "      <td>306</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-10-12 12:09:08.314520</td>\n",
       "      <td>12</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>be72e65b-74f0-4657-ad9d-d8be0be28953</td>\n",
       "      <td>307</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-10-12 12:09:08.501526</td>\n",
       "      <td>12</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>93ba7534-7a0b-493d-8a9c-4a38b9230933</td>\n",
       "      <td>308</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-10-12 12:09:08.642494</td>\n",
       "      <td>12</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>1f75f11c-8392-4cea-8999-61ebef3e3681</td>\n",
       "      <td>309</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-10-12 12:09:08.828683</td>\n",
       "      <td>12</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>37f898e1-9fae-4726-8800-7f823d0e1c4b</td>\n",
       "      <td>310</td>\n",
       "      <td>https://maap-ops-workspace.s3.amazonaws.com/ld...</td>\n",
       "      <td>2022-10-12 12:09:09.009827</td>\n",
       "      <td>12</td>\n",
       "      <td>run_extract_filter_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     status  http_status_code                                job_id  dps_num  \\\n",
       "0   success               200  3ccd9f7a-f082-48fd-b3fd-34116e979f2c        1   \n",
       "0   success               200  0151c4ac-095e-4af9-bd18-afa3039354e3        2   \n",
       "0   success               200  b65ab991-9ba7-4129-b644-bb7dcfa1b540        3   \n",
       "0   success               200  5eb98c8b-7634-476f-8cb5-58fef71d9190        4   \n",
       "0   success               200  10dce289-b7b1-4450-a9a1-28411d40a120        5   \n",
       "..      ...               ...                                   ...      ...   \n",
       "0   success               200  4fbd75d7-04ac-4e09-b470-e4fdbdc1db6c      306   \n",
       "0   success               200  be72e65b-74f0-4657-ad9d-d8be0be28953      307   \n",
       "0   success               200  93ba7534-7a0b-493d-8a9c-4a38b9230933      308   \n",
       "0   success               200  1f75f11c-8392-4cea-8999-61ebef3e3681      309   \n",
       "0   success               200  37f898e1-9fae-4726-8800-7f823d0e1c4b      310   \n",
       "\n",
       "                                             tile_num  \\\n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "..                                                ...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "0   https://maap-ops-workspace.s3.amazonaws.com/ld...   \n",
       "\n",
       "                  submit_time  dbs_job_hour                          algo_id  \\\n",
       "0  2022-10-12 12:07:53.285507            12  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-10-12 12:07:53.348068            12  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-10-12 12:07:53.435756            12  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-10-12 12:07:53.537544            12  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-10-12 12:07:53.749112            12  run_extract_filter_atl08_ubuntu   \n",
       "..                        ...           ...                              ...   \n",
       "0  2022-10-12 12:09:08.314520            12  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-10-12 12:09:08.501526            12  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-10-12 12:09:08.642494            12  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-10-12 12:09:08.828683            12  run_extract_filter_atl08_ubuntu   \n",
       "0  2022-10-12 12:09:09.009827            12  run_extract_filter_atl08_ubuntu   \n",
       "\n",
       "          user          worker_type  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "..         ...                  ...  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "0   lduncanson  maap-dps-worker-8gb  \n",
       "\n",
       "[310 rows x 10 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_ATL08_GRANULE_LIST)\n",
    "\n",
    "for i, INPUT_ATL08_GRANULE in enumerate(DPS_INPUT_ATL08_GRANULE_LIST):\n",
    "\n",
    "    DPS_num = i+1\n",
    "    IDENTIFIER = 'run_extract_filter_atl08'\n",
    "    ALGO_ID = f'{IDENTIFIER}_ubuntu'\n",
    "    USER = 'lduncanson'\n",
    "    WORKER_TYPE = 'maap-dps-worker-8gb'\n",
    "    \n",
    "    in_param_dict = {\n",
    "                        'input': INPUT_ATL08_GRANULE,\n",
    "                        'rh_type': 'atl03',\n",
    "                        'seg_length': 30\n",
    "                    }\n",
    "\n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version='master',\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_param_dict\n",
    "        )\n",
    "    \n",
    "    # Build a dataframe of submission details\n",
    "    submit_result['dps_num'] = DPS_num\n",
    "    submit_result['tile_num'] = INPUT_ATL08_GRANULE\n",
    "    submit_result['submit_time'] = datetime.datetime.now()\n",
    "    submit_result['dbs_job_hour'] = datetime.datetime.now().hour\n",
    "    submit_result['algo_id'] = ALGO_ID\n",
    "    submit_result['user'] = USER\n",
    "    submit_result['worker_type'] = WORKER_TYPE\n",
    "    \n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(pd.DataFrame([submit_result]))\n",
    "    \n",
    "    if DPS_num in [1, 100, 500, 1000, 1500, 2000, 3000, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000, len_input_list]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| granule name: {os.path.basename(INPUT_ATL08_GRANULE)}\\t| submit status: {submit_result['status']}\\t| job id: {submit_result['job_id']}\") \n",
    "\n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-delay",
   "metadata": {},
   "source": [
    "After almost any DPS job, you have to assess what succeeded and failed. This involves:\n",
    "1. building a table of job status based on job ids captured in the job_results_df from the DPS run chunk (this takes 40 mins for ~47k jobs) --> this tells you how many jobs failed\n",
    "2. merging the job status table with the job results df --> this tells you which specific granules (or tile nums) failed\n",
    "3. building another input list of granules for a follow-up DPS\n",
    "## Assess DPS results\n",
    "Build a table of job status based on job id - how many jobs failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "intelligent-soccer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPS alg:\t\trun_extract_filter_atl08\n",
      "DPS launch time:\t202210121209\n",
      "Count total jobs:\t310\n",
      "Count pending jobs:\t0\n",
      "Count running jobs:\t0\n",
      "Count succeeded jobs:\t310\n",
      "Count failed jobs:\t0\n",
      "% of failed jobs:\tNothing has failed...yet\n",
      "\n",
      "CPU times: user 3.52 s, sys: 224 ms, total: 3.75 s\n",
      "Wall time: 8.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LIST_SUBMISSIONS = sorted(glob.glob(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_*.csv'),key=ExtractUtils.func, reverse=True)\n",
    "for DPS_DATETIME in [nowtime]:\n",
    "    for fn in LIST_SUBMISSIONS:\n",
    "        if DPS_DATETIME in fn and not 'job_status' in fn:\n",
    "            DPS_alg_id = os.path.basename(fn.split('_submission_results_')[0].replace('DPS_',''))\n",
    "            thentime = fn.split('_')[-1].replace('.csv','')\n",
    "            print(f'DPS alg:\\t\\t{DPS_alg_id}')\n",
    "            print(f'DPS launch time:\\t{thentime}')\n",
    "            z = ExtractUtils.BUILD_TABLE_JOBSTATUS(pd.read_csv(fn))\n",
    "            # Save job status table\n",
    "            z.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_job_status_{len(z)}_{thentime}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "coordinate-compact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "Building a list of tiles:  ATL08\n",
      "\n",
      "Output dir:  /projects/my-public-bucket/DPS_tile_lists\n",
      "# of duplicate tiles: 209\n",
      "Final # of tiles: 46166\n",
      "df shape :                                              s3_path  ... tile_num\n",
      "0  s3://maap-ops-workspace/lduncanson/dps_output/...  ...       NA\n",
      "1  s3://maap-ops-workspace/lduncanson/dps_output/...  ...       NA\n",
      "2  s3://maap-ops-workspace/lduncanson/dps_output/...  ...       NA\n",
      "3  s3://maap-ops-workspace/lduncanson/dps_output/...  ...       NA\n",
      "4  s3://maap-ops-workspace/lduncanson/dps_output/...  ...       NA\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "Writing tindex master csv: /projects/my-public-bucket/DPS_tile_lists/ATL08_tindex_master.csv\n"
     ]
    }
   ],
   "source": [
    "!python /projects/icesat2_boreal/lib/build_tindex_master_v2.py -t 'ATL08' -y 2022 -alg_name 'run_rebinning_ubuntu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "useful-transmission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('wps:Result',\n",
       "              OrderedDict([('@xmlns:ows', 'http://www.opengis.net/ows/2.0'),\n",
       "                           ('@xmlns:schemaLocation',\n",
       "                            'http://schemas.opengis.net/wps/2.0/wps.xsd'),\n",
       "                           ('@xmlns:wps', 'http://www.opengis.net/wps/2.0'),\n",
       "                           ('@xmlns:xsi',\n",
       "                            'http://www.w3.org/2001/XMLSchema-instance'),\n",
       "                           ('wps:JobID',\n",
       "                            '83d185f4-9df2-4095-b1f1-f5260375362c'),\n",
       "                           ('wps:Output',\n",
       "                            OrderedDict([('@id',\n",
       "                                          'output-2022-03-10T22:36:22.286595'),\n",
       "                                         ('wps:Data',\n",
       "                                          ['http://maap-ops-workspace.s3-website-us-west-2.amazonaws.com/lduncanson/dps_output/run_extract_filter_atl08_ubuntu/master/2022/03/10/22/36/22/286595',\n",
       "                                           's3://s3.us-west-2.amazonaws.com:80/maap-ops-workspace/lduncanson/dps_output/run_extract_filter_atl08_ubuntu/master/2022/03/10/22/36/22/286595',\n",
       "                                           'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/lduncanson/dps_output/run_extract_filter_atl08_ubuntu/master/2022/03/10/22/36/22/286595/?region=us-east-1&tab=overview'])]))]))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Succeeded'].iloc[2].job_id).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-definition",
   "metadata": {},
   "source": [
    "##### Pick a job and manually test - does it produce output as expected?\n",
    "if so, then AWS had some issue during the DPS: contact MAAP dev about this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nominated-phrase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Written by:\n",
      "\tNathan Thomas\t| @Nmt28\n",
      "\tPaul Montesano\t| paul.m.montesano@nasa.gov\n",
      "\n",
      "Min lat: 45.0\n",
      "Max lat: 75.0\n",
      "Min lon: -180.0\n",
      "Max lon: 180.0\n",
      "Month range: 6-9\n",
      "\n",
      "ATL08 granule name: \tATL08_30m_20210714235338_03261206_005_01\n",
      "Input dir: \t\t/projects/my-private-bucket/dps_output/run_rebinning_ubuntu/master/2022/03/05/04/51/06/341661\n",
      "\n",
      "Segment length: 30m\n",
      "Find src nodata value using max of h_can: \tnan\n",
      "\n",
      "Building pandas dataframe...\n",
      "Setting pandas df nodata values to np.nan for some basic eval.\n",
      "# of ATL08 obs: \t\t211006\n",
      "# of ATL08 obs (can pho.>=0): \t77997\n",
      "# of ATL08 obs (toc pho.>=0): \t77997\n",
      "# of ATL08 obs (h_can>=0): \t77905\n",
      "# of ATL08 obs (h_can<0): \t92\n",
      "Setting out pandas df nodata values: \t3.4028234663852886e+38\n",
      "Quality Filtering: \t[OFF] (do downstream)\n",
      "Geographic Filtering: \t[ON] xmin = -180.0, xmax = 180.0, ymin = 45.0, ymax = 75.0\n",
      "Creating CSV: \t\t/projects/my-public-bucket/test_output/ATL08_30m_20210714235338_03261206_005_01_30m.csv\n"
     ]
    }
   ],
   "source": [
    "f = fails_df.iloc[1].tile_num.replace('https://maap-ops-workspace.s3.amazonaws.com/lduncanson','/projects/my-private-bucket')\n",
    "!python /projects/icesat2_boreal/dps/alg_2-3/extract_filter_atl08.py --i $f --no-filter-qual --do_30m -o /projects/my-public-bucket/test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-location",
   "metadata": {},
   "source": [
    "### Compile granules of the fails for another DPS batch (Round2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "supported-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "varied-badge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 9000 9000 9000 8472\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "DPS_INPUT_ATL08_GRANULE_LIST_1, DPS_INPUT_ATL08_GRANULE_LIST_2, DPS_INPUT_ATL08_GRANULE_LIST_3, DPS_INPUT_ATL08_GRANULE_LIST_4, DPS_INPUT_ATL08_GRANULE_LIST_5 = list(chunks(DPS_INPUT_ATL08_GRANULE_LIST, 9000))\n",
    "print(\n",
    "    len(DPS_INPUT_ATL08_GRANULE_LIST_1), len(DPS_INPUT_ATL08_GRANULE_LIST_2), len(DPS_INPUT_ATL08_GRANULE_LIST_3), len(DPS_INPUT_ATL08_GRANULE_LIST_4), len(DPS_INPUT_ATL08_GRANULE_LIST_5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-shield",
   "metadata": {},
   "source": [
    "### Examine output of DPS - but replace glob.glob with s3.glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adjusted-aspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For DPS job that returned results in hour 20, # granules that ran: 10\n"
     ]
    }
   ],
   "source": [
    "#print(f\"The data frame show you submitted {len(job_results_df)} jobs. Check the returned results to see if the total returned = total submitted...\")\n",
    "for JOB_HOUR in range(20,21):\n",
    "    returned_results_list = glob.glob(f\"/projects/my-private-bucket/dps_output/run_extract_filter_atl08_ubuntu/master/2022/03/04/{JOB_HOUR}/**/*.csv\", recursive=True)\n",
    "    print(f\"For DPS job that returned results in hour {JOB_HOUR}, # granules that ran: {len(returned_results_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "linear-gender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pandas data frame...\n"
     ]
    }
   ],
   "source": [
    "# Merge all files in the list\n",
    "print(\"Creating pandas data frame...\")\n",
    "atl08_gdf = pd.concat((pd.read_csv(f) for f in returned_results_list ), sort=False, ignore_index=True) # <--generator is (), list is []\n",
    "atl08_gdf = gpd.GeoDataFrame(atl08_gdf, geometry=gpd.points_from_xy(atl08_gdf.lon, atl08_gdf.lat), crs='epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "touched-accommodation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fid</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>dt</th>\n",
       "      <th>orb_orient</th>\n",
       "      <th>orb_num</th>\n",
       "      <th>rgt</th>\n",
       "      <th>gt</th>\n",
       "      <th>segid_beg</th>\n",
       "      <th>segid_end</th>\n",
       "      <th>...</th>\n",
       "      <th>asr</th>\n",
       "      <th>h_dif_ref</th>\n",
       "      <th>ter_flg</th>\n",
       "      <th>ph_rem_flg</th>\n",
       "      <th>dem_rem_flg</th>\n",
       "      <th>seg_wmask</th>\n",
       "      <th>lyr_flg</th>\n",
       "      <th>seg_cover</th>\n",
       "      <th>granule_name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14887</td>\n",
       "      <td>75.620664</td>\n",
       "      <td>45.000490</td>\n",
       "      <td>b'2019-04-08T02:04:27.000000Z'</td>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>150</td>\n",
       "      <td>b'gt1r'</td>\n",
       "      <td>249813.842915</td>\n",
       "      <td>249817.842915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158199</td>\n",
       "      <td>-1.342682</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>ATL08_30m_20190408015557_01500302_005_01.h5</td>\n",
       "      <td>POINT (75.62066 45.00049)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14888</td>\n",
       "      <td>75.620628</td>\n",
       "      <td>45.000758</td>\n",
       "      <td>b'2019-04-08T02:04:27.000000Z'</td>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>150</td>\n",
       "      <td>b'gt1r'</td>\n",
       "      <td>249815.336919</td>\n",
       "      <td>249819.336919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158199</td>\n",
       "      <td>-1.342682</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>ATL08_30m_20190408015557_01500302_005_01.h5</td>\n",
       "      <td>POINT (75.62063 45.00076)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14889</td>\n",
       "      <td>75.620401</td>\n",
       "      <td>45.002371</td>\n",
       "      <td>b'2019-04-08T02:04:27.000000Z'</td>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>150</td>\n",
       "      <td>b'gt1r'</td>\n",
       "      <td>249824.331298</td>\n",
       "      <td>249828.331298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158199</td>\n",
       "      <td>-1.073212</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>ATL08_30m_20190408015557_01500302_005_01.h5</td>\n",
       "      <td>POINT (75.62040 45.00237)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14890</td>\n",
       "      <td>75.620364</td>\n",
       "      <td>45.002640</td>\n",
       "      <td>b'2019-04-08T02:04:27.000000Z'</td>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>150</td>\n",
       "      <td>b'gt1r'</td>\n",
       "      <td>249825.830880</td>\n",
       "      <td>249829.830880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158199</td>\n",
       "      <td>-1.073212</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>ATL08_30m_20190408015557_01500302_005_01.h5</td>\n",
       "      <td>POINT (75.62036 45.00264)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14891</td>\n",
       "      <td>75.620291</td>\n",
       "      <td>45.003178</td>\n",
       "      <td>b'2019-04-08T02:04:27.000000Z'</td>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>150</td>\n",
       "      <td>b'gt1r'</td>\n",
       "      <td>249828.830880</td>\n",
       "      <td>249832.830880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>-1.743439</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>ATL08_30m_20190408015557_01500302_005_01.h5</td>\n",
       "      <td>POINT (75.62029 45.00318)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fid        lon        lat                              dt  orb_orient  \\\n",
       "0  14887  75.620664  45.000490  b'2019-04-08T02:04:27.000000Z'           0   \n",
       "1  14888  75.620628  45.000758  b'2019-04-08T02:04:27.000000Z'           0   \n",
       "2  14889  75.620401  45.002371  b'2019-04-08T02:04:27.000000Z'           0   \n",
       "3  14890  75.620364  45.002640  b'2019-04-08T02:04:27.000000Z'           0   \n",
       "4  14891  75.620291  45.003178  b'2019-04-08T02:04:27.000000Z'           0   \n",
       "\n",
       "   orb_num  rgt       gt      segid_beg      segid_end  ...       asr  \\\n",
       "0     3125  150  b'gt1r'  249813.842915  249817.842915  ...  0.158199   \n",
       "1     3125  150  b'gt1r'  249815.336919  249819.336919  ...  0.158199   \n",
       "2     3125  150  b'gt1r'  249824.331298  249828.331298  ...  0.158199   \n",
       "3     3125  150  b'gt1r'  249825.830880  249829.830880  ...  0.158199   \n",
       "4     3125  150  b'gt1r'  249828.830880  249832.830880  ...  0.129356   \n",
       "\n",
       "   h_dif_ref  ter_flg  ph_rem_flg  dem_rem_flg  seg_wmask  lyr_flg  seg_cover  \\\n",
       "0  -1.342682        1           0            0          0        0         48   \n",
       "1  -1.342682        1           0            0          0        0         48   \n",
       "2  -1.073212        1           0            0          0        0         42   \n",
       "3  -1.073212        1           0            0          0        0         42   \n",
       "4  -1.743439        1           0            0          0        0         24   \n",
       "\n",
       "                                  granule_name                   geometry  \n",
       "0  ATL08_30m_20190408015557_01500302_005_01.h5  POINT (75.62066 45.00049)  \n",
       "1  ATL08_30m_20190408015557_01500302_005_01.h5  POINT (75.62063 45.00076)  \n",
       "2  ATL08_30m_20190408015557_01500302_005_01.h5  POINT (75.62040 45.00237)  \n",
       "3  ATL08_30m_20190408015557_01500302_005_01.h5  POINT (75.62036 45.00264)  \n",
       "4  ATL08_30m_20190408015557_01500302_005_01.h5  POINT (75.62029 45.00318)  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atl08_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "antique-numbers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system( \"python /projects/icesat2_boreal/lib/build_tindex_master.py -t ATL08 -dps_year 2022 -m 3 --start_day '04' --outdir /projects/my-public-bucket/test_output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
