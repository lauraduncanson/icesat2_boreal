{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "according-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host='api.ops.maap-project.org')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-smell",
   "metadata": {},
   "source": [
    "# Launch DPS for tile_atl08.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hidden-excellence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.7/site-packages (0.12.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "!pip install xmltodict\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "soviet-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_fn(stack_list_fn, in_tile_num):\n",
    "    # Find most recent topo/Landsat stack path for tile in list of stack paths from *tindex_master.csv\n",
    "    all_stacks_df = pd.read_csv(stack_list_fn)\n",
    "    stack_for_tile = all_stacks_df[all_stacks_df['location'].str.contains(\"_\"+str(in_tile_num))]\n",
    "    [print(i) for i in stack_for_tile.path.to_list()]\n",
    "    stack_for_tile_fn = stack_for_tile.path.to_list()[0]\n",
    "    if len(stack_for_tile)==0:\n",
    "        stack_for_tile_fn = None\n",
    "    return(stack_for_tile_fn)\n",
    "\n",
    "# nmt added: code that returns df of landsat locations and tile number\n",
    "# This is basically CountOutput.py\n",
    "def get_stack_df(dps_dir, TYPE, dps_year):\n",
    "    \n",
    "    if \"Landsat\" in TYPE:\n",
    "        root = f\"/projects/my-private-bucket/dps_output/do_landsat_stack_3-1-2_ubuntu/ops/{dps_year}/\"\n",
    "        ends_with_str = \"_dps.tif\"\n",
    "    if \"Topo\" in TYPE:\n",
    "        root = f\"/projects/my-private-bucket/dps_output/do_topo_stack_3-1-5_ubuntu/ops/{dps_year}/\"\n",
    "        ends_with_str = \"_stack.tif\"\n",
    "    if \"ATL08\" in TYPE:\n",
    "        root = f\"/projects/my-private-bucket/dps_output/run_extract_ubuntu/ops/{dps_year}/\"\n",
    "        ends_with_str = \"0m.csv\"\n",
    "            \n",
    "    df = pd.DataFrame(columns=['location', 'tile_num'])\n",
    "\n",
    "    for dir, subdir, files in os.walk(root):\n",
    "        for fname in files:\n",
    "            if fname.endswith(ends_with_str): \n",
    "                 \n",
    "                tile_num = fname.split('_')[1]\n",
    "                   \n",
    "                if \"ATL08\" in TYPE:\n",
    "                    df = df.append({'location':os.path.join(dir+\"/\", fname)},ignore_index=True)\n",
    "                else:\n",
    "                    df = df.append({'location':os.path.join(dir+\"/\", fname), 'tile_num':tile_num},ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-beauty",
   "metadata": {},
   "source": [
    "#### Set the names of the data frames to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "demographic-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topo and Landsat tindex_master csvs from build_tindex_master.py\n",
    "topo_tindex = \"/projects/my-public-bucket/DPS_tile_lists/Topo_tindex_master.csv\"\n",
    "landsat_tindex = \"/projects/my-public-bucket/DPS_tile_lists/Landsat_tindex_master.csv\"\n",
    "\n",
    "# Model-ready subset of tiles for which Topo and Landsat coincide\n",
    "model_ready_tiles_topo = \"/projects/my-public-bucket/DPS_tile_lists/model_ready_tiles_topo_paths.csv\"\n",
    "model_ready_tiles_landsat = \"/projects/my-public-bucket/DPS_tile_lists/model_ready_tiles_landsat_paths.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-legislature",
   "metadata": {},
   "source": [
    "## Make the data frames from build_tindex_master.py csvs for Topo and Landsat tiles\n",
    "python lib/build_tindex_master.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "muslim-valley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading existing...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>local_path</th>\n",
       "      <th>tile_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/do_topo...</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/do_topo...</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/do_topo...</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/do_topo...</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/do_topo...</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         local_path  tile_num\n",
       "0           0  /projects/my-private-bucket/dps_output/do_topo...       421\n",
       "1           1  /projects/my-private-bucket/dps_output/do_topo...       455\n",
       "2           2  /projects/my-private-bucket/dps_output/do_topo...       456\n",
       "3           3  /projects/my-private-bucket/dps_output/do_topo...       491\n",
       "4           4  /projects/my-private-bucket/dps_output/do_topo...       492"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile(landsat_tindex) and os.path.isfile(topo_tindex):\n",
    "    print('Reading existing...')\n",
    "    ls8_df = pd.read_csv(landsat_tindex)\n",
    "    topo_df = pd.read_csv(topo_tindex)\n",
    "else:\n",
    "    s3_stem = 'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/nathanmthomas'\n",
    "    local_stem = '/projects/my-private-bucket'\n",
    "\n",
    "    ls8_root =  s3_stem + '/dps_output/do_landsat_stack_3-1-2_ubuntu'\n",
    "    topo_root = s3_stem + '/dps_output/do_topo_stack_3-1-5_ubuntu'\n",
    "    \n",
    "    ls8_df = get_stack_df(ls8_root, \"Landsat\")\n",
    "    topo_df = get_stack_df(topo_root, \"Topo\")\n",
    "topo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "weighted-kitty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/nathanmthomas/dps_output/do_topo_stack_3-1-5_ubuntu/ops/2021/07/23/23/32/27/934649/Copernicus_3457_covars_cog_topo_stack.tif'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "topo_df = pd.read_csv(topo_tindex)\n",
    "topo_df[topo_df.tile_num == 3457].local_path.tolist()[0].replace('/projects/my-private-bucket', 'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/nathanmthomas')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-bankruptcy",
   "metadata": {},
   "source": [
    "## Get tile ids for which both Topo and Landsat stacks exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "italian-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by nmt: get filenames of co-incident landsat and topo\n",
    "if False:\n",
    "    topo_sub_df = pd.DataFrame(columns=['local_path','tile_num'])\n",
    "    ls8_sub_df = pd.DataFrame(columns=['local_path','tile_num'])\n",
    "\n",
    "    for i in range(len(ls8_df['tile_num'])):\n",
    "        ls_tile_num = ls8_df['tile_num'][i]\n",
    "        for j in range(len(topo_df['tile_num'])):\n",
    "            topo_tile_num = topo_df['tile_num'][j]\n",
    "            if ls_tile_num == topo_tile_num:\n",
    "                # Only need to choose one, but we'll do 2 and then check\n",
    "                ls8_sub_df = ls8_sub_df.append({'local_path':ls8_df['local_path'][i],'tile_num':ls8_df['tile_num'][i].astype(int)}, ignore_index=True)\n",
    "                topo_sub_df = topo_sub_df.append({'local_path':topo_df['local_path'][j],'tile_num':topo_df['tile_num'][j].astype(int)}, ignore_index=True)\n",
    "\n",
    "    #ls8_sub_df['tile_num'] = ls8_sub_df['tile_num'].astype(float, errors = 'raise')\n",
    "    print(ls8_sub_df.head())\n",
    "    print(topo_sub_df.head())\n",
    "    print(len(ls8_sub_df),len(topo_sub_df))\n",
    "\n",
    "    topo_sub_df.to_csv( model_ready_tiles_topo, index=False, encoding='utf-8-sig')\n",
    "    ls8_sub_df.to_csv( model_ready_tiles_landsat, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-houston",
   "metadata": {},
   "source": [
    "#### Now you have a set of tile ids for which both Landsat and Topo stacks exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "forty-sister",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4465"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topo_sub_df = pd.read_csv(\"/projects/my-public-bucket/DPS_tile_lists/model_ready_tiles_topo_paths.csv\")\n",
    "INPUT_TILE_NUM_LIST = topo_sub_df['tile_num'].values.astype(int).tolist()\n",
    "len(INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-delight",
   "metadata": {},
   "source": [
    "##### Test: get a subset of tile ids for test tiles (Norway and others in NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "recovered-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORWAY_TILE_LIST = pd.read_csv('/projects/my-public-bucket/misc_files/norway_tiles.csv').layer.tolist()\n",
    "DELTA_TILE_LIST = [3365,3366,3367,3458,3460,3353,3354,3355,3549]\n",
    "BONA_TILE_LIST  = [3270,3271,3272,3364,3456,3457,3458,3364,3365]\n",
    "HEALY_TILE_LIST = [3456,3457,3458,3551,3553,3645,3646,3647,3552]\n",
    "\n",
    "INPUT_TEST_TILE_NUM_LIST = NORWAY_TILE_LIST + DELTA_TILE_LIST + BONA_TILE_LIST + HEALY_TILE_LIST\n",
    "\n",
    "len(INPUT_TEST_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-hawaiian",
   "metadata": {},
   "source": [
    "#### Read in the latest tindex and compare with a previous set of completed tiles to see which ones still need to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "appointed-storage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiles completed: 128\n",
      "Tiles missing: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tiles_completed = pd.read_csv('/projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022/run_no_LC_height_thresholds/ATL08_filt_tindex_master.csv')\n",
    "print(f'Tiles completed: {len(tiles_completed)}')\n",
    "tile_nums_missing = np.setdiff1d(INPUT_TEST_TILE_NUM_LIST, tiles_completed.tile_num)\n",
    "print(f'Tiles missing: {len(tile_nums_missing)}')\n",
    "INPUT_TEST_TILE_NUM_LIST = tile_nums_missing.tolist()\n",
    "len(INPUT_TEST_TILE_NUM_LIST)\n",
    "#print(INPUT_TEST_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "bridal-consent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46166"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tindex_master_fn = f'/projects/shared-buckets/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv'\n",
    "tiles = pd.read_csv(tindex_master_fn)\n",
    "len(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "romance-philippines",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tiles for no LC:\t132\n",
      "# tiles for LC:\t\t133\n",
      "tiles missing for no LC:\t{5, 6, 7, 328, 26025, 301, 14, 26574, 178, 275, 21, 3549}\n",
      "tiles missing for LC:\t\t{5, 6, 7, 328, 357, 10, 26025, 301, 14, 26574, 178, 275, 21}\n",
      "Tiles still needed for no LC run: [3549]\n",
      "Tiles still needed for LC run: [10, 357]\n"
     ]
    }
   ],
   "source": [
    "tiles_completed_no_LC = pd.read_csv('/projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022/run_no_LC_height_thresholds/ATL08_filt_tindex_master.csv')\n",
    "tiles_completed_LC = pd.read_csv('/projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022/run_LC_height_thresholds/ATL08_filt_tindex_master.csv')\n",
    "print(f\"# tiles for no LC:\\t{len(tiles_completed_no_LC)}\")\n",
    "print(f\"# tiles for LC:\\t\\t{len(tiles_completed_LC)}\")\n",
    "tile_nums_missing_no_LC = np.setdiff1d(INPUT_TEST_TILE_NUM_LIST, tiles_completed_no_LC.tile_num)\n",
    "tile_nums_missing_LC = np.setdiff1d(INPUT_TEST_TILE_NUM_LIST, tiles_completed_LC.tile_num)\n",
    "tile_nums_missing_no_LC = set(INPUT_TEST_TILE_NUM_LIST) - set(tiles_completed_no_LC.tile_num)\n",
    "tile_nums_missing_LC = set(INPUT_TEST_TILE_NUM_LIST) - set(tiles_completed_LC.tile_num)\n",
    "print(f\"tiles missing for no LC:\\t{tile_nums_missing_no_LC}\")\n",
    "print(f\"tiles missing for LC:\\t\\t{tile_nums_missing_LC}\")\n",
    "\n",
    "#print(f\"tiles for no LC:\\t{tiles_completed_no_LC.tile_num}\")\n",
    "#print(f\"tiles for LC:\\t\\t{tiles_completed_LC.tile_num}\")\n",
    "\n",
    "# The missing tiles common to both runs probably wont process b/c they have no ATL08 over land, or no corresponding Landsat or Topo tiles.\n",
    "# Those missing that are different in each set need to be run\n",
    "DPS_INPUT_TILE_NUM_LIST_no_LC = list(set(tiles_completed_LC.tile_num) - set(tiles_completed_no_LC.tile_num))\n",
    "DPS_INPUT_TILE_NUM_LIST_LC = list(set(tiles_completed_no_LC.tile_num) - set(tiles_completed_LC.tile_num))\n",
    "print(f\"Tiles still needed for no LC run: {DPS_INPUT_TILE_NUM_LIST_no_LC}\")\n",
    "print(f\"Tiles still needed for LC run: {DPS_INPUT_TILE_NUM_LIST_LC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "interior-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "Building a list of tiles:  ATL08_filt\n",
      "\n",
      "Output dir:  /projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022/run_LC_height_thresholds\n",
      "                                              s3_path  ...                                               file\n",
      "0   s3://maap-ops-workspace/lduncanson/dps_output/...  ...  atl08_005_30m_filt_topo_landsat_20220312_0043.csv\n",
      "2   s3://maap-ops-workspace/lduncanson/dps_output/...  ...  atl08_005_30m_filt_topo_landsat_20220312_0054.csv\n",
      "5   s3://maap-ops-workspace/lduncanson/dps_output/...  ...  atl08_005_30m_filt_topo_landsat_20220312_0065.csv\n",
      "7   s3://maap-ops-workspace/lduncanson/dps_output/...  ...  atl08_005_30m_filt_topo_landsat_20220312_0030.csv\n",
      "10  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  atl08_005_30m_filt_topo_landsat_20220312_0042.csv\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "# of duplicate tiles: 23\n",
      "Final # of tiles: 133\n",
      "df shape :                                               s3_path  ... tile_num\n",
      "0   s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0043\n",
      "2   s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0054\n",
      "5   s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0065\n",
      "7   s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0030\n",
      "10  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0042\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "Writing tindex master csv: /projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022/run_LC_height_thresholds/ATL08_filt_tindex_master.csv\n"
     ]
    }
   ],
   "source": [
    "month_dir_str = 'run_LC_height_thresholds'\n",
    "index_out_dir = os.path.join('/projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022', month_dir_str)\n",
    "!python /projects/icesat2_boreal/lib/build_tindex_master.py -t ATL08_filt -y 2022 -m $month_dir_str -o $index_out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "convinced-algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3552, 4, 5, 6, 7, 328, 9, 26025, 301, 14, 26574, 177, 178, 275, 21, 3549]\n",
      "# of tiles to run: 16\n",
      " [3552, 4, 5, 6, 7, 328, 9, 26025, 301, 14, 26574, 177, 178, 275, 21, 3549]\n"
     ]
    }
   ],
   "source": [
    "TEST_DPS  = True\n",
    "\n",
    "if TEST_DPS:\n",
    "    DPS_INPUT_TILE_NUM_LIST = INPUT_TEST_TILE_NUM_LIST\n",
    "    \n",
    "    if True:\n",
    "        #!python /projects/icesat2_boreal/lib/build_tindex_master_v2.py -t ATL08_filt -y 2022 -m $month_dir_str -o $index_out_dir\n",
    "        t = pd.read_csv(os.path.join(index_out_dir,'ATL08_filt_tindex_master.csv'))\n",
    "        COMPLETED_TILES = t.tile_num.to_list()\n",
    "        NEED_TILES = list(set(DPS_INPUT_TILE_NUM_LIST) - set(COMPLETED_TILES))\n",
    "\n",
    "        print(NEED_TILES)\n",
    "        DPS_INPUT_TILE_NUM_LIST = NEED_TILES\n",
    "    \n",
    "else:\n",
    "    DPS_INPUT_TILE_NUM_LIST = INPUT_TILE_NUM_LIST\n",
    "   \n",
    "\n",
    "#DPS_INPUT_TILE_NUM_LIST = [248, 273, 272, 271, 324]\n",
    "print(f\"# of tiles to run: {len(DPS_INPUT_TILE_NUM_LIST)}\\n\", DPS_INPUT_TILE_NUM_LIST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-squad",
   "metadata": {},
   "source": [
    "#### Customize the DPS run: set up the parameters dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "approved-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norway test 01\n",
    "# Just include sol_el so we can use sol_el < 5\n",
    "in_param_dict_norway01 = {\n",
    "                        'in_tile_num': '',\n",
    "                        'in_tile_fn': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "                        'in_tile_layer': 'boreal_tiles_v003',\n",
    "                        'csv_list_fn': 's3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv',\n",
    "                        'topo_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv',\n",
    "                        'landsat_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv',\n",
    "                        'years_list': '2019 2020 2021',\n",
    "                        'user_stacks': 'nathanmthomas',\n",
    "                        'user_atl08': 'lduncanson',\n",
    "                        'thresh_sol_el': 5,\n",
    "                        'v_ATL08': 5,\n",
    "                        'minmonth': 4,\n",
    "                        'maxmonth': 10,\n",
    "                        'LC_filter': False\n",
    "    }\n",
    "# Norway test 02\n",
    "# Use v005 ATL08, which will apply lc-based thresholds, extend to all months\n",
    "# NOTE!! make sure you manually update to use the correct filter in tile_atl08.py\n",
    "in_param_dict_norway02 = {\n",
    "                        'in_tile_num': '',\n",
    "                        'in_tile_fn': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "                        'in_tile_layer': 'boreal_tiles_v003',\n",
    "                        'csv_list_fn': 's3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv',\n",
    "                        'topo_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv',\n",
    "                        'landsat_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv',\n",
    "                        'years_list': '2019 2020 2021',\n",
    "                        'user_stacks': 'nathanmthomas',\n",
    "                        'user_atl08': 'lduncanson',\n",
    "                        'thresh_sol_el': 5,\n",
    "                        'v_ATL08': 5,\n",
    "                        'minmonth': 4,\n",
    "                        'maxmonth': 10,\n",
    "                        'LC_filter': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "clean-linux",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_tile_num': '',\n",
       " 'in_tile_fn': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
       " 'in_tile_layer': 'boreal_tiles_v003',\n",
       " 'csv_list_fn': 's3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv',\n",
       " 'topo_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv',\n",
       " 'landsat_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv',\n",
       " 'years_list': '2019 2020 2021',\n",
       " 'user_stacks': 'nathanmthomas',\n",
       " 'user_atl08': 'lduncanson',\n",
       " 'thresh_sol_el': 5,\n",
       " 'v_ATL08': 5,\n",
       " 'minmonth': 4,\n",
       " 'maxmonth': 10,\n",
       " 'LC_filter': True}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_param_dict = in_param_dict_norway02\n",
    "in_param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-stopping",
   "metadata": {},
   "source": [
    "## Run a DPS job across the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "partial-channels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input tiles for DPS: 2\n",
      "DPS run #: 1\t| tile num: 10\t| submit status: success\t| job id: 846a76f0-408e-47f3-9793-a95e22517c08\n",
      "DPS run #: 2\t| tile num: 357\t| submit status: success\t| job id: 32dba8a0-251d-4056-ac8f-80a4f9bf78d0\n",
      "Current time:\t202203171706\n",
      "CPU times: user 23.8 ms, sys: 6.72 ms, total: 30.5 ms\n",
      "Wall time: 433 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>http_status_code</th>\n",
       "      <th>job_id</th>\n",
       "      <th>dps_num</th>\n",
       "      <th>tile_num</th>\n",
       "      <th>submit_time</th>\n",
       "      <th>dbs_job_hour</th>\n",
       "      <th>algo_id</th>\n",
       "      <th>user</th>\n",
       "      <th>worker_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>846a76f0-408e-47f3-9793-a95e22517c08</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2022-03-17 17:06:57.631194</td>\n",
       "      <td>17</td>\n",
       "      <td>run_tile_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>32dba8a0-251d-4056-ac8f-80a4f9bf78d0</td>\n",
       "      <td>2</td>\n",
       "      <td>357</td>\n",
       "      <td>2022-03-17 17:06:57.716327</td>\n",
       "      <td>17</td>\n",
       "      <td>run_tile_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-16gb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    status  http_status_code                                job_id  dps_num  \\\n",
       "0  success               200  846a76f0-408e-47f3-9793-a95e22517c08        1   \n",
       "0  success               200  32dba8a0-251d-4056-ac8f-80a4f9bf78d0        2   \n",
       "\n",
       "   tile_num                submit_time  dbs_job_hour                algo_id  \\\n",
       "0        10 2022-03-17 17:06:57.631194            17  run_tile_atl08_ubuntu   \n",
       "0       357 2022-03-17 17:06:57.716327            17  run_tile_atl08_ubuntu   \n",
       "\n",
       "         user           worker_type  \n",
       "0  lduncanson  maap-dps-worker-16gb  \n",
       "0  lduncanson  maap-dps-worker-16gb  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(f\"# of input tiles for DPS: {len_input_list}\")\n",
    "\n",
    "for i, INPUT_TILE_NUM in enumerate(DPS_INPUT_TILE_NUM_LIST):\n",
    "\n",
    "    DPS_num = i+1\n",
    "    IDENTIFIER = 'run_tile_atl08'\n",
    "    ALGO_ID = f'{IDENTIFIER}_ubuntu'\n",
    "    USER = 'lduncanson'\n",
    "    WORKER_TYPE = 'maap-dps-worker-16gb'\n",
    "    \n",
    "    in_param_dict['in_tile_num'] = INPUT_TILE_NUM\n",
    "    \n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version='master',\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_param_dict\n",
    "        )\n",
    "    \n",
    "    # Build a dataframe of submission details\n",
    "    submit_result['dps_num'] = DPS_num\n",
    "    submit_result['tile_num'] = INPUT_TILE_NUM\n",
    "    submit_result['submit_time'] = datetime.datetime.now()\n",
    "    submit_result['dbs_job_hour'] =datetime.datetime.now().hour\n",
    "    submit_result['algo_id'] = ALGO_ID\n",
    "    submit_result['user'] = USER\n",
    "    submit_result['worker_type'] = WORKER_TYPE\n",
    "    \n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(pd.DataFrame([submit_result]))\n",
    "    \n",
    "    if DPS_num in [1, 25, 50, 100, 500, 1000, 1500, 2000, 3000, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000, len_input_list]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| tile num: {INPUT_TILE_NUM}\\t| submit status: {submit_result['status']}\\t| job id: {submit_result['job_id']}\") \n",
    "\n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "print(f\"Current time:\\t{nowtime}\")\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/DPS_{IDENTIFIER}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-nation",
   "metadata": {},
   "source": [
    "After almost any DPS job, you have to assess what succeeded and failed. This involves:\n",
    "1. building a table of job status based on job ids captured in the job_results_df from the DPS run chunk (this takes 40 mins for ~47k jobs) --> this tells you how many jobs failed\n",
    "2. merging the job status table with the job results df --> this tells you which specific granules (or tile nums) failed\n",
    "3. building another input list of granules for a follow-up DPS\n",
    "## Assess DPS results\n",
    "Build a table of job status based on job id - how many jobs failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "breathing-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count total jobs:\t2\n",
      "Count pending jobs:\t0\n",
      "Count running jobs:\t0\n",
      "Count succeeded jobs:\t2\n",
      "Count failed jobs:\t0\n",
      "% of failed jobs:\t0.0\n",
      "CPU times: user 32.9 ms, sys: 0 ns, total: 32.9 ms\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def BUILD_TABLE_JOBSTATUS(submit_results_df):\n",
    "    import xmltodict\n",
    "    df = pd.concat([pd.DataFrame(xmltodict.parse(maap.getJobStatus(job_id).content)).transpose() for job_id in submit_results_df.job_id.to_list()])\n",
    "    return df\n",
    "\n",
    "job_status_df = BUILD_TABLE_JOBSTATUS(submit_results_df)\n",
    "#print(job_status_df.head())\n",
    "\n",
    "num_jobs = submit_results_df.shape[0]\n",
    "z = submit_results_df.merge(job_status_df, how='left', left_on='job_id',  right_on='wps:JobID')\n",
    "\n",
    "print(f'Count total jobs:\\t{num_jobs}')\n",
    "print(f\"Count pending jobs:\\t{z[z['wps:Status'] =='Accepted'].shape[0]}\")\n",
    "print(f\"Count running jobs:\\t{z[z['wps:Status'] =='Running'].shape[0]}\")\n",
    "print(f\"Count succeeded jobs:\\t{z[z['wps:Status'] =='Succeeded'].shape[0]}\")\n",
    "print(f\"Count failed jobs:\\t{z[z['wps:Status'] =='Failed'].shape[0]}\")\n",
    "print(f\"% of failed jobs:\\t{round(z[z['wps:Status'] =='Failed'].shape[0] / ( z[z['wps:Status'] =='Failed'].shape[0] + z[z['wps:Status'] =='Succeeded'].shape[0] ), 4) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "meaningful-fantasy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('wps:Result',\n",
       "              OrderedDict([('@xmlns:ows', 'http://www.opengis.net/ows/2.0'),\n",
       "                           ('@xmlns:schemaLocation',\n",
       "                            'http://schemas.opengis.net/wps/2.0/wps.xsd'),\n",
       "                           ('@xmlns:wps', 'http://www.opengis.net/wps/2.0'),\n",
       "                           ('@xmlns:xsi',\n",
       "                            'http://www.w3.org/2001/XMLSchema-instance'),\n",
       "                           ('wps:JobID',\n",
       "                            '7523cb62-fc0f-44e9-b54e-b35f6e54fba8'),\n",
       "                           ('wps:Output',\n",
       "                            OrderedDict([('@id',\n",
       "                                          'output-2022-03-17T02:53:00.066869'),\n",
       "                                         ('wps:Data',\n",
       "                                          ['http://maap-ops-workspace.s3-website-us-west-2.amazonaws.com/lduncanson/dps_output/run_tile_atl08_ubuntu/master/2022/03/17/02/53/00/066869',\n",
       "                                           's3://s3.us-west-2.amazonaws.com:80/maap-ops-workspace/lduncanson/dps_output/run_tile_atl08_ubuntu/master/2022/03/17/02/53/00/066869',\n",
       "                                           'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/lduncanson/dps_output/run_tile_atl08_ubuntu/master/2022/03/17/02/53/00/066869/?region=us-east-1&tab=overview'])]))]))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Succeeded'].iloc[0].job_id).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "governmental-paragraph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('wps:Result',\n",
       "              OrderedDict([('@xmlns:ows', 'http://www.opengis.net/ows/2.0'),\n",
       "                           ('@xmlns:schemaLocation',\n",
       "                            'http://schemas.opengis.net/wps/2.0/wps.xsd'),\n",
       "                           ('@xmlns:wps', 'http://www.opengis.net/wps/2.0'),\n",
       "                           ('@xmlns:xsi',\n",
       "                            'http://www.w3.org/2001/XMLSchema-instance'),\n",
       "                           ('wps:JobID',\n",
       "                            'f7409afd-53df-4423-8300-97a9eda7430e'),\n",
       "                           ('wps:Output',\n",
       "                            OrderedDict([('@id', 'traceback'),\n",
       "                                         ('wps:Data',\n",
       "                                          'activate does not accept more than one argument:\\n[\\'/app/icesat2_boreal/dps/alg_2-4/run_tile_atl08.sh\\', \\'328\\', \\'boreal_tiles_v003\\', \\'s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv\\', \\'s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv\\', \\'s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv\\', \\'2019 2020 2021\\', \\'nathanmthomas\\', \\'lduncanson\\', \\'5\\', \\'5\\', \\'4\\', \\'10\\', \\'True\\']\\n\\n+ /app/icesat2_boreal/dps/alg_2-4/run_tile_atl08.sh 328 boreal_tiles_v003 s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv \\'2019 2020 2021\\' nathanmthomas lduncanson 5 5 4 10 True\\n+++ dirname /app/icesat2_boreal/dps/alg_2-4/run_tile_atl08.sh\\n++ cd /app/icesat2_boreal/dps/alg_2-4\\n++ pwd -P\\n+ basedir=/app/icesat2_boreal/dps/alg_2-4\\n+ unset PROJ_LIB\\n+ mkdir output\\n+ FILENAMELIST=($(ls -d input/*gpkg))\\n++ ls -d input/boreal_tiles_v003.gpkg\\n+ INPUT1=/data/work/jobs/2022/03/17/02/40/job-run_tile_atl08_ubuntu__master-20220317T023353.811572Z/input/boreal_tiles_v003.gpkg\\n+ OUTPUTDIR=/data/work/jobs/2022/03/17/02/40/job-run_tile_atl08_ubuntu__master-20220317T023353.811572Z/output\\n+ python /app/icesat2_boreal/dps/alg_2-4/../../lib/tile_atl08.py --extract_covars --do_dps --do_30m -o /data/work/jobs/2022/03/17/02/40/job-run_tile_atl08_ubuntu__master-20220317T023353.811572Z/output -in_tile_num 328 -in_tile_fn /data/work/jobs/2022/03/17/02/40/job-run_tile_atl08_ubuntu__master-20220317T023353.811572Z/input/boreal_tiles_v003.gpkg -in_tile_layer boreal_tiles_v003 -csv_list_fn s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv -topo_stack_list_fn s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv -landsat_stack_list_fn s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv -years_list 2019 2020 2021 -user_stacks nathanmthomas -user_atl08 lduncanson -thresh_sol_el 5 -v_ATL08 5 -minmonth 4 -maxmonth 10 -LC_filter True\\nERROR 1: PROJ: proj_create_from_database: Open of /opt/conda/envs/icesat2_boreal/share/proj failed\\nTraceback (most recent call last):\\n  File \"/app/icesat2_boreal/lib/FilterUtils.py\", line 54, in prep_filter_atl08_qual\\n    atl08.loc[( (atl08.orb_orient == 1 ) & (atl08[\\'gt\\'].str.contains(\\'r\\')) ), \"beam_type\"] = \\'Strong\\' \\n  File \"/opt/conda/envs/icesat2_boreal/lib/python3.10/site-packages/pandas/core/indexing.py\", line 716, in __setitem__\\n    iloc._setitem_with_indexer(indexer, value, self.name)\\n  File \"/opt/conda/envs/icesat2_boreal/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1615, in _setitem_with_indexer\\n    raise ValueError(\\nValueError: cannot set a frame with no defined index and a scalar\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/app/icesat2_boreal/dps/alg_2-4/../../lib/tile_atl08.py\", line 386, in <module>\\n    main()\\n  File \"/app/icesat2_boreal/dps/alg_2-4/../../lib/tile_atl08.py\", line 309, in main\\n    atl08 = FilterUtils.prep_filter_atl08_qual(atl08)\\n  File \"/app/icesat2_boreal/lib/FilterUtils.py\", line 59, in prep_filter_atl08_qual\\n    atl08.loc[( (atl08.orb_orient == 1 ) & (atl08[\\'gt\\'].str.decode(\\'utf-8\\').str.contains(\\'r\\')) ), \"beam_type\"] = \\'Strong\\' \\n  File \"/opt/conda/envs/icesat2_boreal/lib/python3.10/site-packages/pandas/core/indexing.py\", line 716, in __setitem__\\n    iloc._setitem_with_indexer(indexer, value, self.name)\\n  File \"/opt/conda/envs/icesat2_boreal/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1615, in _setitem_with_indexer\\n    raise ValueError(\\nValueError: cannot set a frame with no defined index and a scalar\\n+ cp _stderr.txt _alt_traceback.txt')]))]))])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Failed'].iloc[1].job_id).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "elect-drunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "Building a list of tiles:  ATL08_filt\n",
      "\n",
      "Output dir:  /projects/test_dps\n",
      "/projects/icesat2_boreal/lib/build_tindex_master_v2.py:154: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['tile_num'] = df['file'].str.split('_', expand=True)[7].str.replace('.csv','')\n",
      "# of duplicate tiles: 4\n",
      "Final # of tiles: 103\n",
      "df shape :                                              s3_path  ... tile_num\n",
      "0  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0054\n",
      "2  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0043\n",
      "4  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0031\n",
      "6  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0042\n",
      "8  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0052\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "Writing tindex master csv: /projects/test_dps/ATL08_filt_tindex_master.csv\n"
     ]
    }
   ],
   "source": [
    "!python /projects/icesat2_boreal/lib/build_tindex_master_v2.py -t ATL08_filt -y 2022 -m 3 -o /projects/test_dps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "compound-bryan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[387, 388, 5, 6, 7, 10, 14, 16, 275, 21, 25, 27, 28, 29, 416, 37, 38, 296, 26025, 299, 300, 301, 48, 177, 50, 178, 60, 198, 3270, 72, 326, 327, 328, 26574, 354, 355, 356, 357, 247]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "beneficial-blackjack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-LC_filter True --extract_covars --do_30m --do_dps -years_list 2019 2020 2021 -o /projects/my-public-bucket/atl08_filt_covar_tiles -in_tile_num 131 -in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg -in_tile_layer boreal_tiles_v003 -in_tile_id_col tile_num -csv_list_fn /projects/shared-buckets/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv -topo_stack_list_fn /projects/shared-buckets/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv -landsat_stack_list_fn /projects/shared-buckets/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv -user_stacks nathanmthomas -user_atl08 lduncanson -thresh_sol_el 5 -v_ATL08 5 -minmonth 4 -maxmonth 10\n",
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "Land cover filtering set to: True\n",
      "\n",
      "Working on tile:\t 131\n",
      "From layer:\t\t boreal_tiles_v003\n",
      "In vector file:\t\t /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg\n",
      "ATL08 version:\t\t 5\n",
      "Season start:\t\t 04-01\n",
      "Season end:\t\t 10-31\n",
      "Years:\t\t\t [2019, 2020, 2021]\n",
      "ATL08 bin length:\t 30m\n",
      "\n",
      "Doing MAAP query by tile bounds to find all intersecting ATL08 \n",
      "\tTILE_NUM: 131 (13.542284675000083,63.85506319420587,16.20544971460586,64.97513449757871)\n",
      "\tSearching MAAP for granules using these parameters: \n",
      "\t[{'short_name': 'ATL08', 'version': '005', 'bounding_box': '13.542284675000083,63.85506319420587,16.20544971460586,64.97513449757871', 'limit': 10000, 'temporal': '2019-04-01T00:00:00Z,2019-10-31T23:59:59Z'}, {'short_name': 'ATL08', 'version': '005', 'bounding_box': '13.542284675000083,63.85506319420587,16.20544971460586,64.97513449757871', 'limit': 10000, 'temporal': '2020-04-01T00:00:00Z,2020-10-31T23:59:59Z'}, {'short_name': 'ATL08', 'version': '005', 'bounding_box': '13.542284675000083,63.85506319420587,16.20544971460586,64.97513449757871', 'limit': 10000, 'temporal': '2021-04-01T00:00:00Z,2021-10-31T23:59:59Z'}]\n",
      "\t\t# ATL08 for tile 131: 195\n",
      "\n",
      "This is either a DPS job (for which the CSV has an s3 path, or the CSV exists locally.)\n",
      "\n",
      "Reading existing list of ATL08 CSVs: /projects/shared-buckets/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv\n",
      "\tDoing 30m ATL08 data?  True\n",
      "\tFind ATL08 CSVs you expect for a tile based on the h5 granule search...\n",
      "\t\t# of all ATL08 granules for tile: 195\n",
      "\t\t# of all_atl08_csvs: 46166\n",
      "\t# of ATL08 CSV found for tile 131: 158\n",
      "\t# of ATL08 CSV NOT found for tile 131: 37\n",
      "Creating pandas data frame...\n",
      "\n",
      "Filtering by tile: 131\n",
      "[13.542284675000083, 16.20544971460586, 63.85506319420587, 64.97513449757871]\n",
      "Bounds clipped 67410 obs. down to 8432 obs.\n",
      "Bounds clipped 33050 obs. down to 1581 obs.\n",
      "Bounds clipped 39560 obs. down to 0 obs.\n",
      "Bounds clipped 50324 obs. down to 3208 obs.\n",
      "Bounds clipped 95567 obs. down to 11660 obs.\n",
      "Bounds clipped 111142 obs. down to 2070 obs.\n",
      "Bounds clipped 105092 obs. down to 811 obs.\n",
      "Bounds clipped 110665 obs. down to 8897 obs.\n",
      "Bounds clipped 8143 obs. down to 0 obs.\n",
      "Bounds clipped 34114 obs. down to 0 obs.\n",
      "Bounds clipped 47439 obs. down to 666 obs.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/projects/icesat2_boreal/lib/tile_atl08.py\", line 386, in <module>\n",
      "    main()\n",
      "  File \"/projects/icesat2_boreal/lib/tile_atl08.py\", line 308, in main\n",
      "    atl08 = pd.concat([  FilterUtils.filter_atl08_bounds_clip(pd.read_csv(f), tile['geom_4326']) for f in all_atl08_csvs_FOUND ], sort=False, ignore_index=True)\n",
      "  File \"/projects/icesat2_boreal/lib/tile_atl08.py\", line 308, in <listcomp>\n",
      "    atl08 = pd.concat([  FilterUtils.filter_atl08_bounds_clip(pd.read_csv(f), tile['geom_4326']) for f in all_atl08_csvs_FOUND ], sort=False, ignore_index=True)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/io/parsers.py\", line 468, in _read\n",
      "    return parser.read(nrows)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1057, in read\n",
      "    index, columns, col_dict = self._engine.read(nrows)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/io/parsers.py\", line 2061, in read\n",
      "    data = self._reader.read(nrows)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 756, in pandas._libs.parsers.TextReader.read\n",
      "  File \"pandas/_libs/parsers.pyx\", line 771, in pandas._libs.parsers.TextReader._read_low_memory\n",
      "  File \"pandas/_libs/parsers.pyx\", line 827, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"pandas/_libs/parsers.pyx\", line 814, in pandas._libs.parsers.TextReader._tokenize_rows\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1943, in pandas._libs.parsers.raise_parser_error\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/s3fs/core.py\", line 1938, in _fetch_range\n",
      "    req_kw=self.req_kw,\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/s3fs/core.py\", line 2081, in _fetch_range\n",
      "    return sync(fs.loop, resp[\"Body\"].read)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/fsspec/asyn.py\", line 59, in sync\n",
      "    if event.wait(1):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 552, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 300, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "CPU times: user 480 ms, sys: 165 ms, total: 645 ms\n",
      "Wall time: 38.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TILE_NUM = 131 #NEED_TILES[6]\n",
    "args = f\"\\\n",
    "-LC_filter True \\\n",
    "--extract_covars \\\n",
    "--do_30m \\\n",
    "--do_dps \\\n",
    "-years_list 2019 2020 2021 \\\n",
    "-o /projects/my-public-bucket/atl08_filt_covar_tiles \\\n",
    "-in_tile_num {TILE_NUM} \\\n",
    "-in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg \\\n",
    "-in_tile_layer boreal_tiles_v003 \\\n",
    "-in_tile_id_col tile_num \\\n",
    "-csv_list_fn /projects/shared-buckets/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv \\\n",
    "-topo_stack_list_fn /projects/shared-buckets/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv \\\n",
    "-landsat_stack_list_fn /projects/shared-buckets/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv \\\n",
    "-user_stacks nathanmthomas \\\n",
    "-user_atl08 lduncanson \\\n",
    "-thresh_sol_el 5 \\\n",
    "-v_ATL08 5 -minmonth 4 -maxmonth 10\"\n",
    "print(args)\n",
    "!python /projects/icesat2_boreal/lib/tile_atl08.py $args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
