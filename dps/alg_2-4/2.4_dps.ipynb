{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "normal-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host='api.ops.maap-project.org')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-universe",
   "metadata": {},
   "source": [
    "# Launch DPS for tile_atl08.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "taken-roman",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.7/site-packages (0.12.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "!pip install xmltodict\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exclusive-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_fn(stack_list_fn, in_tile_num):\n",
    "    # Find most recent topo/Landsat stack path for tile in list of stack paths from *tindex_master.csv\n",
    "    all_stacks_df = pd.read_csv(stack_list_fn)\n",
    "    stack_for_tile = all_stacks_df[all_stacks_df['location'].str.contains(\"_\"+str(in_tile_num))]\n",
    "    [print(i) for i in stack_for_tile.path.to_list()]\n",
    "    stack_for_tile_fn = stack_for_tile.path.to_list()[0]\n",
    "    if len(stack_for_tile)==0:\n",
    "        stack_for_tile_fn = None\n",
    "    return(stack_for_tile_fn)\n",
    "\n",
    "# nmt added: code that returns df of landsat locations and tile number\n",
    "# This is basically CountOutput.py\n",
    "def get_stack_df(dps_dir, TYPE, dps_year):\n",
    "    \n",
    "    if \"Landsat\" in TYPE:\n",
    "        root = f\"/projects/my-private-bucket/dps_output/do_landsat_stack_3-1-2_ubuntu/ops/{dps_year}/\"\n",
    "        ends_with_str = \"_dps.tif\"\n",
    "    if \"Topo\" in TYPE:\n",
    "        root = f\"/projects/my-private-bucket/dps_output/do_topo_stack_3-1-5_ubuntu/ops/{dps_year}/\"\n",
    "        ends_with_str = \"_stack.tif\"\n",
    "    if \"ATL08\" in TYPE:\n",
    "        root = f\"/projects/my-private-bucket/dps_output/run_extract_ubuntu/ops/{dps_year}/\"\n",
    "        ends_with_str = \"0m.csv\"\n",
    "            \n",
    "    df = pd.DataFrame(columns=['location', 'tile_num'])\n",
    "\n",
    "    for dir, subdir, files in os.walk(root):\n",
    "        for fname in files:\n",
    "            if fname.endswith(ends_with_str): \n",
    "                 \n",
    "                tile_num = fname.split('_')[1]\n",
    "                   \n",
    "                if \"ATL08\" in TYPE:\n",
    "                    df = df.append({'location':os.path.join(dir+\"/\", fname)},ignore_index=True)\n",
    "                else:\n",
    "                    df = df.append({'location':os.path.join(dir+\"/\", fname), 'tile_num':tile_num},ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-flooring",
   "metadata": {},
   "source": [
    "#### Set the names of the data frames to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "actual-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topo and Landsat tindex_master csvs from build_tindex_master.py\n",
    "topo_tindex = \"/projects/my-public-bucket/DPS_tile_lists/Topo_tindex_master.csv\"\n",
    "landsat_tindex = \"/projects/my-public-bucket/DPS_tile_lists/Landsat_tindex_master.csv\"\n",
    "\n",
    "# Model-ready subset of tiles for which Topo and Landsat coincide\n",
    "model_ready_tiles_topo = \"/projects/my-public-bucket/DPS_tile_lists/model_ready_tiles_topo_paths.csv\"\n",
    "model_ready_tiles_landsat = \"/projects/my-public-bucket/DPS_tile_lists/model_ready_tiles_landsat_paths.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-miller",
   "metadata": {},
   "source": [
    "## Make the data frames from build_tindex_master.py csvs for Topo and Landsat tiles\n",
    "python lib/build_tindex_master.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "upset-muscle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading existing...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>local_path</th>\n",
       "      <th>tile_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/do_topo...</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/do_topo...</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/do_topo...</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/do_topo...</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/do_topo...</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         local_path  tile_num\n",
       "0           0  /projects/my-private-bucket/dps_output/do_topo...       421\n",
       "1           1  /projects/my-private-bucket/dps_output/do_topo...       455\n",
       "2           2  /projects/my-private-bucket/dps_output/do_topo...       456\n",
       "3           3  /projects/my-private-bucket/dps_output/do_topo...       491\n",
       "4           4  /projects/my-private-bucket/dps_output/do_topo...       492"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile(landsat_tindex) and os.path.isfile(topo_tindex):\n",
    "    print('Reading existing...')\n",
    "    ls8_df = pd.read_csv(landsat_tindex)\n",
    "    topo_df = pd.read_csv(topo_tindex)\n",
    "else:\n",
    "    s3_stem = 'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/nathanmthomas'\n",
    "    local_stem = '/projects/my-private-bucket'\n",
    "\n",
    "    ls8_root =  s3_stem + '/dps_output/do_landsat_stack_3-1-2_ubuntu'\n",
    "    topo_root = s3_stem + '/dps_output/do_topo_stack_3-1-5_ubuntu'\n",
    "    \n",
    "    ls8_df = get_stack_df(ls8_root, \"Landsat\")\n",
    "    topo_df = get_stack_df(topo_root, \"Topo\")\n",
    "topo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expired-soviet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/nathanmthomas/dps_output/do_topo_stack_3-1-5_ubuntu/ops/2021/07/23/23/32/27/934649/Copernicus_3457_covars_cog_topo_stack.tif'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "topo_df = pd.read_csv(topo_tindex)\n",
    "topo_df[topo_df.tile_num == 3457].local_path.tolist()[0].replace('/projects/my-private-bucket', 'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/nathanmthomas')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-necklace",
   "metadata": {},
   "source": [
    "## Get tile ids for which both Topo and Landsat stacks exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "economic-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by nmt: get filenames of co-incident landsat and topo\n",
    "if False:\n",
    "    topo_sub_df = pd.DataFrame(columns=['local_path','tile_num'])\n",
    "    ls8_sub_df = pd.DataFrame(columns=['local_path','tile_num'])\n",
    "\n",
    "    for i in range(len(ls8_df['tile_num'])):\n",
    "        ls_tile_num = ls8_df['tile_num'][i]\n",
    "        for j in range(len(topo_df['tile_num'])):\n",
    "            topo_tile_num = topo_df['tile_num'][j]\n",
    "            if ls_tile_num == topo_tile_num:\n",
    "                # Only need to choose one, but we'll do 2 and then check\n",
    "                ls8_sub_df = ls8_sub_df.append({'local_path':ls8_df['local_path'][i],'tile_num':ls8_df['tile_num'][i].astype(int)}, ignore_index=True)\n",
    "                topo_sub_df = topo_sub_df.append({'local_path':topo_df['local_path'][j],'tile_num':topo_df['tile_num'][j].astype(int)}, ignore_index=True)\n",
    "\n",
    "    #ls8_sub_df['tile_num'] = ls8_sub_df['tile_num'].astype(float, errors = 'raise')\n",
    "    print(ls8_sub_df.head())\n",
    "    print(topo_sub_df.head())\n",
    "    print(len(ls8_sub_df),len(topo_sub_df))\n",
    "\n",
    "    topo_sub_df.to_csv( model_ready_tiles_topo, index=False, encoding='utf-8-sig')\n",
    "    ls8_sub_df.to_csv( model_ready_tiles_landsat, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-indonesian",
   "metadata": {},
   "source": [
    "#### Now you have a set of tile ids for which both Landsat and Topo stacks exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dietary-canadian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4465"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topo_sub_df = pd.read_csv(\"/projects/my-public-bucket/DPS_tile_lists/model_ready_tiles_topo_paths.csv\")\n",
    "INPUT_TILE_NUM_LIST = topo_sub_df['tile_num'].values.astype(int).tolist()\n",
    "len(INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-christopher",
   "metadata": {},
   "source": [
    "##### Test: get a subset of tile ids for test tiles (Norway and others in NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confident-insulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORWAY_TILE_LIST = pd.read_csv('/projects/my-public-bucket/misc_files/norway_tiles.csv').layer.tolist()\n",
    "DELTA_TILE_LIST = [3365,3366,3367,3458,3460,3353,3354,3355]\n",
    "BONA_TILE_LIST  = [3270,3271,3272,3364,3456,3457,3458]\n",
    "HEALY_TILE_LIST = [3456,3457,3458,3551,3553,3645,3646,3647]\n",
    "\n",
    "INPUT_TEST_TILE_NUM_LIST = NORWAY_TILE_LIST + DELTA_TILE_LIST + BONA_TILE_LIST + HEALY_TILE_LIST\n",
    "\n",
    "len(INPUT_TEST_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-knitting",
   "metadata": {},
   "source": [
    "#### Read in the latest tindex and compare with a previous set of completed tiles to see which ones still need to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "statewide-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiles completed: 128\n",
      "Tiles missing: 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tiles_completed = pd.read_csv('/projects/my-public-bucket/DPS_tile_lists/ATL08_filt_tindex_master.csv')\n",
    "print(f'Tiles completed: {len(tiles_completed)}')\n",
    "tile_nums_missing = np.setdiff1d(INPUT_TEST_TILE_NUM_LIST, tiles_completed.tile_num)\n",
    "print(f'Tiles missing: {len(tile_nums_missing)}')\n",
    "INPUT_TILE_NUM_LIST = tile_nums_missing.tolist()\n",
    "len(INPUT_TILE_NUM_LIST)\n",
    "#print(INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "found-dance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46166"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tindex_master_fn = f'/projects/shared-buckets/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv'\n",
    "tiles = pd.read_csv(tindex_master_fn)\n",
    "len(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fewer-powder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "Building a list of tiles:  ATL08_filt\n",
      "\n",
      "Output dir:  /projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022/run_no_LC_height_thresholds\n",
      "                                             s3_path  ...                                               file\n",
      "0  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  atl08_005_30m_filt_topo_landsat_20220312_0132.csv\n",
      "2  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  atl08_005_30m_filt_topo_landsat_20220312_0010.csv\n",
      "4  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  atl08_005_30m_filt_topo_landsat_20220312_0131.csv\n",
      "6  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  atl08_005_30m_filt_topo_landsat_20220312_0043.csv\n",
      "8  s3://maap-ops-workspace/lduncanson/dps_output/...  ...  atl08_005_30m_filt_topo_landsat_20220312_0270.csv\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "# of duplicate tiles: 4\n",
      "Final # of tiles: 128\n",
      "df shape :                                              s3_path  ... tile_num\n",
      "0  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0132\n",
      "2  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0010\n",
      "4  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0131\n",
      "6  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0043\n",
      "8  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0270\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "Writing tindex master csv: /projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022/run_no_LC_height_thresholds/ATL08_filt_tindex_master.csv\n"
     ]
    }
   ],
   "source": [
    "month_dir_str = 'run_no_LC_height_thresholds'\n",
    "index_out_dir = os.path.join('/projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022', month_dir_str)\n",
    "!python /projects/icesat2_boreal/lib/build_tindex_master_v2.py -t ATL08_filt -y 2022 -m $month_dir_str -o $index_out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "connected-fancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "Building a list of tiles:  ATL08_filt\n",
      "\n",
      "Output dir:  /projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022/runs_LC_height_thresholds\n",
      "Traceback (most recent call last):\n",
      "  File \"/projects/icesat2_boreal/lib/build_tindex_master_v2.py\", line 165, in <module>\n",
      "    main()\n",
      "  File \"/projects/icesat2_boreal/lib/build_tindex_master_v2.py\", line 146, in main\n",
      "    df['tile_num'] = df['file'].str.split('_', expand=True)[7].str.replace('.csv','')\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/core/generic.py\", line 5458, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/core/accessor.py\", line 180, in __get__\n",
      "    accessor_obj = self._accessor(obj)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/core/strings/accessor.py\", line 154, in __init__\n",
      "    self._inferred_dtype = self._validate(data)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/core/strings/accessor.py\", line 217, in _validate\n",
      "    raise AttributeError(\"Can only use .str accessor with string values!\")\n",
      "AttributeError: Can only use .str accessor with string values!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022/runs_LC_height_thresholds/ATL08_filt_tindex_master.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5d80560a27fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python /projects/icesat2_boreal/lib/build_tindex_master_v2.py -t ATL08_filt -y 2022 -m $month_dir_str -o $index_out_dir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_out_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ATL08_filt_tindex_master.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mCOMPLETED_TILES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mNEED_TILES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDPS_INPUT_TILE_NUM_LIST\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMPLETED_TILES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             )\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/projects/my-private-bucket/dps_output/run_tile_atl08_ubuntu/master/2022/runs_LC_height_thresholds/ATL08_filt_tindex_master.csv'"
     ]
    }
   ],
   "source": [
    "TEST_DPS  = True\n",
    "\n",
    "if TEST_DPS:\n",
    "    DPS_INPUT_TILE_NUM_LIST = INPUT_TEST_TILE_NUM_LIST\n",
    "    \n",
    "    if True:\n",
    "        !python /projects/icesat2_boreal/lib/build_tindex_master_v2.py -t ATL08_filt -y 2022 -m $month_dir_str -o $index_out_dir\n",
    "        t = pd.read_csv(os.path.join(index_out_dir,'ATL08_filt_tindex_master.csv'))\n",
    "        COMPLETED_TILES = t.tile_num.to_list()\n",
    "        NEED_TILES = list(set(DPS_INPUT_TILE_NUM_LIST) - set(COMPLETED_TILES))\n",
    "\n",
    "        print(NEED_TILES)\n",
    "        DPS_INPUT_TILE_NUM_LIST = NEED_TILES\n",
    "    \n",
    "else:\n",
    "    DPS_INPUT_TILE_NUM_LIST = INPUT_TILE_NUM_LIST\n",
    "   \n",
    "\n",
    "DPS_INPUT_TILE_NUM_LIST = [248, 273, 272, 271, 324]\n",
    "print(f\"# of tiles to run: {len(DPS_INPUT_TILE_NUM_LIST)}\\n\", DPS_INPUT_TILE_NUM_LIST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-string",
   "metadata": {},
   "source": [
    "#### Customize the DPS run: set up the parameters dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "martial-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norway test 01\n",
    "# Just include sol_el so we can use sol_el < 5\n",
    "in_param_dict_norway01 = {\n",
    "                        'in_tile_num': '',\n",
    "                        'in_tile_fn': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "                        'in_tile_layer': 'boreal_tiles_v003',\n",
    "                        'csv_list_fn': 's3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv',\n",
    "                        'topo_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv',\n",
    "                        'landsat_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv',\n",
    "                        'years_list': '2019 2020 2021',\n",
    "                        'user_stacks': 'nathanmthomas',\n",
    "                        'user_atl08': 'lduncanson',\n",
    "                        'thresh_sol_el': 5,\n",
    "                        'v_ATL08': 5,\n",
    "                        'minmonth': 4,\n",
    "                        'maxmonth': 10,\n",
    "                        'LC_filter': False\n",
    "    }\n",
    "# Norway test 02\n",
    "# Use v005 ATL08, which will apply lc-based thresholds, extend to all months\n",
    "# NOTE!! make sure you manually update to use the correct filter in tile_atl08.py\n",
    "in_param_dict_norway02 = {\n",
    "                        'in_tile_num': '',\n",
    "                        'in_tile_fn': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "                        'in_tile_layer': 'boreal_tiles_v003',\n",
    "                        'csv_list_fn': 's3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv',\n",
    "                        'topo_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv',\n",
    "                        'landsat_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv',\n",
    "                        'years_list': '2019 2020 2021',\n",
    "                        'user_stacks': 'nathanmthomas',\n",
    "                        'user_atl08': 'lduncanson',\n",
    "                        'thresh_sol_el': 5,\n",
    "                        'v_ATL08': 5,\n",
    "                        'minmonth': 4,\n",
    "                        'maxmonth': 10,\n",
    "                        'LC_filter': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "equipped-rates",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_tile_num': '',\n",
       " 'in_tile_fn': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
       " 'in_tile_layer': 'boreal_tiles_v003',\n",
       " 'csv_list_fn': 's3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv',\n",
       " 'topo_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv',\n",
       " 'landsat_stack_list_fn': 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv',\n",
       " 'years_list': '2019 2020 2021',\n",
       " 'user_stacks': 'nathanmthomas',\n",
       " 'user_atl08': 'lduncanson',\n",
       " 'thresh_sol_el': 5,\n",
       " 'v_ATL08': 5,\n",
       " 'minmonth': 4,\n",
       " 'maxmonth': 10,\n",
       " 'LC_filter': False}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_param_dict = in_param_dict_norway01\n",
    "in_param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-edwards",
   "metadata": {},
   "source": [
    "## Run a DPS job across the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "driven-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input tiles for DPS: 5\n",
      "DPS run #: 1\t| tile num: 248\t| submit status: success\t| job id: 6507ba59-7878-4e5d-a2bc-68c9d5761cbf\n",
      "DPS run #: 5\t| tile num: 324\t| submit status: success\t| job id: 0340e0ab-68fd-49cd-b66c-e018cbb35c0c\n",
      "Current time:\t202203121259\n",
      "CPU times: user 65.6 ms, sys: 7.89 ms, total: 73.4 ms\n",
      "Wall time: 988 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>http_status_code</th>\n",
       "      <th>job_id</th>\n",
       "      <th>dps_num</th>\n",
       "      <th>tile_num</th>\n",
       "      <th>submit_time</th>\n",
       "      <th>dbs_job_hour</th>\n",
       "      <th>algo_id</th>\n",
       "      <th>user</th>\n",
       "      <th>worker_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>6507ba59-7878-4e5d-a2bc-68c9d5761cbf</td>\n",
       "      <td>1</td>\n",
       "      <td>248</td>\n",
       "      <td>2022-03-12 12:59:29.707070</td>\n",
       "      <td>12</td>\n",
       "      <td>run_tile_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>3552542b-f48d-4fd6-96ea-68a7e81be94d</td>\n",
       "      <td>2</td>\n",
       "      <td>273</td>\n",
       "      <td>2022-03-12 12:59:29.791232</td>\n",
       "      <td>12</td>\n",
       "      <td>run_tile_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>c7bd92a3-9de2-4f5d-8b70-9f1e7f997069</td>\n",
       "      <td>3</td>\n",
       "      <td>272</td>\n",
       "      <td>2022-03-12 12:59:29.893361</td>\n",
       "      <td>12</td>\n",
       "      <td>run_tile_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>e11f02e3-671d-4831-bff2-0131d317555f</td>\n",
       "      <td>4</td>\n",
       "      <td>271</td>\n",
       "      <td>2022-03-12 12:59:30.112634</td>\n",
       "      <td>12</td>\n",
       "      <td>run_tile_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>200</td>\n",
       "      <td>0340e0ab-68fd-49cd-b66c-e018cbb35c0c</td>\n",
       "      <td>5</td>\n",
       "      <td>324</td>\n",
       "      <td>2022-03-12 12:59:30.358500</td>\n",
       "      <td>12</td>\n",
       "      <td>run_tile_atl08_ubuntu</td>\n",
       "      <td>lduncanson</td>\n",
       "      <td>maap-dps-worker-8gb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    status  http_status_code                                job_id  dps_num  \\\n",
       "0  success               200  6507ba59-7878-4e5d-a2bc-68c9d5761cbf        1   \n",
       "0  success               200  3552542b-f48d-4fd6-96ea-68a7e81be94d        2   \n",
       "0  success               200  c7bd92a3-9de2-4f5d-8b70-9f1e7f997069        3   \n",
       "0  success               200  e11f02e3-671d-4831-bff2-0131d317555f        4   \n",
       "0  success               200  0340e0ab-68fd-49cd-b66c-e018cbb35c0c        5   \n",
       "\n",
       "   tile_num                submit_time  dbs_job_hour                algo_id  \\\n",
       "0       248 2022-03-12 12:59:29.707070            12  run_tile_atl08_ubuntu   \n",
       "0       273 2022-03-12 12:59:29.791232            12  run_tile_atl08_ubuntu   \n",
       "0       272 2022-03-12 12:59:29.893361            12  run_tile_atl08_ubuntu   \n",
       "0       271 2022-03-12 12:59:30.112634            12  run_tile_atl08_ubuntu   \n",
       "0       324 2022-03-12 12:59:30.358500            12  run_tile_atl08_ubuntu   \n",
       "\n",
       "         user          worker_type  \n",
       "0  lduncanson  maap-dps-worker-8gb  \n",
       "0  lduncanson  maap-dps-worker-8gb  \n",
       "0  lduncanson  maap-dps-worker-8gb  \n",
       "0  lduncanson  maap-dps-worker-8gb  \n",
       "0  lduncanson  maap-dps-worker-8gb  "
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(f\"# of input tiles for DPS: {len_input_list}\")\n",
    "\n",
    "for i, INPUT_TILE_NUM in enumerate(DPS_INPUT_TILE_NUM_LIST):\n",
    "\n",
    "    DPS_num = i+1\n",
    "    IDENTIFIER = 'run_tile_atl08'\n",
    "    ALGO_ID = f'{IDENTIFIER}_ubuntu'\n",
    "    USER = 'lduncanson'\n",
    "    WORKER_TYPE = 'maap-dps-worker-8gb'\n",
    "    \n",
    "    in_param_dict['in_tile_num'] = INPUT_TILE_NUM\n",
    "    \n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version='master',\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_param_dict\n",
    "        )\n",
    "    \n",
    "    # Build a dataframe of submission details\n",
    "    submit_result['dps_num'] = DPS_num\n",
    "    submit_result['tile_num'] = INPUT_TILE_NUM\n",
    "    submit_result['submit_time'] = datetime.datetime.now()\n",
    "    submit_result['dbs_job_hour'] =datetime.datetime.now().hour\n",
    "    submit_result['algo_id'] = ALGO_ID\n",
    "    submit_result['user'] = USER\n",
    "    submit_result['worker_type'] = WORKER_TYPE\n",
    "    \n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(pd.DataFrame([submit_result]))\n",
    "    \n",
    "    if DPS_num in [1, 25, 50, 100, 500, 1000, 1500, 2000, 3000, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000, len_input_list]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| tile num: {INPUT_TILE_NUM}\\t| submit status: {submit_result['status']}\\t| job id: {submit_result['job_id']}\") \n",
    "\n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "print(f\"Current time:\\t{nowtime}\")\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/DPS_{IDENTIFIER}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-coordination",
   "metadata": {},
   "source": [
    "After almost any DPS job, you have to assess what succeeded and failed. This involves:\n",
    "1. building a table of job status based on job ids captured in the job_results_df from the DPS run chunk (this takes 40 mins for ~47k jobs) --> this tells you how many jobs failed\n",
    "2. merging the job status table with the job results df --> this tells you which specific granules (or tile nums) failed\n",
    "3. building another input list of granules for a follow-up DPS\n",
    "## Assess DPS results\n",
    "Build a table of job status based on job id - how many jobs failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "prime-campus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count total jobs:\t5\n",
      "Count pending jobs:\t0\n",
      "Count running jobs:\t0\n",
      "Count succeeded jobs:\t5\n",
      "Count failed jobs:\t0\n",
      "% of failed jobs:\t0.0\n",
      "CPU times: user 52.1 ms, sys: 16.4 ms, total: 68.6 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def BUILD_TABLE_JOBSTATUS(submit_results_df):\n",
    "    import xmltodict\n",
    "    df = pd.concat([pd.DataFrame(xmltodict.parse(maap.getJobStatus(job_id).content)).transpose() for job_id in submit_results_df.job_id.to_list()])\n",
    "    return df\n",
    "\n",
    "job_status_df = BUILD_TABLE_JOBSTATUS(submit_results_df)\n",
    "#print(job_status_df.head())\n",
    "\n",
    "num_jobs = submit_results_df.shape[0]\n",
    "z = submit_results_df.merge(job_status_df, how='left', left_on='job_id',  right_on='wps:JobID')\n",
    "\n",
    "print(f'Count total jobs:\\t{num_jobs}')\n",
    "print(f\"Count pending jobs:\\t{z[z['wps:Status'] =='Accepted'].shape[0]}\")\n",
    "print(f\"Count running jobs:\\t{z[z['wps:Status'] =='Running'].shape[0]}\")\n",
    "print(f\"Count succeeded jobs:\\t{z[z['wps:Status'] =='Succeeded'].shape[0]}\")\n",
    "print(f\"Count failed jobs:\\t{z[z['wps:Status'] =='Failed'].shape[0]}\")\n",
    "print(f\"% of failed jobs:\\t{round(z[z['wps:Status'] =='Failed'].shape[0] / ( z[z['wps:Status'] =='Failed'].shape[0] + z[z['wps:Status'] =='Succeeded'].shape[0] ), 4) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "affected-recorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('wps:Result',\n",
       "              OrderedDict([('@xmlns:ows', 'http://www.opengis.net/ows/2.0'),\n",
       "                           ('@xmlns:schemaLocation',\n",
       "                            'http://schemas.opengis.net/wps/2.0/wps.xsd'),\n",
       "                           ('@xmlns:wps', 'http://www.opengis.net/wps/2.0'),\n",
       "                           ('@xmlns:xsi',\n",
       "                            'http://www.w3.org/2001/XMLSchema-instance'),\n",
       "                           ('wps:JobID',\n",
       "                            '6b40fec8-0275-4f4e-b751-fdf4b86bfd88'),\n",
       "                           ('wps:Output',\n",
       "                            OrderedDict([('@id',\n",
       "                                          'output-2022-03-12T05:19:20.030527'),\n",
       "                                         ('wps:Data',\n",
       "                                          ['http://maap-ops-workspace.s3-website-us-west-2.amazonaws.com/lduncanson/dps_output/run_tile_atl08_ubuntu/master/2022/03/12/05/19/20/030527',\n",
       "                                           's3://s3.us-west-2.amazonaws.com:80/maap-ops-workspace/lduncanson/dps_output/run_tile_atl08_ubuntu/master/2022/03/12/05/19/20/030527',\n",
       "                                           'https://s3.console.aws.amazon.com/s3/buckets/maap-ops-workspace/lduncanson/dps_output/run_tile_atl08_ubuntu/master/2022/03/12/05/19/20/030527/?region=us-east-1&tab=overview'])]))]))])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Succeeded'].iloc[0].job_id).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "aging-lease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('wps:Result',\n",
       "              OrderedDict([('@xmlns:ows', 'http://www.opengis.net/ows/2.0'),\n",
       "                           ('@xmlns:schemaLocation',\n",
       "                            'http://schemas.opengis.net/wps/2.0/wps.xsd'),\n",
       "                           ('@xmlns:wps', 'http://www.opengis.net/wps/2.0'),\n",
       "                           ('@xmlns:xsi',\n",
       "                            'http://www.w3.org/2001/XMLSchema-instance'),\n",
       "                           ('wps:JobID',\n",
       "                            '850f7b6c-94a0-4f59-870f-acc705a1bab9'),\n",
       "                           ('wps:Output',\n",
       "                            OrderedDict([('@id', 'traceback'),\n",
       "                                         ('wps:Data',\n",
       "                                          'activate does not accept more than one argument:\\n[\\'/app/icesat2_boreal/dps/alg_2-4/run_tile_atl08.sh\\', \\'4\\', \\'boreal_tiles_v003\\', \\'s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv\\', \\'s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv\\', \\'s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv\\', \\'2019 2020 2021\\', \\'nathanmthomas\\', \\'lduncanson\\', \\'5\\', \\'5\\', \\'4\\', \\'10\\', \\'False\\']\\n\\n+ /app/icesat2_boreal/dps/alg_2-4/run_tile_atl08.sh 4 boreal_tiles_v003 s3://maap-ops-workspace/shared/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv \\'2019 2020 2021\\' nathanmthomas lduncanson 5 5 4 10 False\\n/home/ops/.local/lib/python3.9/site-packages/geopandas/_compat.py:106: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\\n  warnings.warn(\\nTraceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 159, in _new_conn\\n    conn = connection.create_connection(\\n  File \"/opt/conda/lib/python3.9/site-packages/urllib3/util/connection.py\", line 84, in create_connection\\n    raise err\\n  File \"/opt/conda/lib/python3.9/site-packages/urllib3/util/connection.py\", line 74, in create_connection\\n    sock.connect(sa)\\nTimeoutError: [Errno 110] Connection timed out\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\\n    httplib_response = self._make_request(\\n  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\\n    self._validate_conn(conn)\\n  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\\n    conn.connect()\\n  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 309, in connect\\n    conn = self._new_conn()\\n  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 171, in _new_conn\\n    raise NewConnectionError(\\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f29bfc02700>: Failed to establish a new connection: [Errno 110] Connection timed out\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/requests/adapters.py\", line 439, in send\\n    resp = conn.urlopen(\\n  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\\n    retries = retries.increment(\\n  File \"/opt/conda/lib/python3.9/site-packages/urllib3/util/retry.py\", line 446, in increment\\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=\\'api.ops.maap-project.org\\', port=443): Max retries exceeded with url: /api/cmr/granules?short_name=ATL08&version=005&bounding_box=6.672362303301861%2C56.48186687643973%2C8.748942733447695%2C57.586106963239565&temporal=2019-04-01T00%3A00%3A00Z%2C2019-10-31T23%3A59%3A59Z&page_num=4&page_size=20 (Caused by NewConnectionError(\\'<urllib3.connection.HTTPSConnection object at 0x7f29bfc02700>: Failed to establish a new connection: [Errno 110] Connection timed out\\'))\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/app/icesat2_boreal/dps/alg_2-4/../../lib/tile_atl08.py\", line 386, in <module>\\n    main()\\n  File \"/app/icesat2_boreal/dps/alg_2-4/../../lib/tile_atl08.py\", line 252, in main\\n    all_atl08_for_tile = ExtractUtils.maap_search_get_h5_list(tile_num=in_tile_num, id_col=in_tile_id_col, tile_fn=in_tile_fn, layer=in_tile_layer, DATE_START=date_start, DATE_END=date_end, YEARS=years_list, version=v_ATL08)\\n  File \"/app/icesat2_boreal/lib/ExtractUtils.py\", line 117, in maap_search_get_h5_list\\n    result_chain = itertools.chain.from_iterable([maap.searchGranule(**query) for query in queries])\\n  File \"/app/icesat2_boreal/lib/ExtractUtils.py\", line 117, in <listcomp>\\n    result_chain = itertools.chain.from_iterable([maap.searchGranule(**query) for query in queries])\\n  File \"/opt/conda/lib/python3.9/site-packages/maapPy-0.3.0-py3.9.egg/maap/maap.py\", line 112, in searchGranule\\n    results = self._CMR.get_search_results(url=self._SEARCH_GRANULE_URL, limit=limit, **kwargs)\\n  File \"/opt/conda/lib/python3.9/site-packages/maapPy-0.3.0-py3.9.egg/maap/utils/CMR.py\", line 35, in get_search_results\\n    response = requests.get(\\n  File \"/opt/conda/lib/python3.9/site-packages/requests/api.py\", line 76, in get\\n    return request(\\'get\\', url, params=params, **kwargs)\\n  File \"/opt/conda/lib/python3.9/site-packages/requests/api.py\", line 61, in request\\n    return session.request(method=method, url=url, **kwargs)\\n  File \"/opt/conda/lib/python3.9/site-packages/requests/sessions.py\", line 542, in request\\n    resp = self.send(prep, **send_kwargs)\\n  File \"/opt/conda/lib/python3.9/site-packages/requests/sessions.py\", line 655, in send\\n    r = adapter.send(request, **kwargs)\\n  File \"/opt/conda/lib/python3.9/site-packages/requests/adapters.py\", line 516, in send\\n    raise ConnectionError(e, request=request)\\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host=\\'api.ops.maap-project.org\\', port=443): Max retries exceeded with url: /api/cmr/granules?short_name=ATL08&version=005&bounding_box=6.672362303301861%2C56.48186687643973%2C8.748942733447695%2C57.586106963239565&temporal=2019-04-01T00%3A00%3A00Z%2C2019-10-31T23%3A59%3A59Z&page_num=4&page_size=20 (Caused by NewConnectionError(\\'<urllib3.connection.HTTPSConnection object at 0x7f29bfc02700>: Failed to establish a new connection: [Errno 110] Connection timed out\\'))\\n+ cp _stderr.txt _alt_traceback.txt')]))]))])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Failed'].iloc[0].job_id).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "buried-jumping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "Building a list of tiles:  ATL08_filt\n",
      "\n",
      "Output dir:  /projects/test_dps\n",
      "/projects/icesat2_boreal/lib/build_tindex_master_v2.py:154: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['tile_num'] = df['file'].str.split('_', expand=True)[7].str.replace('.csv','')\n",
      "# of duplicate tiles: 4\n",
      "Final # of tiles: 103\n",
      "df shape :                                              s3_path  ... tile_num\n",
      "0  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0054\n",
      "2  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0043\n",
      "4  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0031\n",
      "6  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0042\n",
      "8  s3://maap-ops-workspace/lduncanson/dps_output/...  ...     0052\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "Writing tindex master csv: /projects/test_dps/ATL08_filt_tindex_master.csv\n"
     ]
    }
   ],
   "source": [
    "!python /projects/icesat2_boreal/lib/build_tindex_master_v2.py -t ATL08_filt -y 2022 -m 3 -o /projects/test_dps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "matched-helena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[387, 388, 5, 6, 7, 10, 14, 16, 275, 21, 25, 27, 28, 29, 416, 37, 38, 296, 26025, 299, 300, 301, 48, 177, 50, 178, 60, 198, 3270, 72, 326, 327, 328, 26574, 354, 355, 356, 357, 247]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "understanding-beast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-LC_filter True --extract_covars --do_30m --do_dps -years_list 2019 2020 2021 -o /projects/my-public-bucket/atl08_filt_covar_tiles -in_tile_num 131 -in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg -in_tile_layer boreal_tiles_v003 -in_tile_id_col tile_num -csv_list_fn /projects/shared-buckets/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv -topo_stack_list_fn /projects/shared-buckets/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv -landsat_stack_list_fn /projects/shared-buckets/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv -user_stacks nathanmthomas -user_atl08 lduncanson -thresh_sol_el 5 -v_ATL08 5 -minmonth 4 -maxmonth 10\n",
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "Land cover filtering set to: True\n",
      "\n",
      "Working on tile:\t 131\n",
      "From layer:\t\t boreal_tiles_v003\n",
      "In vector file:\t\t /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg\n",
      "ATL08 version:\t\t 5\n",
      "Season start:\t\t 04-01\n",
      "Season end:\t\t 10-31\n",
      "Years:\t\t\t [2019, 2020, 2021]\n",
      "ATL08 bin length:\t 30m\n",
      "\n",
      "Doing MAAP query by tile bounds to find all intersecting ATL08 \n",
      "\tTILE_NUM: 131 (13.542284675000083,63.85506319420587,16.20544971460586,64.97513449757871)\n",
      "\tSearching MAAP for granules using these parameters: \n",
      "\t[{'short_name': 'ATL08', 'version': '005', 'bounding_box': '13.542284675000083,63.85506319420587,16.20544971460586,64.97513449757871', 'limit': 10000, 'temporal': '2019-04-01T00:00:00Z,2019-10-31T23:59:59Z'}, {'short_name': 'ATL08', 'version': '005', 'bounding_box': '13.542284675000083,63.85506319420587,16.20544971460586,64.97513449757871', 'limit': 10000, 'temporal': '2020-04-01T00:00:00Z,2020-10-31T23:59:59Z'}, {'short_name': 'ATL08', 'version': '005', 'bounding_box': '13.542284675000083,63.85506319420587,16.20544971460586,64.97513449757871', 'limit': 10000, 'temporal': '2021-04-01T00:00:00Z,2021-10-31T23:59:59Z'}]\n",
      "\t\t# ATL08 for tile 131: 195\n",
      "\n",
      "This is either a DPS job (for which the CSV has an s3 path, or the CSV exists locally.)\n",
      "\n",
      "Reading existing list of ATL08 CSVs: /projects/shared-buckets/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv\n",
      "\tDoing 30m ATL08 data?  True\n",
      "\tFind ATL08 CSVs you expect for a tile based on the h5 granule search...\n",
      "\t\t# of all ATL08 granules for tile: 195\n",
      "\t\t# of all_atl08_csvs: 46166\n",
      "\t# of ATL08 CSV found for tile 131: 158\n",
      "\t# of ATL08 CSV NOT found for tile 131: 37\n",
      "Creating pandas data frame...\n",
      "\n",
      "Filtering by tile: 131\n",
      "[13.542284675000083, 16.20544971460586, 63.85506319420587, 64.97513449757871]\n",
      "Bounds clipped 67410 obs. down to 8432 obs.\n",
      "Bounds clipped 33050 obs. down to 1581 obs.\n",
      "Bounds clipped 39560 obs. down to 0 obs.\n",
      "Bounds clipped 50324 obs. down to 3208 obs.\n",
      "Bounds clipped 95567 obs. down to 11660 obs.\n",
      "Bounds clipped 111142 obs. down to 2070 obs.\n",
      "Bounds clipped 105092 obs. down to 811 obs.\n",
      "Bounds clipped 110665 obs. down to 8897 obs.\n",
      "Bounds clipped 8143 obs. down to 0 obs.\n",
      "Bounds clipped 34114 obs. down to 0 obs.\n",
      "Bounds clipped 47439 obs. down to 666 obs.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/projects/icesat2_boreal/lib/tile_atl08.py\", line 386, in <module>\n",
      "    main()\n",
      "  File \"/projects/icesat2_boreal/lib/tile_atl08.py\", line 308, in main\n",
      "    atl08 = pd.concat([  FilterUtils.filter_atl08_bounds_clip(pd.read_csv(f), tile['geom_4326']) for f in all_atl08_csvs_FOUND ], sort=False, ignore_index=True)\n",
      "  File \"/projects/icesat2_boreal/lib/tile_atl08.py\", line 308, in <listcomp>\n",
      "    atl08 = pd.concat([  FilterUtils.filter_atl08_bounds_clip(pd.read_csv(f), tile['geom_4326']) for f in all_atl08_csvs_FOUND ], sort=False, ignore_index=True)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/io/parsers.py\", line 468, in _read\n",
      "    return parser.read(nrows)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1057, in read\n",
      "    index, columns, col_dict = self._engine.read(nrows)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/pandas/io/parsers.py\", line 2061, in read\n",
      "    data = self._reader.read(nrows)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 756, in pandas._libs.parsers.TextReader.read\n",
      "  File \"pandas/_libs/parsers.pyx\", line 771, in pandas._libs.parsers.TextReader._read_low_memory\n",
      "  File \"pandas/_libs/parsers.pyx\", line 827, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"pandas/_libs/parsers.pyx\", line 814, in pandas._libs.parsers.TextReader._tokenize_rows\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1943, in pandas._libs.parsers.raise_parser_error\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/s3fs/core.py\", line 1938, in _fetch_range\n",
      "    req_kw=self.req_kw,\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/s3fs/core.py\", line 2081, in _fetch_range\n",
      "    return sync(fs.loop, resp[\"Body\"].read)\n",
      "  File \"/projects/.local/lib/python3.7/site-packages/fsspec/asyn.py\", line 59, in sync\n",
      "    if event.wait(1):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 552, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 300, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "CPU times: user 480 ms, sys: 165 ms, total: 645 ms\n",
      "Wall time: 38.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TILE_NUM = 131 #NEED_TILES[6]\n",
    "args = f\"\\\n",
    "-LC_filter True \\\n",
    "--extract_covars \\\n",
    "--do_30m \\\n",
    "--do_dps \\\n",
    "-years_list 2019 2020 2021 \\\n",
    "-o /projects/my-public-bucket/atl08_filt_covar_tiles \\\n",
    "-in_tile_num {TILE_NUM} \\\n",
    "-in_tile_fn /projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg \\\n",
    "-in_tile_layer boreal_tiles_v003 \\\n",
    "-in_tile_id_col tile_num \\\n",
    "-csv_list_fn /projects/shared-buckets/lduncanson/DPS_tile_lists/ATL08_tindex_master.csv \\\n",
    "-topo_stack_list_fn /projects/shared-buckets/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv \\\n",
    "-landsat_stack_list_fn /projects/shared-buckets/nathanmthomas/DPS_tile_lists/Landsat_tindex_master.csv \\\n",
    "-user_stacks nathanmthomas \\\n",
    "-user_atl08 lduncanson \\\n",
    "-thresh_sol_el 5 \\\n",
    "-v_ATL08 5 -minmonth 4 -maxmonth 10\"\n",
    "print(args)\n",
    "!python /projects/icesat2_boreal/lib/tile_atl08.py $args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
