{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook version of 2.3_dps.py, which runs over a single tile\n",
    "#### cant get 2.3_dps.py to work, so alternate approach is to loop over tile ids and return indiv tile CSVs of ATL08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "#import pdal\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "import argparse\n",
    "\n",
    "from maap.maap import MAAP\n",
    "maap = MAAP()\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/projects/icesat2_boreal/notebooks/3.Gridded_product_development/\")\n",
    "sys.path.append('/projects/code/icesat2_boreal/notebooks/3.Gridded_product_development')\n",
    "\n",
    "sys.path.append('/projects/code/icesat2_boreal/notebooks/2.ICESat-2_processing')\n",
    "\n",
    "#TODO: how to get this import right if its in a different dir\n",
    "from CovariateUtils import get_index_tile\n",
    "from FilterUtils import *\n",
    "from ExtractUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h5_list(tile_num, tile_fn=\"/projects/maap-users/alexdevseed/boreal_tiles.gpkg\", layer=\"boreal_tiles_albers\",DATE_START='06-01', DATE_END='09-30', YEARS=[2019, 2020, 2021]):\n",
    "    '''\n",
    "    Return a list of ATL08 h5 names that intersect a tile for a give date range across a set of years\n",
    "    '''\n",
    "    tile_id = get_index_tile(tile_fn, tile_num, buffer=0, layer = layer)\n",
    "\n",
    "    in_bbox = \",\".join(str(coord) for coord in tile_id['bbox_4326'])\n",
    "    \n",
    "    print(\"\\tTILE_NUM: {} ({})\".format(tile_num, in_bbox) )\n",
    "    \n",
    "    out_crs = tile_id['tile_crs']\n",
    "    \n",
    "    DATE_START = DATE_START + 'T00:00:00Z' # SUMMER start\n",
    "    DATE_END = DATE_END + 'T23:59:59Z' # SUMMER end\n",
    "    \n",
    "    date_filters = [f'{year}-{DATE_START},{year}-{DATE_END}' for year in YEARS]\n",
    "    \n",
    "    base_query = {\n",
    "    'short_name':\"ATL08\",\n",
    "    'version':\"003\",\n",
    "    'bounding_box':in_bbox\n",
    "    }\n",
    "\n",
    "    #q3 = [build_query(copy.copy(base_query), date_filter) for date_filter in date_filters]\n",
    "    queries = [dict(base_query, temporal=date_filter) for date_filter in date_filters]\n",
    "    \n",
    "    # query CMR as many seasons as necessary\n",
    "    result_chain = itertools.chain.from_iterable([maap.searchGranule(**query) for query in queries])\n",
    "    \n",
    "    # This is the list of ATL08 that intersect the tile bounds\n",
    "    # Use this list of ATL08 to identify the ATL08 h5/CSV files you have already DPS'd.\n",
    "    # get the s3 urls for granules we want to process\n",
    "    granules = [item.getDownloadUrl() for item in result_chain]\n",
    "    \n",
    "    # Convert to just the h5 basenames (removing the s3 url)\n",
    "    out_h5_list = [os.path.basename(x) for x in granules]\n",
    "    \n",
    "    print(\"\\t\\t# ATL08 for tile {}: {}\".format(tile_num, len(out_h5_list)) )\n",
    "    \n",
    "    return(out_h5_list)\n",
    "\n",
    "def reorder_4326_bounds(boreal_tile_index_path, test_tile_id, buffer, layer):\n",
    "    \n",
    "    tile_parts = get_index_tile(boreal_tile_index_path, test_tile_id, buffer=buffer, layer=layer)\n",
    "    bounds_order = [0, 2, 1, 3]\n",
    "    out_4326_bounds = [tile_parts['bbox_4326'][i] for i in bounds_order]\n",
    "    \n",
    "    return(out_4326_bounds)\n",
    "\n",
    "def get_granules_list(granules):\n",
    "    '''\n",
    "    Function to get list of granules returned from maap.searchGranule()\n",
    "    '''\n",
    "    url_list = []\n",
    "    output_list = []\n",
    "    for res in granules:\n",
    "        url_list.append(res.getDownloadUrl())\n",
    "\n",
    "    for url in url_list:\n",
    "        if url[0:5] == 's3://':\n",
    "            url = url[5:].split('/')\n",
    "            url[0] += '.s3.amazonaws.com'\n",
    "            url = 'https://' + '/'.join(url)\n",
    "        output_list.append(url)\n",
    "    return output_list\n",
    "\n",
    "def prep_filter_atl08_qual(atl08):\n",
    "    '''\n",
    "    Run this data prep on a df built from all CSVs from a DPS of extract_atl08.py for v003 of ATL08\n",
    "    '''\n",
    "    \n",
    "    print(\"\\nPre-filter data cleaning...\")\n",
    "    print(\"\\nGet beam type from orbit orientation and ground track...\") \n",
    "    atl08.loc[( (atl08.orb_orient == 1 ) & (atl08['gt'].str.contains('r')) ), \"beam_type\"] = 'Strong' \n",
    "    atl08.loc[( (atl08.orb_orient == 1 ) & (atl08['gt'].str.contains('l')) ), \"beam_type\"] = 'Weak'\n",
    "    atl08.loc[( (atl08.orb_orient == 0 ) & (atl08['gt'].str.contains('r')) ), \"beam_type\"] = 'Weak'\n",
    "    atl08.loc[( (atl08.orb_orient == 0 ) & (atl08['gt'].str.contains('l')) ), \"beam_type\"] = 'Strong'\n",
    "    print(atl08.beam_type.unique())\n",
    "\n",
    "    cols_float = ['lat', 'lon', 'h_can', 'h_te_best', 'ter_slp'] \n",
    "    print(f\"Cast some columns to type float: {cols_float}\")\n",
    "    atl08[cols_float] = atl08[cols_float].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    cols_int = ['n_ca_ph', 'n_seg_ph', 'n_toc_ph']\n",
    "    print(f\"Cast some columns to type integer: {cols_int}\")\n",
    "    atl08[cols_int] = atl08[cols_int].apply(pd.to_numeric, downcast='signed', errors='coerce')\n",
    "    \n",
    "    if False:\n",
    "        cols_date = ['yr', 'm', 'd']\n",
    "        clist = [c for c in atl08.columns[atl08.dtypes == object] if c in cols_date ]\n",
    "\n",
    "        for c in clist:\n",
    "            print('DEBUG prep')\n",
    "            #Get rid of b strings and convert to int, then datetime\n",
    "            atl08[c] = atl08[c].str.strip(\"b\\'\\\"\").astype(int)\n",
    "    print('DEBUG prep')    \n",
    "    #if set(cols_date).issubset(atl08.columns):\n",
    "    #    print('DEBUG prep date')\n",
    "    #    atl08[\"date\"] = pd.to_datetime(atl08[\"yr\"]*1000 + atl08[\"d\"], format = \"%Y%j\")\n",
    "    \n",
    "    print(\"Returning a prepared dataframe.\")   \n",
    "    return(atl08)\n",
    "\n",
    "def filter_atl08_bounds_tile_ept(in_ept_fn, in_tile_fn, in_tile_num, in_tile_layer, output_dir):\n",
    "        '''Get bounds from a tile_id and apply to an EPT database\n",
    "            Return a path to a GEOJSON that is a subset of the ATL08 db\n",
    "        '''\n",
    "        \n",
    "        # Return the 4326 representation of the input <tile_id> geometry \n",
    "        tile_parts = get_index_tile(in_tile_fn, in_tile_num, buffer=0, layer = in_tile_layer)\n",
    "        geom_4326 = tile_parts[\"geom_4326\"]\n",
    "\n",
    "        xmin, xmax = geom_4326[0:2]\n",
    "        ymin, ymax = geom_4326[2:]\n",
    "        transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)\n",
    "        xmin, ymax = transformer.transform(xmin, ymax)\n",
    "        xmax, ymin = transformer.transform(xmax, ymin)\n",
    "        pdal_tile_bounds = f\"([{xmin}, {xmax}], [{ymin}, {ymax}])\"\n",
    "\n",
    "        # Spatial subset\n",
    "        pipeline_def = [\n",
    "            {\n",
    "                \"type\": \"readers.ept\",\n",
    "                \"filename\": in_ept_fn\n",
    "            },\n",
    "            {\n",
    "                \"type\":\"filters.crop\",\n",
    "                \"bounds\": pdal_tile_bounds\n",
    "            },\n",
    "            {\n",
    "                \"type\" : \"writers.text\",\n",
    "                \"format\": \"geojson\",\n",
    "                \"write_header\": True\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Output the spatial subset as a geojson\n",
    "        out_fn = os.path.join(output_dir, os.path.split(os.path.splitext(in_ept_fn)[0])[1] + \"_\" + in_tile_num + \".geojson\")\n",
    "        run_pipeline(pipeline_def, out_fn)\n",
    "        \n",
    "        return(out_fn)\n",
    "\n",
    "def filter_atl08_bounds(atl08_df=None, in_bounds=None, in_ept_fn=None, in_tile_fn=None, in_tile_num=None, in_tile_layer=None, output_dir=None, return_pdf=False):\n",
    "    '''\n",
    "    Filter an ATL08 database using bounds.\n",
    "    Bounds can come from an input vector tile or a list: [xmin,xmax,ymin,ymax]\n",
    "    '''\n",
    "    out_fn = None\n",
    "    \n",
    "    if all(v is not None for v in [in_ept_fn, in_tile_fn, in_tile_num, in_tile_layer, output_dir]):\n",
    "        #\n",
    "        out_fn = filter_atl08_bounds_tile_ept(in_ept_fn, in_tile_fn, in_tile_num, in_tile_layer, output_dir)\n",
    "    elif in_bounds is not None and atl08_df is not None:\n",
    "        \n",
    "        print(\"Filtering by bounds: {}\".format(in_bounds) )\n",
    "        xmin = in_bounds[0]\n",
    "        xmax = in_bounds[1]\n",
    "        ymin = in_bounds[2]\n",
    "        ymax = in_bounds[3]\n",
    "        \n",
    "        print(\"Returning a data frame\")\n",
    "        return_pdf = True\n",
    "        \n",
    "        atl08_df_prepd = prep_filter_atl08_qual(atl08_df)\n",
    "        atl08_df = None\n",
    "        \n",
    "        if return_pdf :\n",
    "            print('DEBUG atl08_df_prepd')\n",
    "            atl08_df = atl08_df_prepd[(atl08_df_prepd.lat > float(ymin)) &\n",
    "                                        (atl08_df_prepd.lat < float(ymax)) &\n",
    "                                        (atl08_df_prepd.lon > float(xmin)) &\n",
    "                                        (atl08_df_prepd.lon < float(xmax))\n",
    "                               ]\n",
    "            \n",
    "    else:\n",
    "        print(\"Missing input args; can't filter. Check call.\")\n",
    "        os._exit(1)\n",
    "    \n",
    "    if return_pdf:\n",
    "        print(\"Filtered bounds, returning a pandas dataframe.\")\n",
    "        if out_fn is not None:\n",
    "            atl08_df = gpd.read(out_fn)\n",
    "        return(atl08_df)\n",
    "    else:\n",
    "        print(out_fn)\n",
    "        return(out_fn)\n",
    "\n",
    "def filter_atl08_qual(input_fn=None, subset_cols_list=['rh25','rh50','rh60','rh70','rh75','rh80','rh85','rh90','rh95','h_can','h_max_can'], filt_cols = ['h_can','h_dif_ref','m','msw_flg','beam_type','seg_snow'], thresh_h_can=None, thresh_h_dif=None, month_min=None, month_max=None, SUBSET_COLS=True):\n",
    "    '''\n",
    "    Quality filtering Function\n",
    "    Returns a data frame\n",
    "    Note: beams 1 & 5 strong (better radiometric perf, sensitive), then beam 3 [NOT IMPLEMENTED]\n",
    "    '''\n",
    "    # TODO: filt col names: make sure you have these in the EPT db\n",
    "    \n",
    "    if not subset_cols_list:\n",
    "        print(\"filter_atl08: Must supply a list of strings matching ATL08 column names returned from the input EPT\")\n",
    "        os._exit(1) \n",
    "    elif thresh_h_can is None:\n",
    "        print(\"filter_atl08: Must supply a threshold for h_can\")\n",
    "        os._exit(1)    \n",
    "    elif thresh_h_dif is None:\n",
    "        print(\"filter_atl08: Must supply a threshold for h_dif_ref\")\n",
    "        os._exit(1)\n",
    "    elif month_min is None or month_max is None:\n",
    "        print(\"filter_atl08: Must supply a month_min and month_max\")\n",
    "        os._exit(1)  \n",
    "        \n",
    "    if input_fn is not None:\n",
    "        if not isinstance(input_fn, pd.DataFrame):\n",
    "            if input_fn.endswith('geojson'):\n",
    "                atl08_df = gpd.read(input_fn)\n",
    "            elif input_fn.endswith('csv'):\n",
    "                atl08_df = pd.read_csv(input_fn)\n",
    "            else:\n",
    "                print(\"Input filename must be a CSV, GEOJSON, or pd.DataFrame\")\n",
    "                os._exit(1)\n",
    "        else:\n",
    "            atl08_df = input_fn\n",
    "            \n",
    "    # Run the prep to get fields needed (v003)\n",
    "    atl08_df_prepd = prep_filter_atl08_qual(atl08_df)\n",
    "    atl08_df = None\n",
    "    \n",
    "    # Check that you have the cols that are required for the filter\n",
    "    filt_cols_not_in_df = [col for col in filt_cols if col not in atl08_df_prepd.columns] \n",
    "    if len(filt_cols_not_in_df) > 0:\n",
    "        print(\"These filter columns not found in input df: {}\".format(filt_cols_not_in_df))\n",
    "        os._exit(1)\n",
    "    \n",
    "    # Filtering\n",
    "    #\n",
    "    \n",
    "    # Filter list (keep):\n",
    "    #   h_ref_diff < thresh_h_dif\n",
    "    #   h_can < thresh_h_can\n",
    "    #   no LC forest masking: only forest LC classes no good b/c trees outside of forest aer of interest (woodlands, etc)\n",
    "    #   msw = 0\n",
    "    #   night better (but might exclude too much good summer data in the high northern lats)\n",
    "    #   strong beam\n",
    "    #   summer (june - mid sept)\n",
    "    #   seg_snow == 'snow free land'\n",
    "        \n",
    "    print(\"\\nFiltering for quality:\\n\\tfor clear skies + strong beam + snow free land,\\n\\th_can < {},\\n\\televation diff from ref < {},\\n\\tmonths {}-{}\".format(thresh_h_can, thresh_h_dif, month_min, month_max))\n",
    "    atl08_df_filt =  atl08_df_prepd[\n",
    "                                (atl08_df_prepd.h_can < thresh_h_can) &\n",
    "                                (atl08_df_prepd.h_dif_ref < thresh_h_dif) &\n",
    "                                (atl08_df_prepd.m >= month_min ) & \n",
    "                                (atl08_df_prepd.m <= month_max) &\n",
    "                                # Hard coded quality flags for ABoVE AGB\n",
    "                                (atl08_df_prepd.msw_flg == 0) &\n",
    "                                #(atl08_df.night_flg == 'night') & # might exclude too much good summer data in the high northern lats\n",
    "                                (atl08_df_prepd.beam_type == 'Strong') & \n",
    "                                (atl08_df_prepd.seg_snow == 'snow free land')\n",
    "                    ]\n",
    "        \n",
    "    print(f\"Before quaity filtering: {atl08_df_prepd.shape[0]} observations in the input dataframe.\")\n",
    "    print(f\"After quality filtering: {atl08_df_filt.shape[0]} observations in the output dataframe.\")\n",
    "    \n",
    "    atl08_df_prepd = None\n",
    "    \n",
    "    if SUBSET_COLS:\n",
    "        subset_cols_list = ['lon','lat'] + subset_cols_list\n",
    "        print(\"Returning a pandas data frame of filtered observations for columns: {}\".format(subset_cols_list))\n",
    "        print(f\"Shape: {atl08_df_filt[subset_cols_list].shape} \")\n",
    "        return(atl08_df_filt[subset_cols_list])\n",
    "    else:\n",
    "        print(\"Returning a pandas data frame of filtered observations for all columns\")\n",
    "        return(atl08_df_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maap_query = True\n",
    "dps_dir = '/projects/jabba/dps_output/2.3_output'#'/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu'\n",
    "output_dir = '/projects/jabba/data/out_tiles'\n",
    "TEST = True\n",
    "do_30m = True\n",
    "extract_covars = False\n",
    "\n",
    "in_tile_fn = '/projects/maap-users/alexdevseed/boreal_tiles.gpkg'\n",
    "in_tile_layer = 'boreal_tiles_albers'\n",
    "\n",
    "thresh_h_can = 100\n",
    "thresh_h_dif = 100\n",
    "month_min = 6\n",
    "month_max = 9\n",
    "\n",
    "date_start = '06-01'\n",
    "date_end = '09-30'\n",
    "\n",
    "# NA tiles\n",
    "# Read the boreal tile index file\n",
    "\n",
    "boreal_tile_index = gpd.read_file(in_tile_fn)\n",
    "boreal_tile_index_subset = boreal_tile_index.to_crs(4326).cx[-170:-50, 50:75]\n",
    "\n",
    "# Boreal NA tiles: need just a list of tile_ids\n",
    "INPUT_TILE_NUM_LIST = boreal_tile_index_subset['layer'].astype(int).tolist()\n",
    "\n",
    "INPUT_TILE_NUM_LIST[0:5]\n",
    "\n",
    "if False:\n",
    "    # TODO: Should not do glob.glob by tile\n",
    "    # Get a list of all ATL08 CSV files from (from extract_atl08) (this will be a large boreal list)\n",
    "    print(\"\\tDPS dir to find ATL08 CSVs: {}\".format(dps_dir))\n",
    "    all_atl08_csvs = glob.glob(dps_dir + \"/**/ATL08*.csv\", recursive=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over input tiles to output ATL08 tile CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing MAAP query by tile bounds to find all intersecting ATL08 \n",
      "\tTILE_NUM: 30542 (-117.10749852280769,50.78795362739066,-116.50936927974429,51.16389512140189)\n",
      "\t\t# ATL08 for tile 30542: 23\n",
      "\tDPS dir to find ATL08 CSVs: /projects/jabba/dps_output/2.3_output\n",
      "Length of all_atl08_csvs: 6718\n",
      "\t# of ATL08 CSV found for tile 30542: 15\n",
      "Creating pandas data frame...\n",
      "\n",
      "Filtering by tile: 30542\n",
      "Filtering by bounds: [-117.10749852280769, -116.50936927974429, 50.78795362739066, 51.16389512140189]\n",
      "Returning a data frame\n",
      "\n",
      "Pre-filter data cleaning...\n",
      "\n",
      "Get beam type from orbit orientation and ground track...\n",
      "['Weak' 'Strong']\n",
      "Cast some columns to type float: ['lat', 'lon', 'h_can', 'h_te_best', 'ter_slp']\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#for in_tile_num in INPUT_TILE_NUM_LIST[100:105]:\n",
    "for in_tile_num in [30542]:#, 30543, 30821, 30822, 30823]:\n",
    "    \n",
    "    # TODO: make this an arg\n",
    "    years_list = [2019, 2020, 2021]\n",
    "    \n",
    "    seg_str = '_100m'\n",
    "    if do_30m:\n",
    "        seg_str = '_30m'\n",
    "    if TEST:\n",
    "        seg_str = ''\n",
    "    \n",
    "    if maap_query and dps_dir is not None:\n",
    "        \n",
    "        print(\"\\nDoing MAAP query by tile bounds to find all intersecting ATL08 \")\n",
    "        # Get a list of all ATL08 H5 granule names intersecting the tile (this will be a small list)\n",
    "        # all_atl08_for_tile = ExtractUtils.get_h5_list() #<- when you get import to work, change back to this\n",
    "        all_atl08_for_tile = get_h5_list(tile_num=in_tile_num, tile_fn=in_tile_fn, layer=in_tile_layer, DATE_START=date_start, DATE_END=date_end, YEARS=years_list)\n",
    "        \n",
    "        # Change the small ATL08 H5 granule names to match the output filenames from extract_atl08.py (eg, ATL08_*_30m.csv)\n",
    "        all_atl08_csvs_for_tile_BASENAME = [os.path.basename(f).replace('.h5', seg_str+'.csv') for f in all_atl08_for_tile]\n",
    "        ##print(all_atl08_csvs_for_tile_BASENAME)\n",
    "        # Get a list of all ATL08 CSV files from (from extract_atl08) (this will be a large boreal list)\n",
    "        print(\"\\tDPS dir to find ATL08 CSVs: {}\".format(dps_dir))\n",
    "        all_atl08_csvs = glob.glob(dps_dir + \"/**/ATL08*\" + seg_str + \".csv\", recursive=True)\n",
    "        all_atl08_csvs_BASENAME = [os.path.basename(f) for f in all_atl08_csvs]\n",
    "        print('Length of all_atl08_csvs: {}'.format(len(all_atl08_csvs)))\n",
    "        # Get index of ATL08 in tile bounds from the large list of all ATL08 CSVs\n",
    "        ###names = [name for i, name in enumerate(all_atl08_csvs_for_tile_BASENAME) if name in set(all_atl08_csvs_BASENAME)]\n",
    "        ###print(names)\n",
    "        idx = [i for i, name in enumerate(all_atl08_csvs_for_tile_BASENAME) if name in set(all_atl08_csvs_BASENAME)]\n",
    "        # Get the subset of all ATL08 CSVs that just correspond to the ATL08 H5 intersecting the current tile\n",
    "        all_atl08_h5_with_csvs_for_tile = [all_atl08_for_tile[x] for x in idx]       \n",
    "        \n",
    "        # Check to make sure these are in fact files (necessary?)\n",
    "        all_atl08_csvs_NOT_FOUND = []\n",
    "        all_atl08_csvs_FOUND = []\n",
    "        for file in all_atl08_h5_with_csvs_for_tile:\n",
    "            file = os.path.join(dps_dir, os.path.basename(file).replace('.h5',seg_str+'.csv'))       \n",
    "            if not os.path.isfile(file):\n",
    "                all_atl08_csvs_NOT_FOUND.append(file)\n",
    "            else:\n",
    "                all_atl08_csvs_FOUND.append(file)\n",
    "\n",
    "        #all_atl08_csvs_FOUND = [x for x in all_atl08_h5_with_csvs_for_tile if x not in all_atl08_csvs_NOT_FOUND]\n",
    "        print(\"\\t# of ATL08 CSV found for tile {}: {}\".format(in_tile_num, len(all_atl08_csvs_FOUND)))\n",
    "        if len(all_atl08_csvs_FOUND) == 0:\n",
    "            print('\\tNo ATL08 extracted for this tile.')\n",
    "            continue\n",
    "        \n",
    "        # Merge all ATL08 CSV files for the current tile into a pandas df\n",
    "        print(\"Creating pandas data frame...\")\n",
    "        atl08 = pd.concat([pd.read_csv(f) for f in all_atl08_csvs_FOUND ], sort=False)\n",
    "        \n",
    "        print(\"\\nFiltering by tile: {}\".format(in_tile_num))\n",
    "    \n",
    "        # Get tile bounds as xmin,xmax,ymin,ymax\n",
    "        in_bounds = reorder_4326_bounds(in_tile_fn, in_tile_num, buffer=0, layer=in_tile_layer)\n",
    "        \n",
    "        # Now filter ATL08 obs by tile bounds\n",
    "        atl08 = filter_atl08_bounds(atl08_df=atl08, in_bounds=in_bounds)\n",
    "        \n",
    "    elif maap_query and dps_dir is None:\n",
    "        print(\"\\nNo DPS dir specified: cant get ATL08 CSV list to match with tile bound results from MAAP query.\\n\")\n",
    "        os._exit(1)\n",
    "    else:\n",
    "        # Filter by bounds: EPT with a the bounds from an input tile\n",
    "        atl08 = filter_atl08_bounds_tile_ept(in_ept_fn, in_tile_fn, in_tile_num, in_tile_layer, output_dir, return_pdf=True)\n",
    "    \n",
    "    ## Filter by quality: based on a standard filter_atl08_qual() function that we use across all notebooks, scripts, etc\n",
    "    #atl08_pdf_filt = FilterUtils.filter_atl08_qual(atl08, out_cols_list)\n",
    "    # Filter by quality\n",
    "    print('DEBUG qual filter.')\n",
    "    atl08_pdf_filt = filter_atl08_qual(atl08, SUBSET_COLS=True, \n",
    "                                                       subset_cols_list=['rh25','rh50','rh60','rh70','rh75','rh80','rh85','rh90','rh95','h_can','h_max_can'], \n",
    "                                                       filt_cols=['h_can','h_dif_ref','m','msw_flg','beam_type','seg_snow', 'seg_landcov'], \n",
    "                                                       thresh_h_can=100, thresh_h_dif=100, month_min=6, month_max=9)\n",
    "    atl08=None\n",
    "    \n",
    "    # Convert to geopandas data frame in lat/lon\n",
    "    atl08_gdf = GeoDataFrame(atl08_pdf_filt, geometry=gpd.points_from_xy(atl08_pdf_filt.lon, atl08_pdf_filt.lat), crs='epsg:4326')\n",
    "    out_name_stem = \"atl08_filt\"\n",
    "    atl08_pdf_filt=None\n",
    "    \n",
    "    if extract_covars:\n",
    "        ### Below here should be re-worked to follow final chunk of nb 2.3 (6/15/2021)\n",
    "        #\n",
    "        \n",
    "        # Extract topo covar values to ATL08 obs (doing a reproject to tile crs)\n",
    "        # TODO: consider just running 3.1.5_dpy.py here to produce this topo stack right before extracting its values\n",
    "        topo_covar_fn = do_3_1_5_dp.main(in_tile_fn=in_tile_fn, in_tile_num=in_tile_num, tile_buffer_m=120, in_tile_layer=in_tile_layer, topo_tile_fn='https://maap-ops-dataset.s3.amazonaws.com/maap-users/alexdevseed/dem30m_tiles.geojson')\n",
    "        atl08_gdf_out = ExtractUtils.extract_value_gdf(topo_covar_fn, atl08_gdf, [\"elevation\",\"slope\",\"tsri\",\"tpi\", \"slopemask\"], reproject=True)\n",
    "        out_name_stem = out_name_stem + \"_topo\"\n",
    "\n",
    "        # Extract landsat covar values to ATL08 obs\n",
    "        # TODO: consider just running 3.1.2_dpy.py here\n",
    "        landsat_covar_fn = do_3_1_2_dps.main(in_tile_fn=in_tile_fn, in_tile_num=in_tile_num, in_tile_layer=in_tile_layer, sat_api='https://landsatlook.usgs.gov/sat-api', local=args.local)\n",
    "        atl08_gdf_out = ExtractUtils.extract_value_gdf(landsat_covar_fn, atl08_gdf_out, ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo'], reproject=False)\n",
    "        out_name_stem = out_name_stem + \"_landsat\"\n",
    "        \n",
    "    # CSV the file\n",
    "    cur_time = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    out_csv_fn = os.path.join(output_dir, out_name_stem + \"_\" + cur_time + \".csv\")\n",
    "    atl08_gdf_out.to_csv(out_csv_fn,index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(\"Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile {}: {}\".format(in_tile_num, out_csv_fn) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
