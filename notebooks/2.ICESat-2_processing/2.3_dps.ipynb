{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook version of 2.3_dps.py, which runs over a single tile\n",
    "#### cant get 2.3_dps.py to work, so alternate approach is to loop over tile ids and return indiv tile CSVs of ATL08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.8.1-CAPI-1.13.3). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "#import pdal\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# From CovariateUtils\n",
    "import numpy\n",
    "from rasterio import enums\n",
    "from rasterio.io import MemoryFile\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.vrt import WarpedVRT\n",
    "from rasterio.warp import array_bounds, calculate_default_transform\n",
    "import geopandas\n",
    "\n",
    "import argparse\n",
    "\n",
    "from maap.maap import MAAP\n",
    "maap = MAAP()\n",
    "\n",
    "import sys\n",
    "if False:\n",
    "    sys.path.append('/projects/code/icesat2_boreal/notebooks/3.Gridded_product_development')\n",
    "    sys.path.append('/projects/code/icesat2_boreal/notebooks/2.ICESat-2_processing')\n",
    "\n",
    "    #TODO: how to get this import right if its in a different dir\n",
    "    from CovariateUtils import get_index_tile\n",
    "    \n",
    "import FilterUtils\n",
    "import ExtractUtils\n",
    "import CovariateUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maap_query = True\n",
    "dps_dir = '/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17' #'/projects/jabba/dps_output/2.3_output'#\n",
    "output_dir = '/projects/jabba/data/out_tiles'\n",
    "csv_list_fn = '/projects/jabba/data/extract_atl08_csv_list.csv'\n",
    "TEST = False\n",
    "do_30m = True\n",
    "extract_covars = False\n",
    "\n",
    "in_tile_fn = '/projects/maap-users/alexdevseed/boreal_tiles.gpkg'\n",
    "in_tile_layer = 'boreal_tiles_albers'\n",
    "\n",
    "thresh_h_can = 100\n",
    "thresh_h_dif = 100\n",
    "month_min = 6\n",
    "month_max = 9\n",
    "\n",
    "date_start = '06-01'\n",
    "date_end = '09-30'\n",
    "\n",
    "# NA tiles\n",
    "# Read the boreal tile index file\n",
    "\n",
    "boreal_tile_index = geopandas.read_file(in_tile_fn)\n",
    "boreal_tile_index_subset = boreal_tile_index.to_crs(4326).cx[-170:-50, 50:75]\n",
    "\n",
    "# Boreal NA tiles: need just a list of tile_ids\n",
    "INPUT_TILE_NUM_LIST = boreal_tile_index_subset['layer'].astype(int).tolist()\n",
    "\n",
    "INPUT_TILE_NUM_LIST[0:5]\n",
    "\n",
    "if False:\n",
    "    # TODO: Should not do glob.glob by tile\n",
    "    # Get a list of all ATL08 CSV files from (from extract_atl08) (this will be a large boreal list)\n",
    "    print(\"\\tDPS dir to find ATL08 CSVs: {}\".format(dps_dir))\n",
    "    all_atl08_csvs = glob.glob(dps_dir + \"/**/ATL08*.csv\", recursive=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the list of extracted CSV paths from DPS and save all paths in a list - glob.glob takes a while - dont do this for each tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This is sloooow\n",
    "if False:\n",
    "    print(dps_dir + \"/**/ATL08*\" + seg_str + \".csv\") \n",
    "    seg_str=\"_30m\"\n",
    "    all_atl08_csvs = glob.glob(dps_dir + \"/**/ATL08*\" + seg_str + \".csv\", recursive=True)\n",
    "    print(len(all_atl08_csvs))\n",
    "    all_atl08_csvs_df = pd.DataFrame({\"path\": all_atl08_csvs})\n",
    "    all_atl08_csvs_df.to_csv(csv_list_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_atl08_csv_tile_BACKUP(all_atl08_for_tile, all_atl08_csvs_df, seg_str):\n",
    "    \n",
    "    print(\"seg_str: \", seg_str)\n",
    "    print(\"\\tFind ATL08 CSVs for tile...\")\n",
    "    # Change the small ATL08 H5 granule names to match the output filenames from extract_atl08.py (eg, ATL08_*_30m.csv)\n",
    "    all_atl08_for_tile_CSVname = [os.path.basename(f).replace(\"ATL08\", \"ATL08\"+seg_str).replace('.h5', seg_str+'.csv') for f in all_atl08_for_tile]\n",
    "\n",
    "    print('\\t\\tLength of all ATL08 for tile: {}'.format(len(all_atl08_for_tile)))\n",
    "    all_atl08_csvs = all_atl08_csvs_df['path'].to_list()\n",
    "    print('\\t\\tLength of all_atl08_csvs: {}'.format(len(all_atl08_csvs)))\n",
    "    \n",
    "    # Get basenames of CSVs\n",
    "    all_atl08_csvs_BASENAME = [os.path.basename(f) for f in all_atl08_csvs]\n",
    "    \n",
    "    #print(all_atl08_for_tile_CSVname)\n",
    "    # Get index of ATL08 in tile bounds from the large list of all ATL08 CSVs\n",
    "    names = [name for i, name in enumerate(all_atl08_for_tile_CSVname) if name in set(all_atl08_csvs_BASENAME)]\n",
    "    print(names)\n",
    "    idx = [all_atl08_csvs_BASENAME.index(name) for name in names]\n",
    "    print(idx)\n",
    "    print('\\t\\tLength of idx with matches between ATL08 CSVs and ATL08 granules for tile: {}'.format(len(idx)))\n",
    "    all_atl08_csvs_FOUND  = [all_atl08_csvs[i] for i in idx]\n",
    "    print(all_atl08_csvs_FOUND)\n",
    "    ## Get the subset of all ATL08 CSVs that just correspond to the ATL08 H5 intersecting the current tile\n",
    "    #all_atl08_h5_with_csvs_for_tile = [all_atl08_for_tile[x] for x in idx]       \n",
    "    #print(all_atl08_h5_with_csvs_for_tile)\n",
    "\n",
    "    if False:\n",
    "        # Check to make sure these are in fact files (necessary?)\n",
    "        all_atl08_csvs_NOT_FOUND = []\n",
    "        all_atl08_csvs_FOUND = []\n",
    "        for file in all_atl08_h5_with_csvs_for_tile:\n",
    "            print(\"seg_str: \", seg_str)\n",
    "            # Convert the h5 file string back to the actual string of the CSV file\n",
    "            csv_fn = os.path.basename(file).replace('.h5',seg_str+'.csv').replace(\"ATL08_\",\"ATL08\"+seg_str+\"_\")\n",
    "            print(\"\\t\\tcsv_fn: \", csv_fn)\n",
    "            # Find this CSV path from large list\n",
    "            name = [name for i, name in enumerate(all_atl08_csvs) if name in csv_fn]\n",
    "            print(\"\\t\\tname: \",name)\n",
    "            file = os.path.join(dps_dir, csv_fn)    \n",
    "            print(file)\n",
    "            if not os.path.isfile(file):\n",
    "                all_atl08_csvs_NOT_FOUND.append(file)\n",
    "            else:\n",
    "                all_atl08_csvs_FOUND.append(file)\n",
    "            \n",
    "    return(all_atl08_csvs_FOUND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over input tiles to output ATL08 tile CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing MAAP query by tile bounds to find all intersecting ATL08 \n",
      "Getting tile polygon index...\n",
      "\tTILE_NUM: 30542 (-117.10749852280769,50.78795362739066,-116.50936927974429,51.16389512140189)\n",
      "\t\t# ATL08 for tile 30542: 23\n",
      "\tReading existing list og ATL08 CSVs: /projects/jabba/data/extract_atl08_csv_list.csv\n",
      "seg_str:  _30m\n",
      "\tFind ATL08 CSVs for tile...\n",
      "\t\tLength of all ATL08 for tile: 23\n",
      "\t\tLength of all_atl08_csvs: 10513\n",
      "['ATL08_30m_20190621110851_12860302_003_01_30m.csv', 'ATL08_30m_20190818082057_07830402_003_01_30m.csv', 'ATL08_30m_20190822081238_08440402_003_01_30m.csv']\n",
      "[8766, 9875, 9768]\n",
      "\t\tLength of idx with matches between ATL08 CSVs and ATL08 granules for tile: 3\n",
      "['/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/22/56/24/607120/ATL08_30m_20190621110851_12860302_003_01_30m.csv', '/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/23/12/32/945664/ATL08_30m_20190818082057_07830402_003_01_30m.csv', '/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/23/11/17/197105/ATL08_30m_20190822081238_08440402_003_01_30m.csv']\n",
      "\t# of ATL08 CSV found for tile 30542: 3\n",
      "Creating pandas data frame...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#for in_tile_num in INPUT_TILE_NUM_LIST[100:105]:\n",
    "for in_tile_num in [30542]:#, 30543, 30821, 30822, 30823]:\n",
    "    \n",
    "    # TODO: make this an arg\n",
    "    years_list = [2019, 2020, 2021]\n",
    "    \n",
    "    seg_str = '_100m'\n",
    "    if do_30m:\n",
    "        seg_str = '_30m'\n",
    "    if TEST:\n",
    "        seg_str = ''\n",
    "    \n",
    "    \n",
    "    \n",
    "    if maap_query and dps_dir is not None:\n",
    "        \n",
    "        print(\"\\nDoing MAAP query by tile bounds to find all intersecting ATL08 \")\n",
    "        # Get a list of all ATL08 H5 granule names intersecting the tile (this will be a small list)\n",
    "        # all_atl08_for_tile = ExtractUtils.get_h5_list() #<- when you get import to work, change back to this\n",
    "        all_atl08_for_tile = ExtractUtils.get_h5_list(tile_num=in_tile_num, tile_fn=in_tile_fn, layer=in_tile_layer, DATE_START=date_start, DATE_END=date_end, YEARS=years_list)\n",
    "              \n",
    "        if not os.path.isfile(csv_list_fn):\n",
    "            # Get a list of all ATL08 CSV files from (from extract_atl08) (this will be a large boreal list)\n",
    "            print(\"\\tDPS dir to find ATL08 CSVs: {}\".format(dps_dir))\n",
    "            all_atl08_csvs = glob.glob(dps_dir + \"/**/ATL08*\" + seg_str + \".csv\", recursive=True)\n",
    "            print(len(all_atl08_csvs))\n",
    "            all_atl08_csvs_df = pd.DataFrame({\"path\": all_atl08_csvs})\n",
    "            all_atl08_csvs_df.to_csv(csv_list_fn)\n",
    "        else:\n",
    "            print(\"\\tReading existing list og ATL08 CSVs: {}\".format(csv_list_fn))\n",
    "            all_atl08_csvs_df = pd.read_csv(csv_list_fn)\n",
    "            \n",
    "        # Find the ATL08 CSVs from extract that are associated with the ATL08 granules that intersect this tile\n",
    "        # These CSvs are nested deep after DPS runs\n",
    "        # They should match all the ATL08 granules, but probably wont bc: (1) did DPS for ATL08 30m fully complete with no fails? (2) did DPS for extract fully complete with no fails?\n",
    "        all_atl08_csvs_FOUND = FilterUtils.find_atl08_csv_tile(all_atl08_for_tile, all_atl08_csvs_df, seg_str) \n",
    "        \n",
    "        #all_atl08_csvs_FOUND = [x for x in all_atl08_h5_with_csvs_for_tile if x not in all_atl08_csvs_NOT_FOUND]\n",
    "        print(\"\\t# of ATL08 CSV found for tile {}: {}\".format(in_tile_num, len(all_atl08_csvs_FOUND)))\n",
    "        if len(all_atl08_csvs_FOUND) == 0:\n",
    "            print('\\tNo ATL08 extracted for this tile.')\n",
    "            continue\n",
    "        \n",
    "        # Merge all ATL08 CSV files for the current tile into a pandas df\n",
    "        print(\"Creating pandas data frame...\")\n",
    "        atl08 = pd.concat([pd.read_csv(f) for f in all_atl08_csvs_FOUND ], sort=False, ignore_index=True)\n",
    "        #print(atl08_tmp.info())\n",
    "        print(\"\\nFiltering by tile: {}\".format(in_tile_num))\n",
    "        if False:\n",
    "            # Get tile bounds as xmin,xmax,ymin,ymax\n",
    "            print('DEBUG reorder_4326_bounds')\n",
    "            in_bounds = FilterUtils.reorder_4326_bounds(in_tile_fn, in_tile_num, buffer=0, layer=in_tile_layer)\n",
    "            print(in_bounds)\n",
    "        \n",
    "        if False:\n",
    "            # Now filter ATL08 obs by tile bounds\n",
    "            print('DEBUG filter_atl08_bound')\n",
    "            atl08 = FilterUtils.filter_atl08_bounds(atl08_df=atl08, in_bounds=in_bounds)\n",
    "\n",
    "    elif maap_query and dps_dir is None:\n",
    "        print(\"\\nNo DPS dir specified: cant get ATL08 CSV list to match with tile bound results from MAAP query.\\n\")\n",
    "        os._exit(1)\n",
    "    else:\n",
    "        # Filter by bounds: EPT with a the bounds from an input tile\n",
    "        atl08 = FilterUtils.filter_atl08_bounds_tile_ept(in_ept_fn, in_tile_fn, in_tile_num, in_tile_layer, output_dir, return_pdf=True)\n",
    "    \n",
    "    ## Filter by quality: based on a standard filter_atl08_qual() function that we use across all notebooks, scripts, etc\n",
    "    #atl08_pdf_filt = FilterUtils.filter_atl08_qual(atl08, out_cols_list)\n",
    "    # Filter by quality\n",
    "    print('DEBUG filter_atl08_qual')\n",
    "    atl08_pdf_filt = FilterUtils.filter_atl08_qual(atl08, SUBSET_COLS=True, \n",
    "                                                       subset_cols_list=['rh25','rh50','rh60','rh70','rh75','rh80','rh85','rh90','rh95','h_can','h_max_can'], \n",
    "                                                       filt_cols=['h_can','h_dif_ref','m','msw_flg','beam_type','seg_snow', 'seg_landcov'], \n",
    "                                                       thresh_h_can=100, thresh_h_dif=100, month_min=6, month_max=9)\n",
    "    atl08=None\n",
    "    \n",
    "    # Convert to geopandas data frame in lat/lon\n",
    "    atl08_gdf = GeoDataFrame(atl08_pdf_filt, geometry=geopandas.points_from_xy(atl08_pdf_filt.lon, atl08_pdf_filt.lat), crs='epsg:4326')\n",
    "    out_name_stem = \"atl08_filt\"\n",
    "    atl08_pdf_filt=None\n",
    "    \n",
    "    if extract_covars:\n",
    "        ### Below here should be re-worked to follow final chunk of nb 2.3 (6/15/2021)\n",
    "        #\n",
    "        \n",
    "        # Extract topo covar values to ATL08 obs (doing a reproject to tile crs)\n",
    "        # TODO: consider just running 3.1.5_dpy.py here to produce this topo stack right before extracting its values\n",
    "        topo_covar_fn = do_3_1_5_dp.main(in_tile_fn=in_tile_fn, in_tile_num=in_tile_num, tile_buffer_m=120, in_tile_layer=in_tile_layer, topo_tile_fn='https://maap-ops-dataset.s3.amazonaws.com/maap-users/alexdevseed/dem30m_tiles.geojson')\n",
    "        atl08_gdf_out = ExtractUtils.extract_value_gdf(topo_covar_fn, atl08_gdf, [\"elevation\",\"slope\",\"tsri\",\"tpi\", \"slopemask\"], reproject=True)\n",
    "        out_name_stem = out_name_stem + \"_topo\"\n",
    "\n",
    "        # Extract landsat covar values to ATL08 obs\n",
    "        # TODO: consider just running 3.1.2_dpy.py here\n",
    "        landsat_covar_fn = do_3_1_2_dps.main(in_tile_fn=in_tile_fn, in_tile_num=in_tile_num, in_tile_layer=in_tile_layer, sat_api='https://landsatlook.usgs.gov/sat-api', local=args.local)\n",
    "        atl08_gdf_out = ExtractUtils.extract_value_gdf(landsat_covar_fn, atl08_gdf_out, ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo'], reproject=False)\n",
    "        out_name_stem = out_name_stem + \"_landsat\"\n",
    "        \n",
    "    # CSV the file\n",
    "    cur_time = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    out_csv_fn = os.path.join(output_dir, out_name_stem + \"_\" + cur_time + \".csv\")\n",
    "    atl08_gdf_out.to_csv(out_csv_fn,index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(\"Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile {}: {}\".format(in_tile_num, out_csv_fn) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_atl08_csvs_FOUND = ['/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/22/56/24/607120/ATL08_30m_20190621110851_12860302_003_01_30m.csv', '/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/23/12/32/945664/ATL08_30m_20190818082057_07830402_003_01_30m.csv', '/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/23/11/17/197105/ATL08_30m_20190822081238_08440402_003_01_30m.csv']\n",
    "atl08_tmp = pd.concat([pd.read_csv(f) for f in all_atl08_csvs_FOUND ], sort=False, ignore_index=True)\n",
    "in_bounds = [-117.10749852280769, -116.50936927974429, 50.78795362739066, 51.16389512140189]\n",
    "atl08_tmp = atl08_tmp[(atl08_tmp[\"lat\"] > in_bounds[2]) &\n",
    "                   (atl08_tmp[\"lat\"] < in_bounds[3]) &\n",
    "                   (atl08_tmp[\"lon\"] > in_bounds[0]) &\n",
    "                   (atl08_tmp[\"lon\"] < in_bounds[1])\n",
    "                               ]\n",
    "print(len(atl08_tmp))\n",
    "atl08_tmp.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
