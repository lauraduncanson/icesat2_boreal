{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook version of 2.3_dps.py, which runs over a single tile\n",
    "#### cant get 2.3_dps.py to work, so alternate approach is to loop over tile ids and return indiv tile CSVs of ATL08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.8.1-CAPI-1.13.3). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "#import pdal\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# From CovariateUtils\n",
    "import numpy\n",
    "from rasterio import enums\n",
    "from rasterio.io import MemoryFile\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.vrt import WarpedVRT\n",
    "from rasterio.warp import array_bounds, calculate_default_transform\n",
    "import geopandas\n",
    "\n",
    "import argparse\n",
    "\n",
    "from maap.maap import MAAP\n",
    "maap = MAAP()\n",
    "\n",
    "import sys\n",
    "sys.path.append('/projects/code/icesat2_boreal/notebooks/3.Gridded_product_development')\n",
    "if False:\n",
    "    \n",
    "    sys.path.append('/projects/code/icesat2_boreal/notebooks/2.ICESat-2_processing')\n",
    "\n",
    "    #TODO: how to get this import right if its in a different dir\n",
    "    from CovariateUtils import get_index_tile\n",
    "\n",
    "import ExtractUtils\n",
    "import FilterUtils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3270, 3366, 3367, 3461, 3462]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maap_query = True\n",
    "dps_dir_topo = '/projects/r2d2/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/06/18'\n",
    "dps_dir_csv = '/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17' #'/projects/jabba/dps_output/2.3_output'#\n",
    "output_dir = '/projects/jabba/data/out_tiles'\n",
    "topo_stack_list_fn = '/projects/jabba/data/topo_stack_list.csv'\n",
    "csv_list_fn = '/projects/jabba/data/extract_atl08_csv_list.csv'\n",
    "TEST = False\n",
    "do_30m = True\n",
    "extract_covars = False\n",
    "\n",
    "in_tile_fn = '/projects/maap-users/alexdevseed/boreal_tiles.gpkg' # '/projects/jabba/data/grid_boreal_albers_100k.gpkg'# \n",
    "in_tile_layer = 'boreal_tiles_albers'\n",
    "\n",
    "thresh_h_can = 100\n",
    "thresh_h_dif = 100\n",
    "month_min = 6\n",
    "month_max = 9\n",
    "\n",
    "date_start = '06-01'\n",
    "date_end = '09-30'\n",
    "years_list = [2019, 2020, 2021]\n",
    "# NA tiles\n",
    "# Read the boreal tile index file\n",
    "\n",
    "boreal_tile_index = geopandas.read_file(in_tile_fn)\n",
    "boreal_tile_index_subset = boreal_tile_index.to_crs(4326).cx[-170:-50, 50:75]\n",
    "\n",
    "# Boreal NA tiles: need just a list of tile_ids\n",
    "INPUT_TILE_NUM_LIST = boreal_tile_index_subset['layer'].astype(int).tolist()\n",
    "\n",
    "INPUT_TILE_NUM_LIST[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the list of extracted CSV paths from DPS and save all paths in a list - glob.glob takes a while - dont do this for each tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This is sloooow\n",
    "def get_atl08_csv_list(dps_dir_csv, seg_str, csv_list_fn):\n",
    "    print(dps_dir_csv + \"/**/ATL08*\" + seg_str + \".csv\") \n",
    "    seg_str=\"_30m\"\n",
    "    all_atl08_csvs = glob.glob(dps_dir_csv + \"/**/ATL08*\" + seg_str + \".csv\", recursive=True)\n",
    "    print(len(all_atl08_csvs))\n",
    "    all_atl08_csvs_df = pd.DataFrame({\"path\": all_atl08_csvs})\n",
    "    all_atl08_csvs_df.to_csv(csv_list_fn)\n",
    "    return(all_atl08_csvs_df)\n",
    "    \n",
    "def get_topo_stack_list(dps_dir_topo, topo_stack_list_fn):\n",
    "    all_topo_stacks = glob.glob(dps_dir_topo + \"/**/Copernicus_*_covars_cog_topo_stack.tif\", recursive=True)\n",
    "    print(len(all_topo_stacks))\n",
    "    all_topo_stacks_df = pd.DataFrame({\"path\": all_topo_stacks})\n",
    "    all_topo_stacks_df.to_csv(topo_stack_list_fn)\n",
    "    return(all_topo_stacks_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topo_stack_fn(topo_stack_list_fn, in_tile_num):\n",
    "    # Find most recent topo stack path for tile in list of topo stack paths \n",
    "    all_topo_stacks_df = pd.read_csv(topo_stack_list_fn)\n",
    "    stack_for_tile = all_topo_stacks_df[all_topo_stacks_df['path'].str.contains(\"Copernicus_\"+str(in_tile_num))]\n",
    "    #[print(i) for i in stack_for_tile.path.to_list()]\n",
    "    topo_stack_fn = stack_for_tile.path.to_list()[0]\n",
    "    return(topo_stack_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the CSVs produced from extract_atl08 for a given tile id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTILE_NUM: 40620 (-166.348188906566,52.76554520921289,-165.81660486625967,53.08282578323968)\n",
      "\t\t# ATL08 for tile 40620: 18\n",
      "ATL08_20190615030715_11890306_003_01.h5\n",
      "ATL08_20190619025855_12500306_003_01.h5\n",
      "ATL08_20190718013448_03050406_003_01.h5\n",
      "ATL08_20190728123646_04650402_003_01.h5\n",
      "ATL08_20190801122828_05260402_003_01.h5\n",
      "ATL08_20190816001100_07470406_003_01.h5\n",
      "ATL08_20190820000242_08080406_003_01.h5\n",
      "ATL08_20190830110434_09680402_003_01.h5\n",
      "ATL08_20190913224705_11890406_003_01.h5\n",
      "ATL08_20190917223846_12500406_003_01.h5\n",
      "ATL08_20200626203959_00230802_003_01.h5\n",
      "ATL08_20200725191601_04650802_003_01.h5\n",
      "ATL08_20200729190743_05260802_003_01.h5\n",
      "ATL08_20200813065012_07470806_003_01.h5\n",
      "ATL08_20200827174344_09680802_003_01.h5\n",
      "ATL08_20200911052615_11890806_003_01.h5\n",
      "ATL08_20200915051755_12500806_003_01.h5\n",
      "ATL08_20200925161948_00230902_003_01.h5\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/22/50/42/967379/ATL08_30m_20190619025855_12500306_003_01_30m.csv\n",
      "/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/22/57/01/034821/ATL08_30m_20190801122828_05260402_003_01_30m.csv\n",
      "/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/23/04/53/838273/ATL08_30m_20190816001100_07470406_003_01_30m.csv\n",
      "/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/23/06/52/977169/ATL08_30m_20190820000242_08080406_003_01_30m.csv\n"
     ]
    }
   ],
   "source": [
    "all_atl08_for_tile = ExtractUtils.maap_search_get_h5_list(tile_num=40620, tile_fn=in_tile_fn, layer=in_tile_layer, DATE_START=date_start, DATE_END=date_end, YEARS=years_list)\n",
    "all_atl08_csvs_df = pd.read_csv(csv_list_fn)\n",
    "\n",
    "print([print(i) for i in all_atl08_for_tile])\n",
    "\n",
    "for g in all_atl08_for_tile:\n",
    "    s = g.split(\"_\")[1]\n",
    "    f = all_atl08_csvs_df[all_atl08_csvs_df['path'].str.contains(s)]\n",
    "    if len(f) > 0:\n",
    "        print(f.path.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over input tiles to output ATL08 tile CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing MAAP query by tile bounds to find all intersecting ATL08 \n",
      "\tTILE_NUM: 30542 (-117.10749852280769,50.78795362739066,-116.50936927974429,51.16389512140189)\n",
      "\t\t# ATL08 for tile 30542: 23\n",
      "\tReading existing list og ATL08 CSVs: /projects/jabba/data/extract_atl08_csv_list.csv\n",
      "\tFind ATL08 CSVs for tile...\n",
      "\t\tLength of all ATL08 for tile: 23\n",
      "\t\tLength of all_atl08_csvs: 10513\n",
      "\t\tLength of idx with matches between ATL08 CSVs and ATL08 granules for tile: 3\n",
      "\t# of ATL08 CSV found for tile 30542: 3\n",
      "Creating pandas data frame...\n",
      "\n",
      "Pre-filter data cleaning...\n",
      "\tGet beam type from orbit orientation and ground track: ['Strong' 'Weak']\n",
      "\tCast some columns to:\n",
      "\t\ttype float: ['lat', 'lon', 'h_can', 'h_te_best', 'ter_slp']\n",
      "\t\ttype integer: ['n_ca_ph', 'n_seg_ph', 'n_toc_ph']\n",
      "\n",
      "Filtering by tile: 30542\n",
      "[-117.10749852280769, -116.50936927974429, 50.78795362739066, 51.16389512140189]\n",
      "\n",
      "Filtering by bounds: [-117.10749852280769, -116.50936927974429, 50.78795362739066, 51.16389512140189]\n",
      "\tBefore bounds filtering: \t\t67753 observations in the input dataframe.\n",
      "\tMin lon: -118.63223357298055\n",
      "\tMax lon: -115.45402799313496\n",
      "\tData frame where:\n",
      "\t\tlat>50.78795362739066\n",
      "\t\tlat<51.16389512140189\n",
      "\t\tlon>-117.10749852280769\n",
      "\t\tlon<-116.50936927974429\n",
      "After bounds filtering: \t\t168 observations in the input dataframe.\n",
      "\n",
      "Filtering by quality\n",
      "\tBefore quality filtering: \t\t168 observations in the input dataframe.\n",
      "\tAfter msw_flg=0: \t\t6 observations in the dataframe.\n",
      "\tAfter beam_type=Strong: \t\t4 observations in the dataframe.\n",
      "\tAfter seg_snow=1: \t\t4 observations in the dataframe.\n",
      "\tAfter all quality filtering: \t\t4 observations in the output dataframe.\n",
      "\tReturning a pandas data frame of filtered observations for columns: ['lon', 'lat', 'rh25', 'rh50', 'rh60', 'rh70', 'rh75', 'rh80', 'rh90', 'h_can', 'h_max_can']\n",
      "\tData frame shape: (4, 11) \n",
      "Wrote output csv/geojson of filtered ATL08 obs with topo and Landsat covariates for tile 30542: /projects/jabba/data/out_tiles/atl08_filt_20210625_30542\n",
      "CPU times: user 4.85 s, sys: 249 ms, total: 5.09 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "INPUT_TILE_NUM_LIST = range(30823,30824) #, 30543, 30821, 30822, 30823]:\n",
    "INPUT_TILE_NUM_LIST = [30542]#[30543]\n",
    "for in_tile_num in INPUT_TILE_NUM_LIST:\n",
    "    \n",
    "    # TODO: make this an arg\n",
    "    years_list = [2019, 2020, 2021]\n",
    "    \n",
    "    seg_str = '_100m'\n",
    "    if do_30m:\n",
    "        seg_str = '_30m'\n",
    "    if TEST:\n",
    "        seg_str = '' \n",
    "    \n",
    "    if maap_query and dps_dir_csv is not None:\n",
    "        \n",
    "        print(\"\\nDoing MAAP query by tile bounds to find all intersecting ATL08 \")\n",
    "        # Get a list of all ATL08 H5 granule names intersecting the tile (this will be a small list)\n",
    "        all_atl08_for_tile = ExtractUtils.maap_search_get_h5_list(tile_num=in_tile_num, tile_fn=in_tile_fn, layer=in_tile_layer, DATE_START=date_start, DATE_END=date_end, YEARS=years_list)\n",
    "        #print([os.path.basename(f) for f in all_atl08_for_tile])      \n",
    "        \n",
    "        if not os.path.isfile(csv_list_fn):\n",
    "            all_atl08_csvs_df = get_atl08_csv_list(dps_dir_csv, seg_str, csv_list_fn)\n",
    "        else:\n",
    "            print(f\"\\tReading existing list og ATL08 CSVs: {csv_list_fn}\")\n",
    "            all_atl08_csvs_df = pd.read_csv(csv_list_fn)\n",
    "            \n",
    "        # Find the ATL08 CSVs from extract that are associated with the ATL08 granules that intersect this tile\n",
    "        # These CSvs are nested deep after DPS runs\n",
    "        # They should match all the ATL08 granules, but probably wont bc: (1) did DPS for ATL08 30m fully complete with no fails? (2) did DPS for extract fully complete with no fails?\n",
    "        all_atl08_csvs_FOUND = FilterUtils.find_atl08_csv_tile(all_atl08_for_tile, all_atl08_csvs_df, seg_str) \n",
    "        \n",
    "        #all_atl08_csvs_FOUND = [x for x in all_atl08_h5_with_csvs_for_tile if x not in all_atl08_csvs_NOT_FOUND]\n",
    "        print(\"\\t# of ATL08 CSV found for tile {}: {}\".format(in_tile_num, len(all_atl08_csvs_FOUND)))\n",
    "        if len(all_atl08_csvs_FOUND) == 0:\n",
    "            print('\\tNo ATL08 extracted for this tile.')\n",
    "            continue\n",
    "        \n",
    "        # Merge all ATL08 CSV files for the current tile into a pandas df\n",
    "        print(\"Creating pandas data frame...\")\n",
    "        atl08 = pd.concat([pd.read_csv(f) for f in all_atl08_csvs_FOUND ], sort=False, ignore_index=True)\n",
    "        atl08 = FilterUtils.prep_filter_atl08_qual(atl08)\n",
    "\n",
    "        print(\"\\nFiltering by tile: {}\".format(in_tile_num))\n",
    "        # Get tile bounds as xmin,xmax,ymin,ymax\n",
    "        in_bounds = FilterUtils.reorder_4326_bounds(in_tile_fn, in_tile_num, buffer=0, layer=in_tile_layer)\n",
    "        print(in_bounds)\n",
    "        \n",
    "        # Now filter ATL08 obs by tile bounds\n",
    "        atl08 = FilterUtils.filter_atl08_bounds(atl08_df=atl08, in_bounds=in_bounds)\n",
    "\n",
    "    elif maap_query and dps_dir_csv is None:\n",
    "        print(\"\\nNo DPS dir specified: cant get ATL08 CSV list to match with tile bound results from MAAP query.\\n\")\n",
    "        #os._exit(1)\n",
    "        continue\n",
    "    else:\n",
    "        # Filter by bounds: EPT with a the bounds from an input tile\n",
    "        atl08 = FilterUtils.filter_atl08_bounds_tile_ept(in_ept_fn, in_tile_fn, in_tile_num, in_tile_layer, output_dir, return_pdf=True)\n",
    "    \n",
    "    # Filter by quality\n",
    "    atl08_pdf_filt = FilterUtils.filter_atl08_qual(atl08, SUBSET_COLS=True, DO_PREP=False,\n",
    "                                                       subset_cols_list=['rh25','rh50','rh60','rh70','rh75','rh80','rh90','h_can','h_max_can'], \n",
    "                                                       filt_cols=['h_can','h_dif_ref','m','msw_flg','beam_type','seg_snow', 'seg_landcov'], \n",
    "                                                       thresh_h_can=100, thresh_h_dif=100, month_min=6, month_max=9)\n",
    "    atl08=None\n",
    "    \n",
    "    # Convert to geopandas data frame in lat/lon\n",
    "    atl08_gdf = geopandas.GeoDataFrame(atl08_pdf_filt, geometry=geopandas.points_from_xy(atl08_pdf_filt.lon, atl08_pdf_filt.lat), crs='epsg:4326')\n",
    "    out_name_stem = \"atl08_filt\"\n",
    "    atl08_pdf_filt=None\n",
    "    \n",
    "    if extract_covars:\n",
    "        ### Below here should be re-worked to follow final chunk of nb 2.3 (6/15/2021)\n",
    "        #\n",
    "        if False:\n",
    "            # Extract topo covar values to ATL08 obs (doing a reproject to tile crs)\n",
    "            # TODO: consider just running 3.1.5_dpy.py here to produce this topo stack right before extracting its values\n",
    "            topo_covar_fn = do_3_1_5_dp.main(in_tile_fn=in_tile_fn, in_tile_num=in_tile_num, tile_buffer_m=120, in_tile_layer=in_tile_layer, topo_tile_fn='https://maap-ops-dataset.s3.amazonaws.com/maap-users/alexdevseed/dem30m_tiles.geojson')\n",
    "        else:\n",
    "            topo_covar_fn = get_topo_stack_fn(topo_stack_list_fn, in_tile_num)\n",
    "            \n",
    "        atl08_gdf = ExtractUtils.extract_value_gdf(topo_covar_fn, atl08_gdf, [\"elevation\",\"slope\",\"tsri\",\"tpi\", \"slopemask\"], reproject=True)\n",
    "        out_name_stem = out_name_stem + \"_topo\"\n",
    "\n",
    "        if False:\n",
    "            # Extract landsat covar values to ATL08 obs\n",
    "            # TODO: consider just running 3.1.2_dpy.py here\n",
    "            landsat_covar_fn = do_3_1_2_dps.main(in_tile_fn=in_tile_fn, in_tile_num=in_tile_num, in_tile_layer=in_tile_layer, sat_api='https://landsatlook.usgs.gov/sat-api', local=args.local)\n",
    "            atl08_gdf = ExtractUtils.extract_value_gdf(landsat_covar_fn, atl08_gdf, ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo'], reproject=False)\n",
    "            out_name_stem = out_name_stem + \"_landsat\"\n",
    "        \n",
    "    if len(atl08_gdf) == 0:\n",
    "        print(f\"No ATL08 obs. for tile {in_tile_num}\")\n",
    "    else:\n",
    "        # CSV the file\n",
    "        cur_time = time.strftime(\"%Y%m%d\") #\"%Y%m%d%H%M%S\"\n",
    "        out_fn = os.path.join(output_dir, out_name_stem + \"_\" + cur_time + \"_\" + str(in_tile_num) )\n",
    "        atl08_gdf.to_csv(out_fn+\".csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        atl08_gdf.to_file(out_fn+'.geojson', driver=\"GeoJSON\")\n",
    "\n",
    "        print(\"Wrote output csv/geojson of filtered ATL08 obs with topo and Landsat covariates for tile {}: {}\".format(in_tile_num, out_fn) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 390 entries, 0 to 389\n",
      "Data columns (total 37 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   lon        390 non-null    float64\n",
      " 1   lat        390 non-null    float64\n",
      " 2   rh25       390 non-null    float64\n",
      " 3   rh50       390 non-null    float64\n",
      " 4   rh60       390 non-null    float64\n",
      " 5   rh70       390 non-null    float64\n",
      " 6   rh75       390 non-null    float64\n",
      " 7   rh80       390 non-null    float64\n",
      " 8   rh85       390 non-null    float64\n",
      " 9   rh90       390 non-null    float64\n",
      " 10  rh95       390 non-null    float64\n",
      " 11  h_can      390 non-null    float64\n",
      " 12  h_max_can  390 non-null    float64\n",
      " 13  geometry   390 non-null    object \n",
      " 14  elevation  390 non-null    float64\n",
      " 15  slope      390 non-null    float64\n",
      " 16  tsri       390 non-null    float64\n",
      " 17  tpi        390 non-null    float64\n",
      " 18  slopemask  390 non-null    float64\n",
      " 19  Blue       390 non-null    float64\n",
      " 20  Green      390 non-null    float64\n",
      " 21  Red        390 non-null    float64\n",
      " 22  NIR        390 non-null    float64\n",
      " 23  SWIR       390 non-null    float64\n",
      " 24  NDVI       390 non-null    float64\n",
      " 25  SAVI       390 non-null    float64\n",
      " 26  MSAVI      390 non-null    float64\n",
      " 27  NDMI       390 non-null    float64\n",
      " 28  EVI        390 non-null    float64\n",
      " 29  NBR        390 non-null    float64\n",
      " 30  NBR2       390 non-null    float64\n",
      " 31  TCB        390 non-null    float64\n",
      " 32  TCG        390 non-null    float64\n",
      " 33  TCW        390 non-null    float64\n",
      " 34  ValidMask  390 non-null    float64\n",
      " 35  Xgeo       390 non-null    float64\n",
      " 36  Ygeo       390 non-null    float64\n",
      "dtypes: float64(36), object(1)\n",
      "memory usage: 112.9+ KB\n",
      "[-116.85483935749697, 50.57021397455074, -116.26013039328576, 50.94612905730388]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-abd81d599bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#folium.GeoJson(boreal_tile_index_orig, name='tile').add_to(m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mfolium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeoJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matl08_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ATL08\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeoJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matl08_filt_test_gdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ATL08\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m folium.GeoJson(shp.geometry.box(*bbox_list), \n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'color'"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import shapely as shp\n",
    "\n",
    "boreal_tile_index_orig = geopandas.read_file('/projects/maap-users/alexdevseed/boreal_tiles.gpkg')\n",
    "\n",
    "tile_parts = ExtractUtils.get_index_tile(in_tile_fn, in_tile_num, buffer=0, layer=in_tile_layer)\n",
    "\n",
    "atl08_filt_test = pd.read_csv('/projects/jabba/data/atl08_filt_30543_topo_landsat_20210608185220.csv')\n",
    "atl08_filt_test.info()\n",
    "atl08_filt_test_gdf = geopandas.GeoDataFrame(atl08_filt_test, crs='epsg:4326', geometry=geopandas.points_from_xy(atl08_filt_test.lon, atl08_filt_test.lat))\n",
    "\n",
    "# Reformat bbox\n",
    "bbox_list = tile_parts[\"bbox_4326\"]\n",
    "print(bbox_list)\n",
    "\n",
    "m = folium.Map(\n",
    "    # Zoom to center\n",
    "    #location=[center.y,center.x],\n",
    "    # Zoom to corner\n",
    "    location=[bbox_list[1], bbox_list[2]],\n",
    "    tiles=\"cartodbpositron\",\n",
    "    zoom_start=11,\n",
    ")\n",
    "\n",
    "bbox_style = {'fillColor': '#ff0000', 'color': '#ff0000'}\n",
    "bbox_buf_style = {'fillColor': '#fdae61', 'color': '#fdae61'}\n",
    "\n",
    "#folium.GeoJson(boreal_tile_index_orig, name='tile').add_to(m)\n",
    "folium.GeoJson(atl08_gdf.sample(frac=0.1), name=\"ATL08\", color=\"red\").add_to(m)\n",
    "folium.GeoJson(atl08_filt_test_gdf, name=\"ATL08\", color=\"blue\").add_to(m)\n",
    "folium.GeoJson(shp.geometry.box(*bbox_list), \n",
    "               name=\"bbox\",\n",
    "               style_function=lambda x:bbox_style).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import pandas as pd\n",
    "    import FilterUtils\n",
    "\n",
    "    all_atl08_csvs_FOUND = ['/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/22/56/24/607120/ATL08_30m_20190621110851_12860302_003_01_30m.csv', \n",
    "                            '/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/23/12/32/945664/ATL08_30m_20190818082057_07830402_003_01_30m.csv', \n",
    "                            '/projects/r2d2/dps_output/run_extract_atl08_orig_ubuntu/master/2021/06/17/23/11/17/197105/ATL08_30m_20190822081238_08440402_003_01_30m.csv']\n",
    "    atl08_tmp = pd.concat([pd.read_csv(f) for f in all_atl08_csvs_FOUND ], sort=False, ignore_index=True)\n",
    "    in_bounds = [-117.10749852280769, -116.50936927974429, 50.78795362739066, 51.16389512140189]\n",
    "    atl08_tmp = atl08_tmp[(atl08_tmp.lat > in_bounds[2]) &\n",
    "                           (atl08_tmp.lat < in_bounds[3]) &\n",
    "                           (atl08_tmp.lon > in_bounds[0]) &\n",
    "                           (atl08_tmp.lon < in_bounds[1])\n",
    "                                   ]\n",
    "    \n",
    "    print('DEBUG FilterUtils.prep_filter_atl08_qual')\n",
    "    atl08_tmp = FilterUtils.prep_filter_atl08_qual(atl08_tmp)\n",
    "    print(len(atl08_tmp))\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
