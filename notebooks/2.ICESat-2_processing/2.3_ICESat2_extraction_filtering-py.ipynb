{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICESat-2 Filtering and Extractions\n",
    "This notebook searches for a spatial subset of data, runs DPS jobs to extract_atl08.py (by Nathan Thomas & Paul Montesano), processing and visualizing the outputs. Set up a notebook run in the first cell, especially whether or not DPS jobs should be submitted.\n",
    "\n",
    "Outline:\n",
    "1. Get BBOX of interest\n",
    "2. Query CMR for ATL08 Granules\n",
    "3. Convert ATL08 Granules to las format only keeping relevant vars/dimensions (h5py/pdal)\n",
    "4. Convert las files to EPT data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning:\n",
      "\n",
      "The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.8.1-CAPI-1.13.3). Conversions between both will be slow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3 ICESat-2 extraction, merging, filtering, exploring, mapping\n",
    "from maap.maap import MAAP\n",
    "maap = MAAP()\n",
    "\n",
    "import ipycmc\n",
    "w = ipycmc.MapCMC()\n",
    "\n",
    "if False:\n",
    "    import importlib\n",
    "    lib_loader = importlib.find_loader('cartopy')\n",
    "\n",
    "    if lib_loader is not None:\n",
    "        REBUILD_CONDA_ENV = False\n",
    "        print(\"No need to re-build conda env.\")\n",
    "    else:\n",
    "        REBUILD_CONDA_ENV = True\n",
    "        print(\"Re-build conda env...\")\n",
    "\n",
    "    if REBUILD_CONDA_ENV:\n",
    "        #### This notebook uses a DPS job to run extract_atl08.py to convert h5's to csv's, then appends all csv's into a pandas geodataframe.\n",
    "        #### Returns: a pandas geodataframe that should hold the entire set of ATL08 data for this project\n",
    "        #### Notes:\n",
    "        ###### ISSUE: how to relaibly activate a conda env that can support this notebook.\n",
    "        ###### Need to 'conda activate' an env that has geopandas - but where do I do this 'activate'. How does terminal env interact with nb?\n",
    "        ###### Workaround: always do this to base:\n",
    "        ! conda install -c conda-forge geopandas -y\n",
    "        #! conda install -c conda-forge cartopy -y\n",
    "        ! conda install -c conda-forge descartes -y\n",
    "        ! conda install -c conda-forge seaborn -y\n",
    "        ! conda install contextily --channel conda-forge -y\n",
    "        #! conda install -c conda-forge matplotlib_scalebar -y\n",
    "        ##https://www.essoar.org/doi/10.1002/essoar.10501423.1\n",
    "        ##https://www.essoar.org/pdfjs/10.1002/essoar.10501423.1\n",
    "        ##https://github.com/icesat2py/icepyx/blob/master/examples/ICESat-2_DEM_comparison_Colombia_working.ipynb\n",
    "        ##https://github.com/ICESAT-2HackWeek/2020_ICESat-2_Hackweek_Tutorials\n",
    "        ##https://icesat-2hackweek.github.io/learning-resources/logistics/schedule/\n",
    "        ##https://github.com/giswqs/earthengine-py-notebooks\n",
    "\n",
    "import geopandas as gpd\n",
    "#import descartes\n",
    "import numpy as np\n",
    "#import seaborn as sb\n",
    "from geopandas import GeoDataFrame\n",
    "from geopandas.tools import sjoin\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import random \n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import cartopy.crs as ccrs\n",
    "#from cartopy.feature import NaturalEarthFeature, LAND, COASTLINE\n",
    "#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import datetime\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "#from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import contextily as ctx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/projects/Developer/icesat2_boreal/notebooks/3.Gridded_product_development')\n",
    "from CovariateUtils import *\n",
    "\n",
    "import FilterUtils\n",
    "import ExtractUtils\n",
    "\n",
    "boreal_tile_index_path = '/projects/shared-buckets/nathanmthomas/boreal_grid_albers90k_gpkg.gpkg' #'/projects/maap-users/alexdevseed/boreal_tiles.gpkg'\n",
    "\n",
    "# Run extract_at08.py as a DPS job (see nb 1.3 for template of how this can be done)\n",
    "DPS_OUTPUT_DIR = '/projects/r2d2/dps_outputs/extract_atl08_dps_orig/master/2021/06'\n",
    "#DPS_OUTPUT_DIR = '/projects/jabba/dps_output/2.3_output' #'/projects/above/processed_data/2.3_output'\n",
    "\n",
    "RUN_DPS = False\n",
    "\n",
    "EPT_APPROACH = False\n",
    "\n",
    "H_DIFF_THRESH = 100\n",
    "H_CAN_THRESH = 100\n",
    "\n",
    "READ_PICKLE = False\n",
    "DIR_PICKLE = '/projects/jabba/data'#'/projects/above'\n",
    "\n",
    "DO_ATL08_CSV_SUBSET = False # <- set to True for testing\n",
    "SUBSET_FRAC_SIZE = 0.50\n",
    "\n",
    "#COPY_CSVS = False\n",
    "CSV_TO_DIR = '/projects/jabba/data'#/projects/r2d2/above/atl08_csvs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAAP.searchGranule, make h5 and CSV list: specify the tiles (bbox) and years, get a list of granules, and get a list of ATL08 CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the INPUT_TILE_NUM_LIST here (testing, or full runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of ATL08 files you will want to DPS over\n",
    "\n",
    "# Test tiles\n",
    "#INPUT_TILE_NUM_LIST = [30542, 30543, 30821, 30822, 30823]\n",
    "\n",
    "# NA tiles\n",
    "# Read the boreal tile index file\n",
    "boreal_tile_index_path = '/projects/my-public-bucket/boreal_tiles.gpkg'#'/projects/my-public-bucket/boreal_grid_albers90k_gpkg.gpkg'\n",
    "boreal_tile_layer_name = 'boreal_tiles'#'grid_boreal_albers90k_gpkg'\n",
    "boreal_tile_index = gpd.read_file(boreal_tile_index_path)\n",
    "boreal_tile_index_subset = boreal_tile_index.to_crs(4326).cx[-170:-50, 50:75]\n",
    "\n",
    "# Boreal NA tiles: need just a list of tile_ids\n",
    "INPUT_TILE_NUM_LIST = boreal_tile_index_subset['layer'].astype(int).tolist()\n",
    "\n",
    "INPUT_TILE_NUM_LIST=INPUT_TILE_NUM_LIST[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop over the INPUT_TILE_NUM_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year:  2018\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Null layer: 'boreal_tiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b5d0fa548830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# This is fine for a single tile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtile_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_index_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboreal_tile_index_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTILE_NUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboreal_tile_layer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mBBOX_TILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtile_parts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bbox_4326'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/icesat2_boreal/notebooks/3.Gridded_product_development/CovariateUtils.py\u001b[0m in \u001b[0;36mget_index_tile\u001b[0;34m(vector_path, tile_id, buffer, layer)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mtile_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;31m# In this case tile_id is the row, and since row numbering starts at 0 but tiles at 1, subtract 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# TODO: attribute match the value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--> 257\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Null layer: 'boreal_tiles'"
     ]
    }
   ],
   "source": [
    "INPUT_YEARS_LIST = ['2018','2019','2020','2021']\n",
    "\n",
    "all_atl08 = []\n",
    "\n",
    "for YEAR in INPUT_YEARS_LIST:\n",
    "    \n",
    "    print(\"Year: \", YEAR)\n",
    "    all_atl08_year = []\n",
    "    \n",
    "    for TILE_NUM in INPUT_TILE_NUM_LIST:\n",
    "        \n",
    "        # This is fine for a single tile\n",
    "        tile_parts = get_index_tile(boreal_tile_index_path, TILE_NUM, buffer=0, layer = boreal_tile_layer_name)\n",
    "\n",
    "        BBOX_TILE = ','.join(str(x) for x in tile_parts['bbox_4326'])\n",
    "\n",
    "        # Other BBOXs\n",
    "        BBOX_NA = \"-180,50,-50,75\"\n",
    "        BBOX_CIRC = \"-180,40,180,75\" # You'll need to edit run_above.sh to adjust the geo filtering called for with extract_atl08.py\n",
    "\n",
    "        BBOX = BBOX_TILE\n",
    "        print(\"\\tTILE_NUM: {} ({})\".format(TILE_NUM, BBOX) )\n",
    "\n",
    "        COLLECTID_ATL08_V3 = \"C1200235747-NASA_MAAP\"\n",
    "\n",
    "        # Note: we want to be able to do a 'recurring' seasonal search, regardless of year\n",
    "        DATERANGE_SUMMER = YEAR+'-06-01T00:00:00Z,'+YEAR+'-09-30T23:59:59Z'\n",
    "\n",
    "        # We dont really want a limit: Not really sure how to set this; just use very high number?\n",
    "        MAX_ATL08_ORBITS = 100000\n",
    "\n",
    "        granules = maap.searchGranule(collection_concept_id=COLLECTID_ATL08_V3, \n",
    "                                      temporal=DATERANGE_SUMMER, \n",
    "                                      bounding_box=BBOX, \n",
    "                                      limit=MAX_ATL08_ORBITS)\n",
    "\n",
    "        # This is a list of the granule URLs for processing\n",
    "        granules_list_ATL08 = FilterUtils.get_granules_list(granules)\n",
    "               \n",
    "        print(\"\\t\\t# ATL08 in BBOX: {}\".format(len(granules_list_ATL08)) )\n",
    "        all_atl08_year += granules_list_ATL08\n",
    "    print(\"\\t# ATL08 in {}: {}\".format(YEAR, len(all_atl08_year)) )\n",
    "    all_atl08 += all_atl08_year\n",
    "    \n",
    "print('# ATL08 h5s total: {}'.format(len(all_atl08)) )\n",
    "\n",
    "# Change h5s to CSVs\n",
    "all_atl08_csvs = [os.path.join(DPS_OUTPUT_DIR, os.path.basename(f).replace('.h5','_30m.csv')) for f in all_atl08]\n",
    "\n",
    "all_atl08_csvs_NOT_FOUND = []\n",
    "for file in all_atl08_csvs: \n",
    "    if not os.path.isfile(file):\n",
    "        all_atl08_csvs_NOT_FOUND.append(file)\n",
    "             \n",
    "all_atl08_csvs_FOUND = [x for x in all_atl08_csvs if x not in all_atl08_csvs_NOT_FOUND]\n",
    "print('# ATL08 CSVs found: {}'.format(len(all_atl08_csvs_FOUND)) ) \n",
    "#print('\\n'.join(all_atl08_csvs_FOUND) )\n",
    "print('# ATL08 CSVs not found in {}: {}'.format(DPS_OUTPUT_DIR, len(all_atl08_csvs_NOT_FOUND)) )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a single DPS job to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DPS:\n",
    "    ##################################\n",
    "    #Test DPS submission on a single file\n",
    "    granule=granules_list_ATL08[0]\n",
    "\n",
    "    submit_result = maap.submitJob(identifier=\"nothing\", algo_id=\"run_above_ubuntu\", \n",
    "                                       version=\"master\", \n",
    "                                       username=\"r2d2\", \n",
    "                                       icesat2_granule=granule)\n",
    "    print(submit_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DPS in Batch Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running DPS; probably because output from extract_atl08 DPS job already exists.\n",
      "/projects/above/processed_data/2.3_output\n"
     ]
    }
   ],
   "source": [
    "# Extraction\n",
    "#\n",
    "# DPS SUBMISSION\n",
    "if RUN_DPS:\n",
    "    # Here is where I submit a job \n",
    "    # identified with 'algo_id' (in yaml file)\n",
    "    # that specifies a bash script /projects/above/gitlab_repos/atl08_extract_repo/run_above.sh \n",
    "    # that will call the 'algorithm' (extract_atl08.py)\n",
    "\n",
    "    # Uses granule list from nb 2.1\n",
    "    # CHANGE the submitJob args!\n",
    "    for g in range(len(granules_list_ATL08)):\n",
    "        granule = granules_list_ATL08[g]\n",
    "        submit_result = maap.submitJob(identifier=\"nothing\", algo_id=\"run_above_ubuntu\", \n",
    "                                   version=\"master\", \n",
    "                                   username=\"r2d2\", \n",
    "                                   icesat2_granule=granule)\n",
    "        if g == 1:\n",
    "            print(submit_result)\n",
    "        if g == 100:\n",
    "            print (submit_result)\n",
    "        if g == 1000:\n",
    "            print (submit_result)\n",
    "        if g == 2000:\n",
    "            print (submit_result)\n",
    "        if g == 3000:\n",
    "            print (submit_result)\n",
    "        if g == 4000:\n",
    "            print (submit_result)\n",
    "        if g == len(granules_list_ATL08):\n",
    "            print (submit_result)\n",
    "            print ('done!')\n",
    "        \n",
    "else:\n",
    "    print(\"Not running DPS; probably because output from extract_atl08 DPS job already exists.\")\n",
    "    print(DPS_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge DPS outputs into data frame for visualizatioon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of ATL08 files found after DPS to extract atl08 to CSV:  76\n",
      "Creating pandas data frame...\n",
      "CPU times: user 19.3 s, sys: 1.13 s, total: 20.5 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not READ_PICKLE:\n",
    "    \n",
    "    # Merging\n",
    "\n",
    "    # List of CSVs made in MAAP.searchGranule chunk above\n",
    "    print(\"# of ATL08 files found after DPS to extract atl08 to CSV: \",len(all_atl08_csvs_FOUND))\n",
    "    \n",
    "    if False:\n",
    "        # Find and delete any CSV that has a size of 0\n",
    "        #! find $DPS_OUTPUT_DIR -name \"*.csv\" -size 0 -delete\n",
    "        print(\"Making list of ATL08 csv files...\")\n",
    "        # Find all remaining output CSVs from DPS jobs\n",
    "        all_atl08_csvs = glob.glob(DPS_OUTPUT_DIR+\"/ATL08*.csv\", recursive=True)\n",
    "\n",
    "        # This could break if you randomly grab an incomplete or empty CSV\n",
    "        if DO_ATL08_CSV_SUBSET:\n",
    "            all_atl08_csvs_FOUND = random.sample(all_atl08_csvs_FOUND, math.floor(SUBSET_FRAC_SIZE * len(all_atl08_csvs_FOUND)))\n",
    "            print(\"# of ATL08 files after test sample: \",len(all_atl08_csvs_FOUND))\n",
    "\n",
    "    # Merge all files in the list\n",
    "    print(\"Creating pandas data frame...\")\n",
    "    atl08 = pd.concat([pd.read_csv(f) for f in all_atl08_csvs_FOUND ], sort=False)\n",
    "    \n",
    "    # Probably not necessary\n",
    "    #print('finished pickle') #<--no; there isnt any pickling here; its written aftern the Filtering chunk\n",
    "    #atl08.to_csv( \"/projects/above/processed_data/atl08_merged.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in old data frame from a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False:\n",
    "    if not READ_PICKLE and not EPT_APPROACH:\n",
    "\n",
    "        if False:\n",
    "            # Filtering    <------ THIS IS UPDATED USING THE METHOD IN FilterUtils.py (~5-26-2021)\n",
    "            atl08 =  atl08[\n",
    "                           (atl08.msw_flg == 0) & \n",
    "                           (atl08.beam_type == 'Strong') & \n",
    "                           (atl08.seg_snow == 'snow free land')\n",
    "                            ]\n",
    "            print(f\"After filtering, there are {atl08.shape[0]} observations in this dataframe.\")\n",
    "\n",
    "        # Pickle the file\n",
    "        cur_time = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        samp_frac_str = \"samp-all\"\n",
    "        if DO_ATL08_CSV_SUBSET:\n",
    "            samp_frac_str = \"samp-\" + '{:1.2f}'.format(SUBSET_FRAC_SIZE).replace('.','p')\n",
    "        atl08.to_pickle(os.path.join(DIR_PICKLE, \"atl08_\"+samp_frac_str+\"_\"+cur_time+\".pkl\"))\n",
    "    else:\n",
    "        print(\"Getting the latest merged, filtered, & compressed file of ATL08 obs as a pandas dataframe...\")\n",
    "        list_of_pickles = glob.glob(DIR_PICKLE+'/atl08*.pkl') # * means all if need specific format then *.csv\n",
    "        latest_pickle_file = max(list_of_pickles, key=os.path.getctime)\n",
    "        print(latest_pickle_file)\n",
    "        atl08 = pd.read_pickle(latest_pickle_file)\n",
    "        print(\"ATL08 db now available from pickled file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying filtering by bounds and by quality for each TILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying additional filtering using FilterUtils; returning a final atl08 dataframe...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'atl08' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'atl08' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "from importlib import reload \n",
    "reload(FilterUtils)\n",
    "\n",
    "\n",
    "print(\"Applying additional filtering using FilterUtils; returning a final atl08 dataframe...\")\n",
    "print(atl08.shape)\n",
    "in_tile_fn = '/projects/maap-users/alexdevseed/boreal_tiles.gpkg'\n",
    "in_tile_layer = 'boreal_tiles_albers'\n",
    "\n",
    "atl08_filt_df_dict = {}\n",
    "\n",
    "for TILE_NUM in INPUT_TILE_NUM_LIST:\n",
    "    \n",
    "    print(\"\\nFiltering by tile: {}\".format(TILE_NUM))\n",
    "    \n",
    "    # Get tile bounds as xmin,xmax,ymin,ymax\n",
    "    in_bounds = FilterUtils.reorder_4326_bounds(in_tile_fn, TILE_NUM, buffer=0, layer = in_tile_layer)\n",
    "\n",
    "    # Filter by bounds\n",
    "    if EPT_APPROACH:\n",
    "        # INPUT EPT APPROACH HERE to return a filtered atl08 data frame?\n",
    "        # https://docs.maap-project.org/en/latest/query/testing-ept-stores.html\n",
    "        # Read a massive EPT of all ATL08 h5 files\n",
    "        #in_ept_fn = ????\n",
    "        # Filtering bounds of EPT by tile\n",
    "        # EPT is filtered using 2.3_dps.py to subset by tile and filter with above_filter_atl08() function in FilterUtils.py\n",
    "        atl08_tmp = FilterUtils.filter_atl08_bounds_tile_ept(in_ept_fn, in_tile_fn, TILE_NUM, in_tile_layer, output_dir, return_pdf=True)\n",
    "    else:\n",
    "        atl08_tmp = FilterUtils.filter_atl08_bounds(atl08_df=atl08, in_bounds=in_bounds)\n",
    "\n",
    "    # Filter by quality\n",
    "    atl08_tmp = FilterUtils.filter_atl08_qual(atl08_tmp, SUBSET_COLS=True, \n",
    "                                                       subset_cols_list=['rh25','rh50','rh60','rh70','rh75','rh80','rh85','rh90','rh95','h_can','h_max_can'], \n",
    "                                                       filt_cols=['h_can','h_dif_ref','m','msw_flg','beam_type','seg_snow'], \n",
    "                                                       thresh_h_can=100, thresh_h_dif=100, month_min=6, month_max=9)\n",
    "    # Build a dict of filtered atl08 by tile_num\n",
    "    atl08_filt_df_dict[TILE_NUM] = atl08_tmp\n",
    "    atl08_tmp = None\n",
    "\n",
    "#import pprint\n",
    "## Prints the nicely formatted dictionary\n",
    "#pprint.pprint(atl08_filt_df_dict)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting covariates by filtered ATL08 by TILE and writing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tile 30542 has 270 filtered ATL08 obs\n",
      "/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/05/28/18/40/58/507315/Copernicus_30542_covars_cog_topo_stack.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tRe-project points to match raster...\n",
      "\tDataframe has new raster value column: elevation\n",
      "\tDataframe has new raster value column: slope\n",
      "\tDataframe has new raster value column: tsri\n",
      "\tDataframe has new raster value column: tpi\n",
      "\tDataframe has new raster value column: slopemask\n",
      "Returning re-projected points with 5 new raster value column: ['elevation', 'slope', 'tsri', 'tpi', 'slopemask']\n",
      "/projects/jabba/dps_output/do_landsat_stack_3-1-2_ubuntu/master/2021/06/04/20/03/25/674070/Landsat8_30542_comp_cog_2015-2020_dps.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tDataframe has new raster value column: Blue\n",
      "\tDataframe has new raster value column: Green\n",
      "\tDataframe has new raster value column: Red\n",
      "\tDataframe has new raster value column: NIR\n",
      "\tDataframe has new raster value column: SWIR\n",
      "\tDataframe has new raster value column: NDVI\n",
      "\tDataframe has new raster value column: SAVI\n",
      "\tDataframe has new raster value column: MSAVI\n",
      "\tDataframe has new raster value column: NDMI\n",
      "\tDataframe has new raster value column: EVI\n",
      "\tDataframe has new raster value column: NBR\n",
      "\tDataframe has new raster value column: NBR2\n",
      "\tDataframe has new raster value column: TCB\n",
      "\tDataframe has new raster value column: TCG\n",
      "\tDataframe has new raster value column: TCW\n",
      "\tDataframe has new raster value column: ValidMask\n",
      "\tDataframe has new raster value column: Xgeo\n",
      "\tDataframe has new raster value column: Ygeo\n",
      "Returning re-projected points with 18 new raster value column: ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo']\n",
      "Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile 30542: /projects/jabba/data/atl08_filt_30542_topo_landsat_20210608194239.csv\n",
      "\n",
      "Tile 30543 has 390 filtered ATL08 obs\n",
      "/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/05/28/18/41/27/049808/Copernicus_30543_covars_cog_topo_stack.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tRe-project points to match raster...\n",
      "\tDataframe has new raster value column: elevation\n",
      "\tDataframe has new raster value column: slope\n",
      "\tDataframe has new raster value column: tsri\n",
      "\tDataframe has new raster value column: tpi\n",
      "\tDataframe has new raster value column: slopemask\n",
      "Returning re-projected points with 5 new raster value column: ['elevation', 'slope', 'tsri', 'tpi', 'slopemask']\n",
      "/projects/jabba/dps_output/do_landsat_stack_3-1-2_ubuntu/master/2021/06/04/19/59/47/464264/Landsat8_30543_comp_cog_2015-2020_dps.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tDataframe has new raster value column: Blue\n",
      "\tDataframe has new raster value column: Green\n",
      "\tDataframe has new raster value column: Red\n",
      "\tDataframe has new raster value column: NIR\n",
      "\tDataframe has new raster value column: SWIR\n",
      "\tDataframe has new raster value column: NDVI\n",
      "\tDataframe has new raster value column: SAVI\n",
      "\tDataframe has new raster value column: MSAVI\n",
      "\tDataframe has new raster value column: NDMI\n",
      "\tDataframe has new raster value column: EVI\n",
      "\tDataframe has new raster value column: NBR\n",
      "\tDataframe has new raster value column: NBR2\n",
      "\tDataframe has new raster value column: TCB\n",
      "\tDataframe has new raster value column: TCG\n",
      "\tDataframe has new raster value column: TCW\n",
      "\tDataframe has new raster value column: ValidMask\n",
      "\tDataframe has new raster value column: Xgeo\n",
      "\tDataframe has new raster value column: Ygeo\n",
      "Returning re-projected points with 18 new raster value column: ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo']\n",
      "Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile 30543: /projects/jabba/data/atl08_filt_30543_topo_landsat_20210608194243.csv\n",
      "\n",
      "Tile 30821 has 195 filtered ATL08 obs\n",
      "/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/05/28/18/41/24/846754/Copernicus_30821_covars_cog_topo_stack.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tRe-project points to match raster...\n",
      "\tDataframe has new raster value column: elevation\n",
      "\tDataframe has new raster value column: slope\n",
      "\tDataframe has new raster value column: tsri\n",
      "\tDataframe has new raster value column: tpi\n",
      "\tDataframe has new raster value column: slopemask\n",
      "Returning re-projected points with 5 new raster value column: ['elevation', 'slope', 'tsri', 'tpi', 'slopemask']\n",
      "-----> No landsat covar COG for tile 30821\n",
      "\n",
      "\n",
      "\n",
      "Tile 30822 has 215 filtered ATL08 obs\n",
      "/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/05/28/18/41/50/735289/Copernicus_30822_covars_cog_topo_stack.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tRe-project points to match raster...\n",
      "\tDataframe has new raster value column: elevation\n",
      "\tDataframe has new raster value column: slope\n",
      "\tDataframe has new raster value column: tsri\n",
      "\tDataframe has new raster value column: tpi\n",
      "\tDataframe has new raster value column: slopemask\n",
      "Returning re-projected points with 5 new raster value column: ['elevation', 'slope', 'tsri', 'tpi', 'slopemask']\n",
      "/projects/jabba/dps_output/do_landsat_stack_3-1-2_ubuntu/master/2021/06/04/20/07/09/134336/Landsat8_30822_comp_cog_2015-2020_dps.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tDataframe has new raster value column: Blue\n",
      "\tDataframe has new raster value column: Green\n",
      "\tDataframe has new raster value column: Red\n",
      "\tDataframe has new raster value column: NIR\n",
      "\tDataframe has new raster value column: SWIR\n",
      "\tDataframe has new raster value column: NDVI\n",
      "\tDataframe has new raster value column: SAVI\n",
      "\tDataframe has new raster value column: MSAVI\n",
      "\tDataframe has new raster value column: NDMI\n",
      "\tDataframe has new raster value column: EVI\n",
      "\tDataframe has new raster value column: NBR\n",
      "\tDataframe has new raster value column: NBR2\n",
      "\tDataframe has new raster value column: TCB\n",
      "\tDataframe has new raster value column: TCG\n",
      "\tDataframe has new raster value column: TCW\n",
      "\tDataframe has new raster value column: ValidMask\n",
      "\tDataframe has new raster value column: Xgeo\n",
      "\tDataframe has new raster value column: Ygeo\n",
      "Returning re-projected points with 18 new raster value column: ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo']\n",
      "Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile 30822: /projects/jabba/data/atl08_filt_30822_topo_landsat_20210608194251.csv\n",
      "\n",
      "Tile 30823 has 234 filtered ATL08 obs\n",
      "/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/05/28/18/41/51/384742/Copernicus_30823_covars_cog_topo_stack.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tRe-project points to match raster...\n",
      "\tDataframe has new raster value column: elevation\n",
      "\tDataframe has new raster value column: slope\n",
      "\tDataframe has new raster value column: tsri\n",
      "\tDataframe has new raster value column: tpi\n",
      "\tDataframe has new raster value column: slopemask\n",
      "Returning re-projected points with 5 new raster value column: ['elevation', 'slope', 'tsri', 'tpi', 'slopemask']\n",
      "/projects/jabba/dps_output/do_landsat_stack_3-1-2_ubuntu/master/2021/06/04/20/09/59/776322/Landsat8_30823_comp_cog_2015-2020_dps.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tDataframe has new raster value column: Blue\n",
      "\tDataframe has new raster value column: Green\n",
      "\tDataframe has new raster value column: Red\n",
      "\tDataframe has new raster value column: NIR\n",
      "\tDataframe has new raster value column: SWIR\n",
      "\tDataframe has new raster value column: NDVI\n",
      "\tDataframe has new raster value column: SAVI\n",
      "\tDataframe has new raster value column: MSAVI\n",
      "\tDataframe has new raster value column: NDMI\n",
      "\tDataframe has new raster value column: EVI\n",
      "\tDataframe has new raster value column: NBR\n",
      "\tDataframe has new raster value column: NBR2\n",
      "\tDataframe has new raster value column: TCB\n",
      "\tDataframe has new raster value column: TCG\n",
      "\tDataframe has new raster value column: TCW\n",
      "\tDataframe has new raster value column: ValidMask\n",
      "\tDataframe has new raster value column: Xgeo\n",
      "\tDataframe has new raster value column: Ygeo\n",
      "Returning re-projected points with 18 new raster value column: ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo']\n",
      "Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile 30823: /projects/jabba/data/atl08_filt_30823_topo_landsat_20210608194255.csv\n"
     ]
    }
   ],
   "source": [
    "import ExtractUtils\n",
    "from importlib import reload \n",
    "reload(ExtractUtils)\n",
    "\n",
    "# Extract values to points; return a CSV for modelling\n",
    "topo_covar_root = \"/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021\"\n",
    "landsat_covar_root = \"/projects/jabba/dps_output/do_landsat_stack_3-1-2_ubuntu/master/2021\"\n",
    "output_dir = \"/projects/jabba/data\"\n",
    "\n",
    "for tile_num, atl08_df_filt in atl08_filt_df_dict.items():\n",
    "    atl08_gdf_topo_landsat = None\n",
    "    print(\"\\nTile {} has {} filtered ATL08 obs\".format(tile_num, len(atl08_df_filt)))\n",
    "    # Convert to geopandas data frame in lat/lon\n",
    "    atl08_gdf = GeoDataFrame(atl08_df_filt, geometry=gpd.points_from_xy(atl08_df_filt.lon, atl08_df_filt.lat), crs='epsg:4326')\n",
    "    \n",
    "    # Get the topo covar COG\n",
    "    topo_covar_tile_list = ExtractUtils.get_covar_fn_list(topo_covar_root, tile_num)\n",
    "    \n",
    "    # Get most recent topo covar COG, reproject ATL08 to match, extract covars\n",
    "    if len(topo_covar_tile_list)>0:\n",
    "        topo_covar_fn = topo_covar_tile_list[-1]\n",
    "        print(topo_covar_fn) \n",
    "        atl08_gdf_topo = ExtractUtils.extract_value_gdf(topo_covar_fn, atl08_gdf, [\"elevation\",\"slope\",\"tsri\",\"tpi\", \"slopemask\"], reproject=True)\n",
    "    else:\n",
    "        print(\"-----> No topo covar COG for tile {}\\n\\n\".format(tile_num))\n",
    "        continue\n",
    "    \n",
    "    # Get the landsat covar COG\n",
    "    landsat_covar_tile_list = ExtractUtils.get_covar_fn_list(landsat_covar_root, tile_num)\n",
    "    \n",
    "    # Get most recent landsat covar COG, extract covars\n",
    "    if len(landsat_covar_tile_list)>0:\n",
    "        landsat_covar_fn = landsat_covar_tile_list[-1]\n",
    "        print(landsat_covar_fn)\n",
    "        atl08_gdf_topo_landsat = ExtractUtils.extract_value_gdf(landsat_covar_fn, atl08_gdf_topo, ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo'], reproject=False)\n",
    "    else:\n",
    "        print(\"-----> No landsat covar COG for tile {}\\n\\n\".format(tile_num))\n",
    "        continue\n",
    "        \n",
    "    if atl08_gdf_topo_landsat is not None:\n",
    "        # CSV the file\n",
    "        cur_time = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        out_csv_fn = os.path.join(output_dir, \"atl08_filt_\"+str(tile_num)+\"_topo_landsat_\"+cur_time+\".csv\")\n",
    "        atl08_gdf_topo_landsat.to_csv(out_csv_fn,index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(\"Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile {}: {}\".format(tile_num, out_csv_fn) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
