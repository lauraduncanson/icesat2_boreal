{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICESat-2 Filtering and Extractions\n",
    "This notebook searches for a spatial subset of data, runs DPS jobs to extract_atl08.py (by Nathan Thomas & Paul Montesano), processing and visualizing the outputs. Set up a notebook run in the first cell, especially whether or not DPS jobs should be submitted.\n",
    "\n",
    "Outline:\n",
    "1. Get BBOX of interest\n",
    "2. Query CMR for ATL08 Granules\n",
    "3. Convert ATL08 Granules to las format only keeping relevant vars/dimensions (h5py/pdal)\n",
    "4. Convert las files to EPT data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/matplotlib-3.3.2-py3.7-linux-x86_64.egg/mpl_toolkits/mplot3d/__init__.py:1: MatplotlibDeprecationWarning:\n",
      "\n",
      "\n",
      "The deprecated function was deprecated in Matplotlib 3.4 and will be removed two minor releases later.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cbook' has no attribute '_rename_parameter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ae9e6005b0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSegmentedColormap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes_grid1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_axes_locatable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;31m#from matplotlib_scalebar.scalebar import ScaleBar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontextily\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib-3.3.2-py3.7-linux-x86_64.egg/mpl_toolkits/axes_grid1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maxes_size\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maxes_divider\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDivider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubplotDivider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_axes_locatable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maxes_grid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageGrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAxesGrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparasite_axes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhost_subplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost_axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib-3.3.2-py3.7-linux-x86_64.egg/mpl_toolkits/axes_grid1/axes_grid.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCbarAxesBase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morientation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib-3.3.2-py3.7-linux-x86_64.egg/mpl_toolkits/axes_grid1/axes_grid.py\u001b[0m in \u001b[0;36mCbarAxesBase\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rename_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3.2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"locator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ticks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.cbook' has no attribute '_rename_parameter'"
     ]
    }
   ],
   "source": [
    "# 2.3 ICESat-2 extraction, merging, filtering, exploring, mapping\n",
    "from maap.maap import MAAP\n",
    "maap = MAAP()\n",
    "\n",
    "import ipycmc\n",
    "w = ipycmc.MapCMC()\n",
    "\n",
    "if False:\n",
    "    import importlib\n",
    "    lib_loader = importlib.find_loader('cartopy')\n",
    "\n",
    "    if lib_loader is not None:\n",
    "        REBUILD_CONDA_ENV = False\n",
    "        print(\"No need to re-build conda env.\")\n",
    "    else:\n",
    "        REBUILD_CONDA_ENV = True\n",
    "        print(\"Re-build conda env...\")\n",
    "\n",
    "    if REBUILD_CONDA_ENV:\n",
    "        #### This notebook uses a DPS job to run extract_atl08.py to convert h5's to csv's, then appends all csv's into a pandas geodataframe.\n",
    "        #### Returns: a pandas geodataframe that should hold the entire set of ATL08 data for this project\n",
    "        #### Notes:\n",
    "        ###### ISSUE: how to relaibly activate a conda env that can support this notebook.\n",
    "        ###### Need to 'conda activate' an env that has geopandas - but where do I do this 'activate'. How does terminal env interact with nb?\n",
    "        ###### Workaround: always do this to base:\n",
    "        ! conda install -c conda-forge geopandas -y\n",
    "        #! conda install -c conda-forge cartopy -y\n",
    "        ! conda install -c conda-forge descartes -y\n",
    "        ! conda install -c conda-forge seaborn -y\n",
    "        ! conda install contextily --channel conda-forge -y\n",
    "        #! conda install -c conda-forge matplotlib_scalebar -y\n",
    "        ##https://www.essoar.org/doi/10.1002/essoar.10501423.1\n",
    "        ##https://www.essoar.org/pdfjs/10.1002/essoar.10501423.1\n",
    "        ##https://github.com/icesat2py/icepyx/blob/master/examples/ICESat-2_DEM_comparison_Colombia_working.ipynb\n",
    "        ##https://github.com/ICESAT-2HackWeek/2020_ICESat-2_Hackweek_Tutorials\n",
    "        ##https://icesat-2hackweek.github.io/learning-resources/logistics/schedule/\n",
    "        ##https://github.com/giswqs/earthengine-py-notebooks\n",
    "\n",
    "import geopandas as gpd\n",
    "import descartes\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from geopandas import GeoDataFrame\n",
    "from geopandas.tools import sjoin\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import random \n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import cartopy.crs as ccrs\n",
    "#from cartopy.feature import NaturalEarthFeature, LAND, COASTLINE\n",
    "#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import datetime\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "#from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import contextily as ctx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/projects/code/icesat2_boreal/notebooks/3.Gridded_product_development')\n",
    "from CovariateUtils import *\n",
    "\n",
    "import FilterUtils\n",
    "import ExtractUtils\n",
    "\n",
    "boreal_tile_index_path = '/projects/maap-users/alexdevseed/boreal_tiles.gpkg'\n",
    "\n",
    "# Run extract_at08.py as a DPS job (see nb 1.3 for template of how this can be done)\n",
    "DPS_OUTPUT_DIR = '/projects/r2d2/dps_outputs/extract_atl08_dps_orig/master/2021/06'\n",
    "#DPS_OUTPUT_DIR = '/projects/jabba/dps_output/2.3_output' #'/projects/above/processed_data/2.3_output'\n",
    "\n",
    "RUN_DPS = False\n",
    "\n",
    "EPT_APPROACH = False\n",
    "\n",
    "H_DIFF_THRESH = 100\n",
    "H_CAN_THRESH = 100\n",
    "\n",
    "READ_PICKLE = False\n",
    "DIR_PICKLE = '/projects/jabba/data'#'/projects/above'\n",
    "\n",
    "DO_ATL08_CSV_SUBSET = False # <- set to True for testing\n",
    "SUBSET_FRAC_SIZE = 0.50\n",
    "\n",
    "#COPY_CSVS = False\n",
    "CSV_TO_DIR = '/projects/jabba/data'#/projects/r2d2/above/atl08_csvs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAAP.searchGranule, make h5 and CSV list: specify the tiles (bbox) and years, get a list of granules, and get a list of ATL08 CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year:  2019\n",
      "\tTILE_NUM: 30542 (-117.10749852280769,50.78795362739066,-116.50936927974429,51.16389512140189)\n",
      "\t\t# ATL08 in BBOX: 12\n",
      "\tTILE_NUM: 30543 (-116.85483935749697,50.57021397455074,-116.26013039328576,50.94612905730388)\n",
      "\t\t# ATL08 in BBOX: 11\n",
      "\tTILE_NUM: 30821 (-117.45119389452712,50.62860368381672,-116.85483935749697,51.00498537669879)\n",
      "\t\t# ATL08 in BBOX: 11\n",
      "\tTILE_NUM: 30822 (-117.19773367251135,50.4115903977891,-116.60475601802764,50.78795362739066)\n",
      "\t\t# ATL08 in BBOX: 11\n",
      "\tTILE_NUM: 30823 (-116.94683824344975,50.19386902261471,-116.35721614098954,50.57021397455074)\n",
      "\t\t# ATL08 in BBOX: 12\n",
      "\t# ATL08 in 2019: 57\n",
      "Year:  2020\n",
      "\tTILE_NUM: 30542 (-117.10749852280769,50.78795362739066,-116.50936927974429,51.16389512140189)\n",
      "\t\t# ATL08 in BBOX: 11\n",
      "\tTILE_NUM: 30543 (-116.85483935749697,50.57021397455074,-116.26013039328576,50.94612905730388)\n",
      "\t\t# ATL08 in BBOX: 11\n",
      "\tTILE_NUM: 30821 (-117.45119389452712,50.62860368381672,-116.85483935749697,51.00498537669879)\n",
      "\t\t# ATL08 in BBOX: 11\n",
      "\tTILE_NUM: 30822 (-117.19773367251135,50.4115903977891,-116.60475601802764,50.78795362739066)\n",
      "\t\t# ATL08 in BBOX: 10\n",
      "\tTILE_NUM: 30823 (-116.94683824344975,50.19386902261471,-116.35721614098954,50.57021397455074)\n",
      "\t\t# ATL08 in BBOX: 12\n",
      "\t# ATL08 in 2020: 55\n",
      "# ATL08 h5s total: 112\n",
      "# ATL08 CSVs found: 76\n",
      "# ATL08 CSVs not found in /projects/jabba/dps_output/2.3_output: 36\n"
     ]
    }
   ],
   "source": [
    "# Make a list of ATL08 files you will want to DPS over\n",
    "\n",
    "# Test tiles\n",
    "#INPUT_TILE_NUM_LIST = [30542, 30543, 30821, 30822, 30823]\n",
    "\n",
    "# NA tiles\n",
    "# Read the boreal tile index file\n",
    "boreal_tile_index_path = '/projects/maap-users/alexdevseed/boreal_tiles.gpkg'\n",
    "boreal_tile_index = gpd.read_file(boreal_tile_index_path)\n",
    "boreal_tile_index_subset = boreal_tile_index.to_crs(4326).cx[-170:-50, 50:75]\n",
    "\n",
    "# Boreal NA tiles: need just a list of tile_ids\n",
    "INPUT_TILE_NUM_LIST = boreal_tile_index_subset['layer'].astype(int).tolist()\n",
    "\n",
    "INPUT_YEARS_LIST = ['2019','2020','2021']\n",
    "\n",
    "all_atl08 = []\n",
    "\n",
    "for YEAR in INPUT_YEARS_LIST:\n",
    "    \n",
    "    print(\"Year: \", YEAR)\n",
    "    all_atl08_year = []\n",
    "    \n",
    "    for TILE_NUM in INPUT_TILE_NUM_LIST:\n",
    "        \n",
    "        # This is fine for a single tile\n",
    "        tile_parts = get_index_tile(boreal_tile_index_path, TILE_NUM, buffer=0, layer = 'boreal_tiles_albers')\n",
    "\n",
    "        BBOX_TILE = ','.join(str(x) for x in tile_parts['bbox_4326'])\n",
    "\n",
    "        # Other BBOXs\n",
    "        BBOX_NA = \"-180,50,-50,75\"\n",
    "        BBOX_CIRC = \"-180,40,180,75\" # You'll need to edit run_above.sh to adjust the geo filtering called for with extract_atl08.py\n",
    "\n",
    "        BBOX = BBOX_TILE\n",
    "        print(\"\\tTILE_NUM: {} ({})\".format(TILE_NUM, BBOX) )\n",
    "\n",
    "        COLLECTID_ATL08_V3 = \"C1200235747-NASA_MAAP\"\n",
    "\n",
    "        # Note: we want to be able to do a 'recurring' seasonal search, regardless of year\n",
    "        DATERANGE_SUMMER = YEAR+'-06-01T00:00:00Z,'+YEAR+'-09-30T23:59:59Z'\n",
    "\n",
    "        # We dont really want a limit: Not really sure how to set this; just use very high number?\n",
    "        MAX_ATL08_ORBITS = 100000\n",
    "\n",
    "        granules = maap.searchGranule(collection_concept_id=COLLECTID_ATL08_V3, \n",
    "                                      temporal=DATERANGE_SUMMER, \n",
    "                                      bounding_box=BBOX, \n",
    "                                      limit=MAX_ATL08_ORBITS)\n",
    "\n",
    "        # This is a list of the granule URLs for processing\n",
    "        granules_list_ATL08 = FilterUtils.get_granules_list(granules)\n",
    "               \n",
    "        print(\"\\t\\t# ATL08 in BBOX: {}\".format(len(granules_list_ATL08)) )\n",
    "        all_atl08_year += granules_list_ATL08\n",
    "    print(\"\\t# ATL08 in {}: {}\".format(YEAR, len(all_atl08_year)) )\n",
    "    all_atl08 += all_atl08_year\n",
    "    \n",
    "print('# ATL08 h5s total: {}'.format(len(all_atl08)) )\n",
    "\n",
    "# Change h5s to CSVs\n",
    "all_atl08_csvs = [os.path.join(DPS_OUTPUT_DIR, os.path.basename(f).replace('.h5','_30m.csv')) for f in all_atl08]\n",
    "\n",
    "all_atl08_csvs_NOT_FOUND = []\n",
    "for file in all_atl08_csvs: \n",
    "    if not os.path.isfile(file):\n",
    "        all_atl08_csvs_NOT_FOUND.append(file)\n",
    "             \n",
    "all_atl08_csvs_FOUND = [x for x in all_atl08_csvs if x not in all_atl08_csvs_NOT_FOUND]\n",
    "print('# ATL08 CSVs found: {}'.format(len(all_atl08_csvs_FOUND)) ) \n",
    "#print('\\n'.join(all_atl08_csvs_FOUND) )\n",
    "print('# ATL08 CSVs not found in {}: {}'.format(DPS_OUTPUT_DIR, len(all_atl08_csvs_NOT_FOUND)) )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a single DPS job to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DPS:\n",
    "    ##################################\n",
    "    #Test DPS submission on a single file\n",
    "    granule=granules_list_ATL08[0]\n",
    "\n",
    "    submit_result = maap.submitJob(identifier=\"nothing\", algo_id=\"run_above_ubuntu\", \n",
    "                                       version=\"master\", \n",
    "                                       username=\"r2d2\", \n",
    "                                       icesat2_granule=granule)\n",
    "    print(submit_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DPS in Batch Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running DPS; probably because output from extract_atl08 DPS job already exists.\n",
      "/projects/above/processed_data/2.3_output\n"
     ]
    }
   ],
   "source": [
    "# Extraction\n",
    "#\n",
    "# DPS SUBMISSION\n",
    "if RUN_DPS:\n",
    "    # Here is where I submit a job \n",
    "    # identified with 'algo_id' (in yaml file)\n",
    "    # that specifies a bash script /projects/above/gitlab_repos/atl08_extract_repo/run_above.sh \n",
    "    # that will call the 'algorithm' (extract_atl08.py)\n",
    "\n",
    "    # Uses granule list from nb 2.1\n",
    "    # CHANGE the submitJob args!\n",
    "    for g in range(len(granules_list_ATL08)):\n",
    "        granule = granules_list_ATL08[g]\n",
    "        submit_result = maap.submitJob(identifier=\"nothing\", algo_id=\"run_above_ubuntu\", \n",
    "                                   version=\"master\", \n",
    "                                   username=\"r2d2\", \n",
    "                                   icesat2_granule=granule)\n",
    "        if g == 1:\n",
    "            print(submit_result)\n",
    "        if g == 100:\n",
    "            print (submit_result)\n",
    "        if g == 1000:\n",
    "            print (submit_result)\n",
    "        if g == 2000:\n",
    "            print (submit_result)\n",
    "        if g == 3000:\n",
    "            print (submit_result)\n",
    "        if g == 4000:\n",
    "            print (submit_result)\n",
    "        if g == len(granules_list_ATL08):\n",
    "            print (submit_result)\n",
    "            print ('done!')\n",
    "        \n",
    "else:\n",
    "    print(\"Not running DPS; probably because output from extract_atl08 DPS job already exists.\")\n",
    "    print(DPS_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge DPS outputs into data frame for visualizatioon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of ATL08 files found after DPS to extract atl08 to CSV:  76\n",
      "Creating pandas data frame...\n",
      "CPU times: user 19.3 s, sys: 1.13 s, total: 20.5 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not READ_PICKLE:\n",
    "    \n",
    "    # Merging\n",
    "\n",
    "    # List of CSVs made in MAAP.searchGranule chunk above\n",
    "    print(\"# of ATL08 files found after DPS to extract atl08 to CSV: \",len(all_atl08_csvs_FOUND))\n",
    "    \n",
    "    if False:\n",
    "        # Find and delete any CSV that has a size of 0\n",
    "        #! find $DPS_OUTPUT_DIR -name \"*.csv\" -size 0 -delete\n",
    "        print(\"Making list of ATL08 csv files...\")\n",
    "        # Find all remaining output CSVs from DPS jobs\n",
    "        all_atl08_csvs = glob.glob(DPS_OUTPUT_DIR+\"/ATL08*.csv\", recursive=True)\n",
    "\n",
    "        # This could break if you randomly grab an incomplete or empty CSV\n",
    "        if DO_ATL08_CSV_SUBSET:\n",
    "            all_atl08_csvs_FOUND = random.sample(all_atl08_csvs_FOUND, math.floor(SUBSET_FRAC_SIZE * len(all_atl08_csvs_FOUND)))\n",
    "            print(\"# of ATL08 files after test sample: \",len(all_atl08_csvs_FOUND))\n",
    "\n",
    "    # Merge all files in the list\n",
    "    print(\"Creating pandas data frame...\")\n",
    "    atl08 = pd.concat([pd.read_csv(f) for f in all_atl08_csvs_FOUND ], sort=False)\n",
    "    \n",
    "    # Probably not necessary\n",
    "    #print('finished pickle') #<--no; there isnt any pickling here; its written aftern the Filtering chunk\n",
    "    #atl08.to_csv( \"/projects/above/processed_data/atl08_merged.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in old data frame from a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False:\n",
    "    if not READ_PICKLE and not EPT_APPROACH:\n",
    "\n",
    "        if False:\n",
    "            # Filtering    <------ THIS IS UPDATED USING THE METHOD IN FilterUtils.py (~5-26-2021)\n",
    "            atl08 =  atl08[\n",
    "                           (atl08.msw_flg == 0) & \n",
    "                           (atl08.beam_type == 'Strong') & \n",
    "                           (atl08.seg_snow == 'snow free land')\n",
    "                            ]\n",
    "            print(f\"After filtering, there are {atl08.shape[0]} observations in this dataframe.\")\n",
    "\n",
    "        # Pickle the file\n",
    "        cur_time = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        samp_frac_str = \"samp-all\"\n",
    "        if DO_ATL08_CSV_SUBSET:\n",
    "            samp_frac_str = \"samp-\" + '{:1.2f}'.format(SUBSET_FRAC_SIZE).replace('.','p')\n",
    "        atl08.to_pickle(os.path.join(DIR_PICKLE, \"atl08_\"+samp_frac_str+\"_\"+cur_time+\".pkl\"))\n",
    "    else:\n",
    "        print(\"Getting the latest merged, filtered, & compressed file of ATL08 obs as a pandas dataframe...\")\n",
    "        list_of_pickles = glob.glob(DIR_PICKLE+'/atl08*.pkl') # * means all if need specific format then *.csv\n",
    "        latest_pickle_file = max(list_of_pickles, key=os.path.getctime)\n",
    "        print(latest_pickle_file)\n",
    "        atl08 = pd.read_pickle(latest_pickle_file)\n",
    "        print(\"ATL08 db now available from pickled file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying filtering by bounds and by quality for each TILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying additional filtering using FilterUtils; returning a final atl08 dataframe...\n",
      "(3265521, 54)\n",
      "\n",
      "Filtering by tile: 30542\n",
      "Filtering by bounds: [-117.10749852280769, -116.50936927974429, 50.78795362739066, 51.16389512140189]\n",
      "Returning a data frame\n",
      "\n",
      "Pre-filter data cleaning...\n",
      "\n",
      "Get beam type from orbit orientation and ground track...\n",
      "['Weak' 'Strong']\n",
      "Cast some columns to type float: ['lat', 'lon', 'h_can', 'h_te_best', 'ter_slp']\n",
      "Cast some columns to type integer: ['n_ca_ph', 'n_seg_ph', 'n_toc_ph']\n",
      "\n",
      "Filtering for quality:\n",
      "\tfor clear skies + strong beam + snow free land,\n",
      "\th_can < 100,\n",
      "\televation diff from ref < 100,\n",
      "\tmonths 6-9\n",
      "Before quaity filtering: 22885 observations in the input dataframe.\n",
      "After quality filtering: 270 observations in the output dataframe.\n",
      "Returning a pandas data frame of filtered observations for columns: ['lon', 'lat', 'rh25', 'rh50', 'rh60', 'rh70', 'rh75', 'rh80', 'rh85', 'rh90', 'rh95', 'h_can', 'h_max_can']\n",
      "Shape: (270, 13) \n",
      "\n",
      "Filtering by tile: 30543\n",
      "Filtering by bounds: [-116.85483935749697, -116.26013039328576, 50.57021397455074, 50.94612905730388]\n",
      "Returning a data frame\n",
      "\n",
      "Pre-filter data cleaning...\n",
      "\n",
      "Get beam type from orbit orientation and ground track...\n",
      "['Weak' 'Strong']\n",
      "Cast some columns to type float: ['lat', 'lon', 'h_can', 'h_te_best', 'ter_slp']\n",
      "Cast some columns to type integer: ['n_ca_ph', 'n_seg_ph', 'n_toc_ph']\n",
      "\n",
      "Filtering for quality:\n",
      "\tfor clear skies + strong beam + snow free land,\n",
      "\th_can < 100,\n",
      "\televation diff from ref < 100,\n",
      "\tmonths 6-9\n",
      "Before quaity filtering: 23213 observations in the input dataframe.\n",
      "After quality filtering: 390 observations in the output dataframe.\n",
      "Returning a pandas data frame of filtered observations for columns: ['lon', 'lat', 'rh25', 'rh50', 'rh60', 'rh70', 'rh75', 'rh80', 'rh85', 'rh90', 'rh95', 'h_can', 'h_max_can']\n",
      "Shape: (390, 13) \n",
      "\n",
      "Filtering by tile: 30821\n",
      "Filtering by bounds: [-117.45119389452712, -116.85483935749697, 50.62860368381672, 51.00498537669879]\n",
      "Returning a data frame\n",
      "\n",
      "Pre-filter data cleaning...\n",
      "\n",
      "Get beam type from orbit orientation and ground track...\n",
      "['Weak' 'Strong']\n",
      "Cast some columns to type float: ['lat', 'lon', 'h_can', 'h_te_best', 'ter_slp']\n",
      "Cast some columns to type integer: ['n_ca_ph', 'n_seg_ph', 'n_toc_ph']\n",
      "\n",
      "Filtering for quality:\n",
      "\tfor clear skies + strong beam + snow free land,\n",
      "\th_can < 100,\n",
      "\televation diff from ref < 100,\n",
      "\tmonths 6-9\n",
      "Before quaity filtering: 19094 observations in the input dataframe.\n",
      "After quality filtering: 195 observations in the output dataframe.\n",
      "Returning a pandas data frame of filtered observations for columns: ['lon', 'lat', 'rh25', 'rh50', 'rh60', 'rh70', 'rh75', 'rh80', 'rh85', 'rh90', 'rh95', 'h_can', 'h_max_can']\n",
      "Shape: (195, 13) \n",
      "\n",
      "Filtering by tile: 30822\n",
      "Filtering by bounds: [-117.19773367251135, -116.60475601802764, 50.4115903977891, 50.78795362739066]\n",
      "Returning a data frame\n",
      "\n",
      "Pre-filter data cleaning...\n",
      "\n",
      "Get beam type from orbit orientation and ground track...\n",
      "['Weak' 'Strong']\n",
      "Cast some columns to type float: ['lat', 'lon', 'h_can', 'h_te_best', 'ter_slp']\n",
      "Cast some columns to type integer: ['n_ca_ph', 'n_seg_ph', 'n_toc_ph']\n",
      "\n",
      "Filtering for quality:\n",
      "\tfor clear skies + strong beam + snow free land,\n",
      "\th_can < 100,\n",
      "\televation diff from ref < 100,\n",
      "\tmonths 6-9\n",
      "Before quaity filtering: 25535 observations in the input dataframe.\n",
      "After quality filtering: 215 observations in the output dataframe.\n",
      "Returning a pandas data frame of filtered observations for columns: ['lon', 'lat', 'rh25', 'rh50', 'rh60', 'rh70', 'rh75', 'rh80', 'rh85', 'rh90', 'rh95', 'h_can', 'h_max_can']\n",
      "Shape: (215, 13) \n",
      "\n",
      "Filtering by tile: 30823\n",
      "Filtering by bounds: [-116.94683824344975, -116.35721614098954, 50.19386902261471, 50.57021397455074]\n",
      "Returning a data frame\n",
      "\n",
      "Pre-filter data cleaning...\n",
      "\n",
      "Get beam type from orbit orientation and ground track...\n",
      "['Weak' 'Strong']\n",
      "Cast some columns to type float: ['lat', 'lon', 'h_can', 'h_te_best', 'ter_slp']\n",
      "Cast some columns to type integer: ['n_ca_ph', 'n_seg_ph', 'n_toc_ph']\n",
      "\n",
      "Filtering for quality:\n",
      "\tfor clear skies + strong beam + snow free land,\n",
      "\th_can < 100,\n",
      "\televation diff from ref < 100,\n",
      "\tmonths 6-9\n",
      "Before quaity filtering: 26589 observations in the input dataframe.\n",
      "After quality filtering: 234 observations in the output dataframe.\n",
      "Returning a pandas data frame of filtered observations for columns: ['lon', 'lat', 'rh25', 'rh50', 'rh60', 'rh70', 'rh75', 'rh80', 'rh85', 'rh90', 'rh95', 'h_can', 'h_max_can']\n",
      "Shape: (234, 13) \n",
      "CPU times: user 7.66 s, sys: 509 ms, total: 8.17 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "from importlib import reload \n",
    "reload(FilterUtils)\n",
    "\n",
    "\n",
    "print(\"Applying additional filtering using FilterUtils; returning a final atl08 dataframe...\")\n",
    "print(atl08.shape)\n",
    "in_tile_fn = '/projects/maap-users/alexdevseed/boreal_tiles.gpkg'\n",
    "in_tile_layer = 'boreal_tiles_albers'\n",
    "\n",
    "atl08_filt_df_dict = {}\n",
    "\n",
    "for TILE_NUM in INPUT_TILE_NUM_LIST:\n",
    "    \n",
    "    print(\"\\nFiltering by tile: {}\".format(TILE_NUM))\n",
    "    \n",
    "    # Get tile bounds as xmin,xmax,ymin,ymax\n",
    "    in_bounds = FilterUtils.reorder_4326_bounds(in_tile_fn, TILE_NUM, buffer=0, layer = in_tile_layer)\n",
    "\n",
    "    # Filter by bounds\n",
    "    if EPT_APPROACH:\n",
    "        # INPUT EPT APPROACH HERE to return a filtered atl08 data frame?\n",
    "        # https://docs.maap-project.org/en/latest/query/testing-ept-stores.html\n",
    "        # Read a massive EPT of all ATL08 h5 files\n",
    "        #in_ept_fn = ????\n",
    "        # Filtering bounds of EPT by tile\n",
    "        # EPT is filtered using 2.3_dps.py to subset by tile and filter with above_filter_atl08() function in FilterUtils.py\n",
    "        atl08_tmp = FilterUtils.filter_atl08_bounds_tile_ept(in_ept_fn, in_tile_fn, TILE_NUM, in_tile_layer, output_dir, return_pdf=True)\n",
    "    else:\n",
    "        atl08_tmp = FilterUtils.filter_atl08_bounds(atl08_df=atl08, in_bounds=in_bounds)\n",
    "\n",
    "    # Filter by quality\n",
    "    atl08_tmp = FilterUtils.filter_atl08_qual(atl08_tmp, SUBSET_COLS=True, \n",
    "                                                       subset_cols_list=['rh25','rh50','rh60','rh70','rh75','rh80','rh85','rh90','rh95','h_can','h_max_can'], \n",
    "                                                       filt_cols=['h_can','h_dif_ref','m','msw_flg','beam_type','seg_snow'], \n",
    "                                                       thresh_h_can=100, thresh_h_dif=100, month_min=6, month_max=9)\n",
    "    # Build a dict of filtered atl08 by tile_num\n",
    "    atl08_filt_df_dict[TILE_NUM] = atl08_tmp\n",
    "    atl08_tmp = None\n",
    "\n",
    "#import pprint\n",
    "## Prints the nicely formatted dictionary\n",
    "#pprint.pprint(atl08_filt_df_dict)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting covariates by filtered ATL08 by TILE and writing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tile 30542 has 270 filtered ATL08 obs\n",
      "/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/05/28/18/40/58/507315/Copernicus_30542_covars_cog_topo_stack.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tRe-project points to match raster...\n",
      "\tDataframe has new raster value column: elevation\n",
      "\tDataframe has new raster value column: slope\n",
      "\tDataframe has new raster value column: tsri\n",
      "\tDataframe has new raster value column: tpi\n",
      "\tDataframe has new raster value column: slopemask\n",
      "Returning re-projected points with 5 new raster value column: ['elevation', 'slope', 'tsri', 'tpi', 'slopemask']\n",
      "/projects/jabba/dps_output/do_landsat_stack_3-1-2_ubuntu/master/2021/06/04/20/03/25/674070/Landsat8_30542_comp_cog_2015-2020_dps.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tDataframe has new raster value column: Blue\n",
      "\tDataframe has new raster value column: Green\n",
      "\tDataframe has new raster value column: Red\n",
      "\tDataframe has new raster value column: NIR\n",
      "\tDataframe has new raster value column: SWIR\n",
      "\tDataframe has new raster value column: NDVI\n",
      "\tDataframe has new raster value column: SAVI\n",
      "\tDataframe has new raster value column: MSAVI\n",
      "\tDataframe has new raster value column: NDMI\n",
      "\tDataframe has new raster value column: EVI\n",
      "\tDataframe has new raster value column: NBR\n",
      "\tDataframe has new raster value column: NBR2\n",
      "\tDataframe has new raster value column: TCB\n",
      "\tDataframe has new raster value column: TCG\n",
      "\tDataframe has new raster value column: TCW\n",
      "\tDataframe has new raster value column: ValidMask\n",
      "\tDataframe has new raster value column: Xgeo\n",
      "\tDataframe has new raster value column: Ygeo\n",
      "Returning re-projected points with 18 new raster value column: ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo']\n",
      "Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile 30542: /projects/jabba/data/atl08_filt_30542_topo_landsat_20210608194239.csv\n",
      "\n",
      "Tile 30543 has 390 filtered ATL08 obs\n",
      "/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/05/28/18/41/27/049808/Copernicus_30543_covars_cog_topo_stack.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tRe-project points to match raster...\n",
      "\tDataframe has new raster value column: elevation\n",
      "\tDataframe has new raster value column: slope\n",
      "\tDataframe has new raster value column: tsri\n",
      "\tDataframe has new raster value column: tpi\n",
      "\tDataframe has new raster value column: slopemask\n",
      "Returning re-projected points with 5 new raster value column: ['elevation', 'slope', 'tsri', 'tpi', 'slopemask']\n",
      "/projects/jabba/dps_output/do_landsat_stack_3-1-2_ubuntu/master/2021/06/04/19/59/47/464264/Landsat8_30543_comp_cog_2015-2020_dps.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tDataframe has new raster value column: Blue\n",
      "\tDataframe has new raster value column: Green\n",
      "\tDataframe has new raster value column: Red\n",
      "\tDataframe has new raster value column: NIR\n",
      "\tDataframe has new raster value column: SWIR\n",
      "\tDataframe has new raster value column: NDVI\n",
      "\tDataframe has new raster value column: SAVI\n",
      "\tDataframe has new raster value column: MSAVI\n",
      "\tDataframe has new raster value column: NDMI\n",
      "\tDataframe has new raster value column: EVI\n",
      "\tDataframe has new raster value column: NBR\n",
      "\tDataframe has new raster value column: NBR2\n",
      "\tDataframe has new raster value column: TCB\n",
      "\tDataframe has new raster value column: TCG\n",
      "\tDataframe has new raster value column: TCW\n",
      "\tDataframe has new raster value column: ValidMask\n",
      "\tDataframe has new raster value column: Xgeo\n",
      "\tDataframe has new raster value column: Ygeo\n",
      "Returning re-projected points with 18 new raster value column: ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo']\n",
      "Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile 30543: /projects/jabba/data/atl08_filt_30543_topo_landsat_20210608194243.csv\n",
      "\n",
      "Tile 30821 has 195 filtered ATL08 obs\n",
      "/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/05/28/18/41/24/846754/Copernicus_30821_covars_cog_topo_stack.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tRe-project points to match raster...\n",
      "\tDataframe has new raster value column: elevation\n",
      "\tDataframe has new raster value column: slope\n",
      "\tDataframe has new raster value column: tsri\n",
      "\tDataframe has new raster value column: tpi\n",
      "\tDataframe has new raster value column: slopemask\n",
      "Returning re-projected points with 5 new raster value column: ['elevation', 'slope', 'tsri', 'tpi', 'slopemask']\n",
      "-----> No landsat covar COG for tile 30821\n",
      "\n",
      "\n",
      "\n",
      "Tile 30822 has 215 filtered ATL08 obs\n",
      "/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/05/28/18/41/50/735289/Copernicus_30822_covars_cog_topo_stack.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tRe-project points to match raster...\n",
      "\tDataframe has new raster value column: elevation\n",
      "\tDataframe has new raster value column: slope\n",
      "\tDataframe has new raster value column: tsri\n",
      "\tDataframe has new raster value column: tpi\n",
      "\tDataframe has new raster value column: slopemask\n",
      "Returning re-projected points with 5 new raster value column: ['elevation', 'slope', 'tsri', 'tpi', 'slopemask']\n",
      "/projects/jabba/dps_output/do_landsat_stack_3-1-2_ubuntu/master/2021/06/04/20/07/09/134336/Landsat8_30822_comp_cog_2015-2020_dps.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tDataframe has new raster value column: Blue\n",
      "\tDataframe has new raster value column: Green\n",
      "\tDataframe has new raster value column: Red\n",
      "\tDataframe has new raster value column: NIR\n",
      "\tDataframe has new raster value column: SWIR\n",
      "\tDataframe has new raster value column: NDVI\n",
      "\tDataframe has new raster value column: SAVI\n",
      "\tDataframe has new raster value column: MSAVI\n",
      "\tDataframe has new raster value column: NDMI\n",
      "\tDataframe has new raster value column: EVI\n",
      "\tDataframe has new raster value column: NBR\n",
      "\tDataframe has new raster value column: NBR2\n",
      "\tDataframe has new raster value column: TCB\n",
      "\tDataframe has new raster value column: TCG\n",
      "\tDataframe has new raster value column: TCW\n",
      "\tDataframe has new raster value column: ValidMask\n",
      "\tDataframe has new raster value column: Xgeo\n",
      "\tDataframe has new raster value column: Ygeo\n",
      "Returning re-projected points with 18 new raster value column: ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo']\n",
      "Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile 30822: /projects/jabba/data/atl08_filt_30822_topo_landsat_20210608194251.csv\n",
      "\n",
      "Tile 30823 has 234 filtered ATL08 obs\n",
      "/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021/05/28/18/41/51/384742/Copernicus_30823_covars_cog_topo_stack.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tRe-project points to match raster...\n",
      "\tDataframe has new raster value column: elevation\n",
      "\tDataframe has new raster value column: slope\n",
      "\tDataframe has new raster value column: tsri\n",
      "\tDataframe has new raster value column: tpi\n",
      "\tDataframe has new raster value column: slopemask\n",
      "Returning re-projected points with 5 new raster value column: ['elevation', 'slope', 'tsri', 'tpi', 'slopemask']\n",
      "/projects/jabba/dps_output/do_landsat_stack_3-1-2_ubuntu/master/2021/06/04/20/09/59/776322/Landsat8_30823_comp_cog_2015-2020_dps.tif\n",
      "\tOpen the raster and store metadata...\n",
      "\tDataframe has new raster value column: Blue\n",
      "\tDataframe has new raster value column: Green\n",
      "\tDataframe has new raster value column: Red\n",
      "\tDataframe has new raster value column: NIR\n",
      "\tDataframe has new raster value column: SWIR\n",
      "\tDataframe has new raster value column: NDVI\n",
      "\tDataframe has new raster value column: SAVI\n",
      "\tDataframe has new raster value column: MSAVI\n",
      "\tDataframe has new raster value column: NDMI\n",
      "\tDataframe has new raster value column: EVI\n",
      "\tDataframe has new raster value column: NBR\n",
      "\tDataframe has new raster value column: NBR2\n",
      "\tDataframe has new raster value column: TCB\n",
      "\tDataframe has new raster value column: TCG\n",
      "\tDataframe has new raster value column: TCW\n",
      "\tDataframe has new raster value column: ValidMask\n",
      "\tDataframe has new raster value column: Xgeo\n",
      "\tDataframe has new raster value column: Ygeo\n",
      "Returning re-projected points with 18 new raster value column: ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo']\n",
      "Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile 30823: /projects/jabba/data/atl08_filt_30823_topo_landsat_20210608194255.csv\n"
     ]
    }
   ],
   "source": [
    "import ExtractUtils\n",
    "from importlib import reload \n",
    "reload(ExtractUtils)\n",
    "\n",
    "# Extract values to points; return a CSV for modelling\n",
    "topo_covar_root = \"/projects/jabba/dps_output/do_topo_stack_3-1-5_ubuntu/master/2021\"\n",
    "landsat_covar_root = \"/projects/jabba/dps_output/do_landsat_stack_3-1-2_ubuntu/master/2021\"\n",
    "output_dir = \"/projects/jabba/data\"\n",
    "\n",
    "for tile_num, atl08_df_filt in atl08_filt_df_dict.items():\n",
    "    atl08_gdf_topo_landsat = None\n",
    "    print(\"\\nTile {} has {} filtered ATL08 obs\".format(tile_num, len(atl08_df_filt)))\n",
    "    # Convert to geopandas data frame in lat/lon\n",
    "    atl08_gdf = GeoDataFrame(atl08_df_filt, geometry=gpd.points_from_xy(atl08_df_filt.lon, atl08_df_filt.lat), crs='epsg:4326')\n",
    "    \n",
    "    # Get the topo covar COG\n",
    "    topo_covar_tile_list = ExtractUtils.get_covar_fn_list(topo_covar_root, tile_num)\n",
    "    \n",
    "    # Get most recent topo covar COG, reproject ATL08 to match, extract covars\n",
    "    if len(topo_covar_tile_list)>0:\n",
    "        topo_covar_fn = topo_covar_tile_list[-1]\n",
    "        print(topo_covar_fn) \n",
    "        atl08_gdf_topo = ExtractUtils.extract_value_gdf(topo_covar_fn, atl08_gdf, [\"elevation\",\"slope\",\"tsri\",\"tpi\", \"slopemask\"], reproject=True)\n",
    "    else:\n",
    "        print(\"-----> No topo covar COG for tile {}\\n\\n\".format(tile_num))\n",
    "        continue\n",
    "    \n",
    "    # Get the landsat covar COG\n",
    "    landsat_covar_tile_list = ExtractUtils.get_covar_fn_list(landsat_covar_root, tile_num)\n",
    "    \n",
    "    # Get most recent landsat covar COG, extract covars\n",
    "    if len(landsat_covar_tile_list)>0:\n",
    "        landsat_covar_fn = landsat_covar_tile_list[-1]\n",
    "        print(landsat_covar_fn)\n",
    "        atl08_gdf_topo_landsat = ExtractUtils.extract_value_gdf(landsat_covar_fn, atl08_gdf_topo, ['Blue', 'Green', 'Red', 'NIR', 'SWIR', 'NDVI', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR', 'NBR2', 'TCB', 'TCG', 'TCW', 'ValidMask', 'Xgeo', 'Ygeo'], reproject=False)\n",
    "    else:\n",
    "        print(\"-----> No landsat covar COG for tile {}\\n\\n\".format(tile_num))\n",
    "        continue\n",
    "        \n",
    "    if atl08_gdf_topo_landsat is not None:\n",
    "        # CSV the file\n",
    "        cur_time = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        out_csv_fn = os.path.join(output_dir, \"atl08_filt_\"+str(tile_num)+\"_topo_landsat_\"+cur_time+\".csv\")\n",
    "        atl08_gdf_topo_landsat.to_csv(out_csv_fn,index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(\"Wrote output csv of filtered ATL08 obs with topo and Landsat covariates for tile {}: {}\".format(tile_num, out_csv_fn) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
