{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 ICESat-2 extraction, merging, filtering, exploring, mapping\n",
    "from maap.maap import MAAP\n",
    "maap = MAAP()\n",
    "\n",
    "import ipycmc\n",
    "w = ipycmc.MapCMC()\n",
    "\n",
    "import importlib\n",
    "lib_loader = importlib.find_loader('cartopy')\n",
    "\n",
    "if lib_loader is not None:\n",
    "    REBUILD_CONDA_ENV = False\n",
    "    print(\"No need to re-build conda env.\")\n",
    "else:\n",
    "    REBUILD_CONDA_ENV = True\n",
    "    print(\"Re-build conda env...\")\n",
    "\n",
    "if REBUILD_CONDA_ENV:\n",
    "    #### This notebook uses a DPS job to run extract_atl08.py to convert h5's to csv's, then appends all csv's into a pandas geodataframe.\n",
    "    #### Returns: a pandas geodataframe that should hold the entire set of ATL08 data for this project\n",
    "    #### Notes:\n",
    "    ###### ISSUE: how to relaibly activate a conda env that can support this notebook.\n",
    "    ###### Need to 'conda activate' an env that has geopandas - but where do I do this 'activate'. How does terminal env interact with nb?\n",
    "    ###### Workaround: always do this to base:\n",
    "    ! conda install -c conda-forge geopandas -y\n",
    "    ! conda install -c conda-forge cartopy -y\n",
    "    ! conda install -c conda-forge descartes -y\n",
    "    ! conda install contextily --channel conda-forge -y\n",
    "    #! conda install -c conda-forge matplotlib_scalebar -y\n",
    "    ##https://www.essoar.org/doi/10.1002/essoar.10501423.1\n",
    "    ##https://www.essoar.org/pdfjs/10.1002/essoar.10501423.1\n",
    "    ##https://github.com/icesat2py/icepyx/blob/master/examples/ICESat-2_DEM_comparison_Colombia_working.ipynb\n",
    "    ##https://github.com/ICESAT-2HackWeek/2020_ICESat-2_Hackweek_Tutorials\n",
    "    ##https://icesat-2hackweek.github.io/learning-resources/logistics/schedule/\n",
    "    ##https://github.com/giswqs/earthengine-py-notebooks\n",
    "\n",
    "# Notes:\n",
    "# Basic filtering themes in invoke below, and remove from extract_atl08.py\n",
    "# snow has impact on errors - remove snow data (summer only)\n",
    "# msw = 0\n",
    "# beams 1 & 5 strong (better radiometric perf, sensitive), then beam 3\n",
    "\n",
    "import geopandas as gpd\n",
    "import descartes\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from geopandas import GeoDataFrame\n",
    "from geopandas.tools import sjoin\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import random \n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from cartopy.feature import NaturalEarthFeature, LAND, COASTLINE\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import datetime\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "#from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import contextily as ctx\n",
    "\n",
    "# Run extract_at08.py as a DPS job (see nb 1.3 for template of how this can be done)\n",
    "#DPS_OUTPUT_DIR = '/projects/r2d2/output_23'\n",
    "DPS_OUTPUT_DIR = '/projects/above/processed_data/2.3_output'\n",
    "#DPS_OUTPUT_DIR = '/projects/jabba/dps_output'\n",
    "#DPS_OUTPUT_DIR = '/projects/jabba/data/bbox_alaska'\n",
    "\n",
    "H_CAN_THRESH = 100\n",
    "RUN_DPS = False\n",
    "READ_PICKLE = False\n",
    "DIR_PICKLE = '/projects/above'\n",
    "\n",
    "DO_ATL08_CSV_SUBSET = True # <- set to True for testing\n",
    "SUBSET_FRAC_SIZE = 0.15\n",
    "\n",
    "#COPY_CSVS = False\n",
    "CSV_TO_DIR = \"/projects/r2d2/above/atl08_csvs\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_granules_list(granules):\n",
    "    url_list = []\n",
    "    output_list = []\n",
    "    for res in granules:\n",
    "        url_list.append(res.getDownloadUrl())\n",
    "\n",
    "    for url in url_list:\n",
    "        if url[0:5] == 's3://':\n",
    "            url = url[5:].split('/')\n",
    "            url[0] += '.s3.amazonaws.com'\n",
    "            url = 'https://' + '/'.join(url)\n",
    "        output_list.append(url)\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DPS:\n",
    "    # Get list of ATL08 granules of interest\n",
    "    BBOX_NA = \"-180,50,-50,75\"\n",
    "    BBOX_CIRC = \"-180,40,180,75\" # You'll need to edit run_above.sh to adjust the geo filtering called for with extract_atl08.py\n",
    "    YEAR = '2019'\n",
    "    COLLECTID_ATL08_V3 = \"C1200235747-NASA_MAAP\"\n",
    "\n",
    "    # Note: we want to be able to do a 'recurring' seasonal search, regardless of year\n",
    "    DATERANGE_SUMMER = YEAR+'-06-01T00:00:00Z,'+YEAR+'-09-30T23:59:59Z'\n",
    "\n",
    "    # We dont really want a limit: Not really sure how to set this; just use very high number?\n",
    "    MAX_ATL08_ORBITS = 100000\n",
    "\n",
    "    granules = maap.searchGranule(collection_concept_id=COLLECTID_ATL08_V3, \n",
    "                                  temporal=DATERANGE_SUMMER, \n",
    "                                  bounding_box=BBOX_CIRC, \n",
    "                                  limit=MAX_ATL08_ORBITS)\n",
    "    #pprint(granules[0], depth=2)\n",
    "\n",
    "    # This is a list of the granule URLs for processing\n",
    "    granules_list_ATL08 = get_granules_list(granules)\n",
    "\n",
    "    ##################################\n",
    "    #Test DPS submission on a single file\n",
    "    granule=granules_list_ATL08[0]\n",
    "\n",
    "    submit_result = maap.submitJob(identifier=\"nothing\", algo_id=\"run_above_ubuntu\", \n",
    "                                       version=\"master\", \n",
    "                                       username=\"r2d2\", \n",
    "                                       icesat2_granule=granule)\n",
    "    print(submit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction\n",
    "#\n",
    "# DPS SUBMISSION\n",
    "if RUN_DPS:\n",
    "    # Here is where I submit a job \n",
    "    # identified with 'algo_id' (in yaml file)\n",
    "    # that specifies a bash script /projects/above/gitlab_repos/atl08_extract_repo/run_above.sh \n",
    "    # that will call the 'algorithm' (extract_atl08.py)\n",
    "\n",
    "    # Uses granule list from nb 2.1\n",
    "    # CHANGE the submitJob args!\n",
    "    for g in range(len(granules_list_ATL08)):\n",
    "        granule = granules_list_ATL08[g]\n",
    "        submit_result = maap.submitJob(identifier=\"nothing\", algo_id=\"run_above_ubuntu\", \n",
    "                                   version=\"master\", \n",
    "                                   username=\"r2d2\", \n",
    "                                   icesat2_granule=granule)\n",
    "        if g == 1:\n",
    "            print(submit_result)\n",
    "        if g == 100:\n",
    "            print (submit_result)\n",
    "        if g == 1000:\n",
    "            print (submit_result)\n",
    "        if g == 2000:\n",
    "            print (submit_result)\n",
    "        if g == 3000:\n",
    "            print (submit_result)\n",
    "        if g == 4000:\n",
    "            print (submit_result)\n",
    "        if g == len(granules_list_ATL08):\n",
    "            print (submit_result)\n",
    "            print ('done!')\n",
    "        \n",
    "else:\n",
    "    print(\"Not running DPS; probably because output from extract_atl08 DPS job already exists.\")\n",
    "    print(DPS_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not READ_PICKLE:\n",
    "    # Merging\n",
    "    # Merging via glob.glob not recommended due to slow processing. Instead copy files into a now s3 bucket\n",
    "    # e.g. any directory not under user name\n",
    "    # Note if file volumes too large this will fill memory on shared instance, so better longterm solution needed.\n",
    "    # Copy files in terminal:\n",
    "    #find /projects/r2d2/dps_output/run_above_ubuntu/master/2021/03/02/04/ -name \"*.csv\" -type f | xargs -n 1 -P 4 -I{} cp -v {} /projects/above/processed_data/2.3_output\n",
    "    # Merge all CSV from extract_at08.py into a dataframe\n",
    "    #\n",
    "    # NOTE: If the ATL08 subset files that we process with extract_atl08.py are h5 files (instead of CSVs), we need to update this.\n",
    "    #\n",
    "\n",
    "    print(\"Making list of ATL08 csv files...hang in there...\")\n",
    "    # Find and delete any CSV that has a size of 0\n",
    "    #! find $DPS_OUTPUT_DIR -name \"*.csv\" -size 0 -delete\n",
    "\n",
    "    # Find all remaining output CSVs from DPS jobs\n",
    "    all_atl08_csvs = glob.glob(DPS_OUTPUT_DIR+\"/ATL08*.csv\", recursive=True)\n",
    "\n",
    "    print(\"# of ATL08 files: \",len(all_atl08_csvs))\n",
    "\n",
    "    # This could break if you randomly grab an incomplete or empty CSV\n",
    "    if DO_ATL08_CSV_SUBSET:\n",
    "        all_atl08_csvs = random.sample(all_atl08_csvs, math.floor(SUBSET_FRAC_SIZE * len(all_atl08_csvs)))\n",
    "        print(\"# of ATL08 files after test sample: \",len(all_atl08_csvs))\n",
    "\n",
    "    # Merge all files in the list\n",
    "    print(\"Creating pandas data frame...\")\n",
    "    atl08 = pd.concat([pd.read_csv(f) for f in all_atl08_csvs ], sort=False)\n",
    "\n",
    "    print(\"\\nPre-filter data cleaning...\")\n",
    "    print(\"\\nGet beam type from orbit orientation and ground track...\") \n",
    "    atl08.loc[( (atl08.orb_orient == 1 ) & (atl08['gt'].str.contains('r')) ), \"beam_type\"] = 'Strong' \n",
    "    atl08.loc[( (atl08.orb_orient == 1 ) & (atl08['gt'].str.contains('l')) ), \"beam_type\"] = 'Weak'\n",
    "    atl08.loc[( (atl08.orb_orient == 0 ) & (atl08['gt'].str.contains('r')) ), \"beam_type\"] = 'Weak'\n",
    "    atl08.loc[( (atl08.orb_orient == 0 ) & (atl08['gt'].str.contains('l')) ), \"beam_type\"] = 'Strong'\n",
    "    print(atl08.beam_type.unique())\n",
    "\n",
    "    cols_float = ['lat', 'lon', 'h_can', 'h_te_best', 'ter_slp'] \n",
    "    print(f\"Cast some columns to type float: {cols_float}\")\n",
    "    atl08[cols_float] = atl08[cols_float].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    cols_int = ['n_ca_ph', 'n_seg_ph', 'n_toc_ph']\n",
    "    print(f\"Cast some columns to type integer: {cols_int}\")\n",
    "    atl08[cols_int] = atl08[cols_int].apply(pd.to_numeric, downcast='signed', errors='coerce')\n",
    "\n",
    "    #Get rid of b strings and convert to int, then datetime\n",
    "    atl08['yr'] = atl08['yr'].str.strip(\"b\\'\\\"\").astype(int)\n",
    "    atl08['m'] = atl08['m'].str.strip(\"b\\'\\\"\").astype(int)\n",
    "    atl08['d'] = atl08['d'].str.strip(\"b\\'\\\"\").astype(int)\n",
    "    atl08[\"date\"] = pd.to_datetime(atl08[\"yr\"]*1000 + atl08[\"d\"], format = \"%Y%j\")\n",
    "    print(atl08.info())\n",
    "    \n",
    "    # Probably not necessary\n",
    "    #print('finished pickle') #<--no; there isnt any pickling here; its written aftern the Filtering chunk\n",
    "    #atl08.to_csv( \"/projects/above/processed_data/atl08_merged.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not READ_PICKLE:\n",
    "    # Filtering\n",
    "    #\n",
    "    MONTH_MIN = atl08.m.unique().min()\n",
    "    MONTH_MAX = atl08.m.unique().max()\n",
    "\n",
    "    print(\"\\nFiltering for quality: for clear skies, h_can < {}, months {}-{}, strong beams, and snow free land...\".format(H_CAN_THRESH, MONTH_MIN, MONTH_MAX))\n",
    "    atl08 =  atl08[\n",
    "                   (atl08.msw_flg == 0) & \n",
    "                   (atl08.h_can < H_CAN_THRESH) &\n",
    "                   #(atl08.m >= MONTH_MIN ) & (atl08.m <= MONTH_MAX) &\n",
    "                   #(atl08.m == int(MONTH_ATL08.to_string()) )\n",
    "                   (atl08.beam_type == 'Strong') & \n",
    "                    (atl08.seg_snow == 'snow free land')\n",
    "                    ]\n",
    "    print(f\"After filtering, there are {atl08.shape[0]} observations in this dataframe.\")\n",
    "\n",
    "    # Pickle the file\n",
    "    cur_time = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    atl08.to_pickle(os.path.join(DIR_PICKLE, \"atl08_filt_\"+cur_time+\".pkl\"))\n",
    "else:\n",
    "    print(\"Getting the latest merged, filtered, & compressed file of ATL08 obs as a pandas dataframe...\")\n",
    "    list_of_pickles = glob.glob(DIR_PICKLE+'/atl08*.pkl') # * means all if need specific format then *.csv\n",
    "    latest_pickle_file = max(list_of_pickles, key=os.path.getctime)\n",
    "    print(latest_pickle_file)\n",
    "    atl08 = pd.read_pickle(latest_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring filtered ATL08 with plots\n",
    "    \n",
    "print(\"# of ATL08 columns: \", len(atl08.columns))\n",
    "print(f\"There are {atl08.shape[0]} observations in this dataframe.\")\n",
    "#print(\"Col names: \", atl08.columns)\n",
    "print(\"Max lat:\",    atl08.lat.max())\n",
    "print(\"Min lat:\",    atl08.lat.min())\n",
    "print(\"Max lon:\",    atl08.lon.max())\n",
    "print(\"Min lon:\",    atl08.lon.min())\n",
    "print(\"Years: \",     atl08.yr.unique())\n",
    "print(\"Months: \",    atl08.m.unique())\n",
    "\n",
    "# Plot obs from night and day\n",
    "# My cmap\n",
    "forest_ht_cmap = LinearSegmentedColormap.from_list('forest_ht', ['#636363','#fc8d59','#fee08b','#ffffbf','#d9ef8b','#91cf60','#1a9850','#005a32'], 12)\n",
    "\n",
    "print(\"\\nExample 1: Get only clear sky data, by year, with ht threshold.\")\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=atl08.night_flg.nunique(), figsize=(20, 4), sharex=False, sharey=True)\n",
    "#ax0, ax1, ax2, ax3 = axes.flatten()\n",
    "\n",
    "SAMP_FRAC = 1 # Map just a portion of the data for speed\n",
    "\n",
    "i=0\n",
    "for group_name, group in atl08.groupby('night_flg'):\n",
    "    print(f\"\\nThere are {len(group)} observations in {group_name}.\")\n",
    "    #print(f\"Filtering for clear skies and h_can < 60\")\n",
    "    #group_tmp = group[ \n",
    "    #                  (group.seg_snow == 'snow free land') \n",
    "    #                 ].sample(frac=SAMP_FRAC)\n",
    "\n",
    "    #prop_of_df = round(len(group_tmp) / len(atl08), 3)\n",
    "\n",
    "    #print(f\"Proportion of {group_name} total satifying this filter: {prop_of_df}\")\n",
    "    #divider = make_axes_locatable(axes[i])\n",
    "    #cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    scat = axes[i].scatter(x=group.lon, y=group.lat, c=group['h_can'], cmap = forest_ht_cmap, vmin=0, vmax=25, s=0.05)\n",
    "    #subplt = group.plot(ax=axes[i], x='lon', y='lat', c='h_can', cmap = forest_ht_cmap, vmin=0, vmax=25, kind='scatter', s=0.05) # \n",
    "    axes[i].set_title(str(len(group))+\" ATL08 obs. during \"+group_name+\"\\nstrong beam, clear skies, snow-free land, h_can<\"+str(H_CAN_THRESH)+\", \"+group_name\n",
    "                      #+\"\\n\"+str(round(prop_of_df*100,1)) +\"% of data\"\n",
    "                      , loc='left')\n",
    "    #cbar = plt.colorbar(scat, extend='max', spacing='proportional', orientation='vertical', shrink=0.7, format=\"%.0f\")\n",
    "    #cbar.set_label(label = 'Canopy height (m)', size=16)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple histogram plotting is **ridiculously** hard to get correct.\n",
    "#\n",
    "# here, i try to plot a timeline of the count of ATL08, by day and month, for each year.\n",
    "f, ax = plt.subplots(1,1,figsize=(12,3))\n",
    "#atl08['fid'].groupby([atl08[\"date\"].dt.year, atl08[\"date\"].dt.month, atl08[\"date\"].dt.day]).count().plot(ax=ax, kind='bar')\n",
    "\n",
    "df = atl08.sample(frac=1)\n",
    "print(len(df))\n",
    "for i, seg_name in enumerate(df.yr.unique()):\n",
    "    #ax.hist(atl08[atl08.yr == seg_name].groupby([atl08[\"date\"].dt.month, atl08[\"date\"].dt.day]), bins=bins, alpha=0.5, label=seg_name)\n",
    "    ax.hist(df[df.yr == seg_name].m, bins=12, alpha=0.5, label=seg_name)\n",
    "    ax.set_title(f\"Timeline (month) of number of obs.\", loc='left')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(1, 12)\n",
    "f, ax = plt.subplots(1,1,figsize=(12,3)) \n",
    "for i, seg_name in enumerate(df.yr.unique()):\n",
    "    #ax.hist(atl08[atl08.yr == seg_name].groupby([atl08[\"date\"].dt.month, atl08[\"date\"].dt.day]), bins=bins, alpha=0.5, label=seg_name)\n",
    "    ax.hist(df[df.yr == seg_name].lon, bins=360, alpha=0.5, label=seg_name)\n",
    "    ax.set_title(f\"Spatial interval (longitude) of number of obs.\", loc='left')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-180, 180)\n",
    "\n",
    "if False:\n",
    "    # Good luck with getting this plot...\n",
    "    f, ax = plt.subplots(2,1,figsize=(12,3), sharex=True, sharey=True)\n",
    "    i=0\n",
    "    for group_name, group in atl08.groupby('yr'):\n",
    "        group['m'].groupby([group[\"date\"].dt.month, group[\"date\"].dt.day]).count().plot(ax=ax[i], kind='bar')\n",
    "        ax[i].set_title(group_name, \n",
    "                        loc='left')\n",
    "        i+=1\n",
    "#plt.xlabel('Date', fontsize=12)\n",
    "#plt.xticks(fontsize=7, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same bins for each\n",
    "xmin = atl08.n_toc_ph.min()\n",
    "xmax = atl08.n_toc_ph.max()\n",
    "bins = np.linspace(xmin, xmax, 300)\n",
    "\n",
    "# Set up correct number of subplots, space them out. \n",
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "\n",
    "for i, seg_name in enumerate(atl08.night_flg.unique()):\n",
    "    ax.hist(atl08[atl08.night_flg == seg_name].n_toc_ph, bins=bins, alpha=0.5, label=seg_name)\n",
    "    ax.set_title(\"Distribution of number of top-of-canopy photons per obs.\", loc='left')\n",
    "    ax.legend()\n",
    "\n",
    "# same xlim for each so we can see differences\n",
    "ax.set_xlim(0, 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE GEOPANDAS DATAFRAME WITH SPATIAL REF and sample a fractions of it.\n",
    "#\n",
    "SAMP_FRAC=1\n",
    "atl08_gdf = GeoDataFrame(atl08, geometry=gpd.points_from_xy(atl08.lon, atl08.lat), crs='epsg:4326').sample(frac=SAMP_FRAC)\n",
    "print(f\"There are {atl08_gdf.shape[0]} ATL08 observations in this sampled ({100*SAMP_FRAC}%) Geodataframe.\")\n",
    "print(len(atl08_gdf.columns))\n",
    "xmin, ymin, xmax, ymax = atl08_gdf.total_bounds\n",
    "\n",
    "# Subset to save for testing\n",
    "bbox_sub = [-102.7684,43.17366,-97,32037,47.48166]\n",
    "atl08_gdf_sub = atl08_gdf.cx[bbox_sub[0]:bbox_sub[2], bbox_sub[1]:bbox_sub[3]]\n",
    "atl08_gdf_sub.to_csv(os.path.split(CSV_TO_DIR)[0]+\"/atl08_gdf_sub.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a boreal albers projection and map the geodataframe\n",
    "boreal_alb = \"+proj=stere +lat_0=90 +lat_ts=71 +lon_0=0 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n",
    "#boreal_alb = \"+proj=aea +lat_1=50 +lat_2=70 +lat_0=60 +lon_0=-170 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\"\n",
    "na_alb =     \"+proj=aea +lat_1=50 +lat_2=70 +lat_0=60 +lon_0=-110 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\"\n",
    "\n",
    "proj_alb = na_alb\n",
    "if atl08_gdf.lon.min() < 0:\n",
    "    proj_alb = boreal_alb\n",
    "\n",
    "# Clip world to ATL08 gdf\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world_atl08 = world.cx[xmin:xmax, ymin:ymax]\n",
    "atl08_gdf_chull = atl08_gdf.unary_union.convex_hull\n",
    "\n",
    "world_atl08 = world[world.intersects(atl08_gdf_chull)]\n",
    "#NA = world[world['continent'] == 'North America'].to_crs(boreal_alb)\n",
    "\n",
    "atl08_gdf_aea = atl08_gdf.to_crs(proj_alb)\n",
    "world_atl08_aea = world_atl08.to_crs(proj_alb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring ATL08 with hexbins\n",
    "# All years\n",
    "ax_map_title = \"ICESat-2 bin median canopy height \"\n",
    "cbar_map_title = 'Canopy height [m] (median, ATL08:h_can)'\n",
    "\n",
    "d = datetime.date.today().strftime(\"%Y%b%d\")\n",
    "# Set up correct number of subplots, space them out. \n",
    "fig, ax = plt.subplots(1,1, figsize=(14,10), sharex=True, sharey=True)\n",
    "\n",
    "bbox = atl08_gdf_aea.total_bounds\n",
    "\n",
    "group_tmp = atl08_gdf_aea[atl08_gdf_aea.seg_snow == 'snow free land'] \n",
    "\n",
    "world_atl08_aea.plot(ax=ax, facecolor='grey', edgecolor='black',  alpha=0.5)\n",
    "hb = ax.hexbin(group_tmp.geometry.x, group_tmp.geometry.y, C=group_tmp['h_can'], \n",
    "                   reduce_C_function=np.median, gridsize=250, cmap=forest_ht_cmap, vmax=25, mincnt=1, alpha=0.7)\n",
    "world_atl08_aea.plot(ax=ax, facecolor='None', edgecolor='black',  alpha=0.9)\n",
    "\n",
    "cbar = plt.colorbar(hb, extend='max', spacing='proportional', orientation='vertical', shrink=0.7, format=\"%.0f\")\n",
    "cbar.set_label(label = cbar_map_title, size=16)\n",
    "   \n",
    "ax.set_xlim(bbox[[0,2]])\n",
    "ax.set_ylim(bbox[[1,3]])\n",
    "ax.set_title(ax_map_title+\"2019-2020\", size=20, loc='left')\n",
    "ax.grid()\n",
    "fig_fn = os.path.join(DPS_OUTPUT_DIR, 'atl08_alb_h_can_2019_2020_'+d+'.png')\n",
    "print(fig_fn)\n",
    "#plt.savefig(fig_fn)\n",
    "\n",
    "#ax.set(aspect='equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctx.providers.keys())\n",
    "print(ctx.providers.Stamen.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring ATL08 with hexbins\n",
    "\n",
    "def add_basemap(ax, crs, zoom='auto'):\n",
    "    ctx.add_basemap(ax=ax, crs=crs, source=ctx.providers.Stamen.TerrainBackground, zoom=zoom) # ctx.sources.ST_TERRAIN\n",
    "    #Create a scalebar object, with scaling factor of 1.0 px, since we're using projected coordinate system with unit 1 m\n",
    "    #scalebar = ScaleBar(1.0)\n",
    "    #Add scalebar to axes\n",
    "    #ax.add_artist(scalebar)\n",
    "\n",
    "bbox = atl08_gdf_aea.total_bounds\n",
    "\n",
    "# Set up correct number of subplots, space them out. \n",
    "fig, ax = plt.subplots(ncols=atl08_gdf_aea.yr.nunique(), figsize=(30,10), sharex=True, sharey=True)\n",
    "\n",
    "print(f\"Filtering by year for clear skies\")\n",
    "for i, (lab, group) in enumerate(atl08_gdf_aea.groupby('yr')):\n",
    "    \n",
    "    world_atl08_aea.plot(ax=ax[i], facecolor='grey', edgecolor='black',  alpha=0.5)\n",
    "    \n",
    "    group_tmp = group[group.msw_flg == 0]\n",
    "    \n",
    "    hb = ax[i].hexbin(group_tmp.geometry.x, group_tmp.geometry.y, C=group_tmp['h_can'], reduce_C_function=np.median, gridsize=60, cmap=forest_ht_cmap, vmax=30, mincnt=1, alpha=0.7)\n",
    "    \n",
    "    #world_atl08_aea.plot(ax=ax[i], facecolor='None', edgecolor='black',  alpha=0.9)\n",
    "    \n",
    "    ax[i].set_xlim(bbox[[0,2]])\n",
    "    ax[i].set_ylim(bbox[[1,3]])\n",
    "    #ax[i].set_title(ax_map_title+lab, size=20, loc='left')\n",
    "    #ax[i].grid()\n",
    "    ax[i].set(aspect='equal')\n",
    "    add_basemap(ax[i], crs=na_alb)\n",
    "    \n",
    "#fig.subplots_adjust(right=0.8)\n",
    "#cbar_ax = fig.add_axes([0.85, 0.15, 0.01, 0.5])\n",
    "#cbar = fig.colorbar(hb, cax=cbar_ax)\n",
    "#fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "cbar = plt.colorbar(hb, ax=ax.ravel().tolist(), extend='max', spacing='proportional', orientation='vertical', shrink=1, format=\"%.0f\")\n",
    "cbar.set_label(label = cbar_map_title, size=14)\n",
    "#plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a DEM for study domain\n",
    "import os\n",
    "import requests\n",
    "import rasterio as rio\n",
    "from rasterio import plot\n",
    "\n",
    "#bbox_ll = [-150, 55, -115, 70]\n",
    "#demtype = \"SRTMGL3\"\n",
    "#url=\"https://portal.opentopography.org/API/globaldem?demtype={}&west={}&south={}&east={}&north={}&outputFormat=GTiff\".format(demtype,*bbox_ll)\n",
    "#response = requests.get(url)\n",
    "#src = rio.open(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
