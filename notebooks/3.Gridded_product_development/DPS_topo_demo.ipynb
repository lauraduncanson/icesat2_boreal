{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfae12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host='api.ops.maap-project.org')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a350d",
   "metadata": {},
   "source": [
    "# Launch DPS for 3.1.5_dps.py  \n",
    "create a topo stack from the Copernicus DEM for each input vector tile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27907afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/.local/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import os, glob\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "!pip install xmltodict\n",
    "import xmltodict\n",
    "import collections\n",
    "import sys\n",
    "sys.path.append('/projects/Developer/icesat2_boreal/lib')\n",
    "import ExtractUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f46dd7",
   "metadata": {},
   "source": [
    "### Get all tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6188a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all boreal tiles\n",
    "boreal_tile_index_path = '/projects/shared-buckets/nathanmthomas/boreal_tiles_v003.gpkg' #shared-buckets/nathanmthomas/boreal_grid_albers90k_gpkg.gpkg\n",
    "boreal_tile_index = gpd.read_file(boreal_tile_index_path)\n",
    "\n",
    "bad_tiles = [3540,3634,3728,3823,3916,4004] #Dropping the tiles near antimeridian that reproject poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e716e290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5337"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_TILE_NUM_LIST = boreal_tile_index.tile_num.to_list()\n",
    "len(INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eaaf9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DPS on the FULL list of input\n",
      "List length: 1\n"
     ]
    }
   ],
   "source": [
    "TEST_DPS  = False\n",
    "\n",
    "if TEST_DPS:\n",
    "    print('Running DPS on a SUBSET list of input')\n",
    "    DPS_INPUT_TILE_NUM_LIST = INPUT_TILE_NUM_LIST[0:100] #INPUT_TILE_NUM_LIST[100:]\n",
    "else:\n",
    "    print('Running DPS on the FULL list of input')\n",
    "    DPS_INPUT_TILE_NUM_LIST = INPUT_TILE_NUM_LIST\n",
    "    \n",
    "print(f\"List length: {len(DPS_INPUT_TILE_NUM_LIST)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67cf05",
   "metadata": {},
   "source": [
    "# Run a DPS of topo tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5609d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(f\"# of input tiles for DPS: {len_input_list}\")\n",
    "\n",
    "for i, INPUT_TILE_NUM in enumerate(DPS_INPUT_TILE_NUM_LIST):\n",
    "\n",
    "    DPS_num = i+1\n",
    "    IDENTIFIER = 'do_topo_stack_3-1-5'\n",
    "    ALGO_ID = f'{IDENTIFIER}_ubuntu'\n",
    "    USER = 'nathanmthomas'\n",
    "    WORKER_TYPE = 'maap-dps-worker-8gb'\n",
    "    \n",
    "    in_param_dict = {\n",
    "                         'in_tile_fn': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "                         'in_tile_num': INPUT_TILE_NUM,\n",
    "                         'tile_buffer_m': 120,\n",
    "                         'in_tile_layer': 'boreal_tiles_v003',\n",
    "                         'topo_tile_fn': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/dem30m_tiles.geojson',\n",
    "        }\n",
    "\n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version='master',\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_param_dict\n",
    "        )\n",
    "    \n",
    "    # Build a dataframe of submission details\n",
    "    submit_result['dps_num'] = DPS_num\n",
    "    submit_result['tile_num'] = INPUT_TILE_NUM\n",
    "    submit_result['submit_time'] = datetime.datetime.now()\n",
    "    submit_result['dbs_job_hour'] =datetime.datetime.now().hour\n",
    "    submit_result['algo_id'] = ALGO_ID\n",
    "    submit_result['user'] = USER\n",
    "    submit_result['worker_type'] = WORKER_TYPE\n",
    "    \n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(pd.DataFrame([submit_result]))\n",
    "    \n",
    "    if DPS_num in [1, 100, 500, 1000, 1500, 2000, 3000, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000, len_input_list]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| tile num: {INPUT_TILE_NUM}\\t| submit status: {submit_result['status']}\\t| job id: {submit_result['job_id']}\") \n",
    "        \n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "print(f\"Current time:\\t{nowtime}\")\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c803de",
   "metadata": {},
   "source": [
    "After almost any DPS job, you have to assess what succeeded and failed. This involves:\n",
    "1. building a table of job status based on job ids captured in the job_results_df from the DPS run chunk --> this tells you how many jobs failed\n",
    "2. merging the job status table with the job results df --> this tells you which specific granules (or tile nums) failed\n",
    "3. building another input list of granules for a follow-up DPS\n",
    "## Assess DPS results\n",
    "Build a table of job status based on job id - how many jobs failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e35805",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LIST_SUBMISSIONS = sorted(glob.glob(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_*.csv'),key=ExtractUtils.func, reverse=True)\n",
    "for DPS_DATETIME in ['2022042119']:\n",
    "    for fn in LIST_SUBMISSIONS:\n",
    "        if DPS_DATETIME in fn and not 'job_status' in fn:\n",
    "            DPS_alg_id = os.path.basename(fn.split('_submission_results_')[0].replace('DPS_',''))\n",
    "            thentime = fn.split('_')[-1].replace('.csv','')\n",
    "            print(f'DPS alg:\\t\\t{DPS_alg_id}')\n",
    "            print(f'DPS launch time:\\t{thentime}')\n",
    "            z = ExtractUtils.BUILD_TABLE_JOBSTATUS(pd.read_csv(fn))\n",
    "            # Save job status table\n",
    "            z.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_job_status_{len(z)}_{thentime}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
