{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c286c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host='api.ops.maap-project.org')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fffff75",
   "metadata": {},
   "source": [
    "# Launch DPS for build_stack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09891176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "!pip install xmltodict\n",
    "import xmltodict\n",
    "import sys\n",
    "sys.path.append('/projects/Developer/icesat2_boreal/lib')\n",
    "import ExtractUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140a23d",
   "metadata": {},
   "source": [
    "##### To run build_stack.py across a tiled raster dataset you need a dictionary with args\n",
    "\n",
    "s3 you need to have a vector footprint of that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2eeebacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_NUM = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b23f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESA Worldcover 2020\n",
    "BUILD_STACK_DICT = {\n",
    "            #'INDEX_FN': '/projects/my-public-bucket/boreal_tiles_v003.gpkg',\n",
    "            'INDEX_FN': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "            'ID_COL_NAME': 'tile_num',\n",
    "            'TILE_NUM':TILE_NUM,\n",
    "            'INDEX_LYR': 'boreal_tiles_v003',\n",
    "            # Worldcover data is accessed via its footprint, with a 's3_path' col identifying the s3 locations of each tile\n",
    "            'RASTER_NAME': 'esa_worldcover_v100_2020',\n",
    "            #'COVAR_TILE_FN': '/projects/my-public-bucket/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
    "            'COVAR_TILE_FN': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
    "            'IN_COVAR_S3_COL': 's3_path',\n",
    "            'OUTDIR': '/projects/my-public-bucket/DPS_ESA_LC',\n",
    "            'NODATA_VAL': 0,\n",
    "            'OUTPUT_CLIP_COG_FN':'',\n",
    "            'CREDENTIALS_FN': None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54ad28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEX clip shapes should be in equal area\n",
    "# COVAR_TILE_FN tiles (of raster COGs) should be in 4326\n",
    "args = f\"\\\n",
    "    --in_tile_fn {BUILD_STACK_DICT['INDEX_FN']} \\\n",
    "    --in_tile_id_col {BUILD_STACK_DICT['ID_COL_NAME']} \\\n",
    "    --in_tile_num {BUILD_STACK_DICT['TILE_NUM']} \\\n",
    "    --tile_buffer_m 0 \\\n",
    "    --in_tile_layer {BUILD_STACK_DICT['INDEX_LYR']} \\\n",
    "    -o {BUILD_STACK_DICT['OUTDIR']} \\\n",
    "    --topo_off \\\n",
    "    --covar_src_name {BUILD_STACK_DICT['RASTER_NAME']} \\\n",
    "    --covar_tile_fn {BUILD_STACK_DICT['COVAR_TILE_FN']} \\\n",
    "    --in_covar_s3_col {BUILD_STACK_DICT['IN_COVAR_S3_COL']} \\\n",
    "    --input_nodata_value {BUILD_STACK_DICT['NODATA_VAL']} \\\n",
    "    --clip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4d711443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n",
      "\n",
      "---Running build_stack()---\n",
      "\n",
      "The covariate's filename(s) intersecting the 0 m bbox for tile id 2000:\n",
      " s3://esa-worldcover/v100/2020/map/ESA_WorldCover_10m_2020_v100_N51W093_Map.tif\n",
      "s3://esa-worldcover/v100/2020/map/ESA_WorldCover_10m_2020_v100_N51W096_Map.tif\n",
      "bbox: [4238521.999999994, 4653304.000000009, 4328521.999999994, 4743304.000000009]\n",
      "Writing stack as cloud-optimized geotiff: /projects/my-public-bucket/test_build_stack/esa_worldcover_v100_2020_2000_cog.tif\n",
      "Orig stack shape:\t\t (1, 3000, 3000)\n",
      "Output resolution:\t\t (30, 30)\n",
      "Writing img to memory...\n",
      "/projects/Developer/icesat2_boreal/lib/build_stack.py:93: RasterioDeprecationWarning: Source dataset should be opened in read-only mode. Use of datasets opened in modes other than 'r' will be disallowed in a future version.\n",
      "  input_nodata_value=input_nodata_value\n",
      "{'driver': 'VRT', 'dtype': 'uint8', 'nodata': 0, 'width': 3000, 'height': 3000, 'count': 1, 'crs': CRS.from_wkt('PROJCS[\"unnamed\",GEOGCS[\"GRS 1980(IUGG, 1980)\",DATUM[\"unknown\",SPHEROID[\"GRS80\",6378137,298.257222101],TOWGS84[0,0,0,0,0,0,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"standard_parallel_1\",50],PARAMETER[\"standard_parallel_2\",70],PARAMETER[\"latitude_of_center\",40],PARAMETER[\"longitude_of_center\",180],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(30.0, 0.0, 4238521.999999994,\n",
      "       0.0, -30.0, 4743304.000000009), 'blockxsize': 512, 'blockysize': 128, 'tiled': True}\n",
      "Reading input: <open WarpedVRT name='WarpedVRT(/vsimem/1646c740-8187-4433-9424-7021f9975de4/1646c740-8187-4433-9424-7021f9975de4.tif)' mode='r'>\n",
      "Adding overviews...\n",
      "Updating dataset tags...\n",
      "Writing output to: /projects/my-public-bucket/test_build_stack/esa_worldcover_v100_2020_2000_cog.tif\n",
      "Image written to disk:\t\t /projects/my-public-bucket/test_build_stack/esa_worldcover_v100_2020_2000_cog.tif\n"
     ]
    }
   ],
   "source": [
    "!python /projects/Developer/icesat2_boreal/lib/build_stack.py $args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54a3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec9827e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_params_dict = BUILD_STACK_DICT\n",
    "DPS_INPUT_TILE_NUM_LIST = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81133504",
   "metadata": {},
   "source": [
    "# NOTE: \n",
    "We need to register a DPS algorithm called 'run_build_stack' before proceeding to the chunks below..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d09dc2",
   "metadata": {},
   "source": [
    "## Run a DPS job across the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8299a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = 'build_stack_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "750447ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4441  4476 38433 ...  3848   275     7]\n",
      "['s3://maap-ops-workspace/nathanmthomas/dps_output/do_HLS_stack_3-1-2_ubuntu/master/2022/spring2022/04/13/04/15/00/257106/HLS_4441_06-01_09-15_2019_2021.tif'\n",
      " 's3://maap-ops-workspace/nathanmthomas/dps_output/do_HLS_stack_3-1-2_ubuntu/master/2022/spring2022/04/13/03/51/26/317765/HLS_4476_06-01_09-15_2019_2021.tif'\n",
      " 's3://maap-ops-workspace/nathanmthomas/dps_output/do_HLS_stack_3-1-2_ubuntu/master/2022/spring2022/04/13/03/49/47/028016/HLS_38433_06-01_09-15_2019_2021.tif'\n",
      " ...\n",
      " 's3://maap-ops-workspace/nathanmthomas/dps_output/do_HLS_stack_3-1-2_ubuntu/master/2022/spring2022/03/24/20/44/50/862412/HLS_3848_06-01_09-15_2019_2021.tif'\n",
      " 's3://maap-ops-workspace/nathanmthomas/dps_output/do_HLS_stack_3-1-2_ubuntu/master/2022/spring2022/03/24/20/43/54/290821/HLS_275_06-01_09-15_2019_2021.tif'\n",
      " 's3://maap-ops-workspace/nathanmthomas/dps_output/do_HLS_stack_3-1-2_ubuntu/master/2022/spring2022/03/24/20/38/59/840829/HLS_7_06-01_09-15_2019_2021.tif']\n"
     ]
    }
   ],
   "source": [
    "HLS_tindex_master_fn = 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/HLS_test_redo/spring2022/HLS_tindex_master.csv'\n",
    "HLS_index = pd.read_csv(HLS_tindex_master_fn)\n",
    "\n",
    "DPS_INPUT_TILE_NUM_LIST = HLS_index['tile_num'].values\n",
    "DPS_INPUT_TILE_NUM_LIST_S3 = HLS_index['s3_path'].values\n",
    "print(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(DPS_INPUT_TILE_NUM_LIST_S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(f\"# of input tiles for DPS: {len_input_list}\")\n",
    "\n",
    "for i, INPUT_TILE_NUM in enumerate(DPS_INPUT_TILE_NUM_LIST):\n",
    "\n",
    "    DPS_num = i+1\n",
    "    IDENTIFIER = 'run_build_stack'\n",
    "    ALGO_ID = f'{IDENTIFIER}_ubuntu'\n",
    "    USER = 'nathanmthomas'\n",
    "    WORKER_TYPE = 'maap-dps-worker-8gb'\n",
    "    \n",
    "    in_params_dict['in_tile_num'] = INPUT_TILE_NUM\n",
    "    \n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version='master',\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_params_dict\n",
    "        )\n",
    "    \n",
    "    # Build a dataframe of submission details\n",
    "    submit_result['dps_num'] = DPS_num\n",
    "    submit_result['tile_num'] = INPUT_TILE_NUM\n",
    "    submit_result['submit_time'] = datetime.datetime.now()\n",
    "    submit_result['dbs_job_hour'] =datetime.datetime.now().hour\n",
    "    submit_result['algo_id'] = ALGO_ID\n",
    "    submit_result['user'] = USER\n",
    "    submit_result['worker_type'] = WORKER_TYPE\n",
    "\n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(pd.DataFrame([submit_result]))\n",
    "    \n",
    "    if DPS_num in [1, 25,50, 100,200,300,400,500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4250, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000, len_input_list]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| tile num: {INPUT_TILE_NUM}\\t| submit status: {submit_result['status']}\\t| job id: {submit_result['job_id']}\") \n",
    "        \n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "submit_results_df['run_name'] = RUN_NAME\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "print(f\"Current time:\\t{nowtime}\")\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c39ef",
   "metadata": {},
   "source": [
    "After almost any DPS job, you have to assess what succeeded and failed. This involves:\n",
    "1. building a table of job status based on job ids captured in the job_results_df from the DPS run chunk (this takes 40 mins for ~47k jobs) --> this tells you how many jobs failed\n",
    "2. merging the job status table with the job results df --> this tells you which specific granules (or tile nums) failed\n",
    "3. building another input list of granules for a follow-up DPS\n",
    "## Assess DPS results\n",
    "Build a table of job status based on job id - how many jobs failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90451ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LIST_SUBMISSIONS = sorted(glob.glob(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_*.csv'),key=ExtractUtils.func, reverse=True)\n",
    "for DPS_DATETIME in [nowtime]:\n",
    "    for fn in LIST_SUBMISSIONS:\n",
    "        if DPS_DATETIME in fn and not 'job_status' in fn:\n",
    "            DPS_alg_id = os.path.basename(fn.split('_submission_results_')[0].replace('DPS_',''))\n",
    "            thentime = fn.split('_')[-1].replace('.csv','')\n",
    "            print(f'DPS alg:\\t\\t{DPS_alg_id}')\n",
    "            print(f'DPS launch time:\\t{thentime}')\n",
    "            z = ExtractUtils.BUILD_TABLE_JOBSTATUS(pd.read_csv(fn))\n",
    "            # Save job status table\n",
    "            z.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_job_status_{len(z)}_{thentime}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
