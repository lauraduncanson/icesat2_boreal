{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7bcf9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from maap.maap import MAAP\n",
    "#maap = MAAP(maap_host='api.ops.maap-project.org')\n",
    "maap = MAAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "be655aaf-644c-4041-8d04-e1237a50a7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'api.maap-project.org'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maap._MAAP_HOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c8ff57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U -r /projects/Developer/icesat2_boreal/dps/requirements_main.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e3ace",
   "metadata": {},
   "source": [
    "# Launch DPS for build_stack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7041b2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For some reason this is needed to get s3fs to work in ExtractUtils\n",
    "# this upgrades to 0.3.4 even though we already specify this version in requirements_main...\n",
    "#!pip install s3fs --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c541eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.10/site-packages (0.13.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "!pip install xmltodict\n",
    "import xmltodict\n",
    "import sys\n",
    "sys.path.append('/projects/code/icesat2_boreal/lib')\n",
    "import ExtractUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b52264b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023.12.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "s3fs.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e42d",
   "metadata": {},
   "source": [
    "# Test (locally) the script for DPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4283ab1",
   "metadata": {},
   "source": [
    "##### To run build_stack.py across a tiled raster dataset you need a bunch of args that we'll gather into a dictionary\n",
    "\n",
    "s3 you need to have a vector footprint of that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1147899c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TILE_NUM = 1274"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df593ad",
   "metadata": {},
   "source": [
    "#### Dictionary preparation makes this script very flexible and transferable to another s3 dataset\n",
    "This dictionary is specific to the ESA Worldcover dataset.  \n",
    "To run '`build_stack.py` across another dataset, just prepare another dictionary here and everything below should be exactly the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f57e6230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ESA Worldcover 2020\n",
    "BUILD_STACK_DICT = {\n",
    "            #'INDEX_FN': '/projects/my-public-bucket/boreal_tiles_v003.gpkg',\n",
    "            'INDEX_FN': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "            'ID_COL_NAME': 'tile_num',\n",
    "            'TILE_NUM':TILE_NUM,\n",
    "            'INDEX_LYR': 'boreal_tiles_v003',\n",
    "            # Worldcover data is accessed via its footprint, with a 's3_path' col identifying the s3 locations of each tile\n",
    "            'RASTER_NAME': 'esa_worldcover_v100_2020',\n",
    "            #'COVAR_TILE_FN': '/projects/my-public-bucket/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
    "            'COVAR_TILE_FN': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
    "            'IN_COVAR_S3_COL': 's3_path',\n",
    "            'OUTDIR': '/projects/my-public-bucket/DPS_ESA_LC',\n",
    "            'NODATA_VAL': 0,\n",
    "            'OUTPUT_CLIP_COG_FN':'',\n",
    "            'CREDENTIALS_FN': None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ae6e066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INDEX clip shapes should be in equal area\n",
    "# COVAR_TILE_FN tiles (of raster COGs) should be in 4326\n",
    "args = f\"\\\n",
    "    --in_tile_fn {BUILD_STACK_DICT['INDEX_FN']} \\\n",
    "    --in_tile_id_col {BUILD_STACK_DICT['ID_COL_NAME']} \\\n",
    "    --in_tile_num {BUILD_STACK_DICT['TILE_NUM']} \\\n",
    "    --tile_buffer_m 0 \\\n",
    "    --in_tile_layer {BUILD_STACK_DICT['INDEX_LYR']} \\\n",
    "    -o {BUILD_STACK_DICT['OUTDIR']} \\\n",
    "    --topo_off \\\n",
    "    --covar_src_name {BUILD_STACK_DICT['RASTER_NAME']} \\\n",
    "    --covar_tile_fn {BUILD_STACK_DICT['COVAR_TILE_FN']} \\\n",
    "    --in_covar_s3_col {BUILD_STACK_DICT['IN_COVAR_S3_COL']} \\\n",
    "    --input_nodata_value {BUILD_STACK_DICT['NODATA_VAL']} \\\n",
    "    --shape 3000 \\\n",
    "    --clip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a27d919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /projects/code/icesat2_boreal/lib/build_stack.py --in_tile_fn https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg --in_tile_id_col tile_num --in_tile_num 1274 --tile_buffer_m 0 --in_tile_layer boreal_tiles_v003 -o /projects/my-public-bucket/DPS_ESA_LC --topo_off --covar_src_name esa_worldcover_v100_2020 --covar_tile_fn https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg --in_covar_s3_col s3_path --input_nodata_value 0 --shape 3000 --clip\n"
     ]
    }
   ],
   "source": [
    "!echo python /projects/code/icesat2_boreal/lib/build_stack.py $args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b16d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DPS_INPUT_TILE_NUM_LIST = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a058f23-c0a1-4445-9656-70eb7489441b",
   "metadata": {},
   "source": [
    "### Use MAAP Registration call in notebook chunk to register DPS algorithm\n",
    " - We need to register a DPS algorithm called `run_build_stack_LC` before proceeding to the chunks below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7810c9e6-5dc8-4969-b1f4-beb3d06e9d96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"code\": 200, \"message\": {\"id\": \"0a25ec97c13cba1eb525b665d6cf32e48ce8c5fa\", \"short_id\": \"0a25ec97\", \"created_at\": \"2023-12-07T02:01:00.000+00:00\", \"parent_ids\": [\"48378de9a13c91ef1fe922456e6711ddd837ee01\"], \"title\": \"Registering algorithm: run_build_stack\", \"message\": \"Registering algorithm: run_build_stack\", \"author_name\": \"root\", \"author_email\": \"root@e49d5cea4b76\", \"authored_date\": \"2023-12-07T02:01:00.000+00:00\", \"committer_name\": \"root\", \"committer_email\": \"root@e49d5cea4b76\", \"committed_date\": \"2023-12-07T02:01:00.000+00:00\", \"trailers\": {}, \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/commit/0a25ec97c13cba1eb525b665d6cf32e48ce8c5fa\", \"stats\": {\"additions\": 1, \"deletions\": 1, \"total\": 2}, \"status\": \"pending\", \"project_id\": 3, \"last_pipeline\": {\"id\": 10345, \"iid\": 616, \"project_id\": 3, \"sha\": \"0a25ec97c13cba1eb525b665d6cf32e48ce8c5fa\", \"ref\": \"main\", \"status\": \"pending\", \"source\": \"push\", \"created_at\": \"2023-12-07T02:01:02.128Z\", \"updated_at\": \"2023-12-07T02:01:02.701Z\", \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/pipelines/10345\"}, \"job_web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/10597\", \"job_log_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/10597/raw\"}}\\n'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maap.register_algorithm_from_yaml_file(\"/projects/code/icesat2_boreal/dps/registered/run_build_stack.yml\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f00f1e42-d230-421b-8f50-1c4e767ac486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal error: An error occurred (404) when calling the HeadObject operation: Key \"montesano/dps_output/run_build_stack/LC_ESA_WC_2020\" does not exist\n"
     ]
    }
   ],
   "source": [
    "#!aws s3 rm --recursive s3://maap-ops-workspace/montesano/dps_output/run_build_stack/LC_ESA_WC_2020\n",
    "!aws s3 mv s3://maap-ops-workspace/montesano/dps_output/run_build_stack/LC_ESA_WC_2020 s3://maap-ops-workspace/montesano/dps_output/run_build_stack/build_stack_v2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836409b4",
   "metadata": {},
   "source": [
    "# Build a DPS list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d40e2d35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5241\n",
      "5241 tiles in s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/HLS/c2020/HLS_stack_2023_v1/HLS_tindex_master.csv\n",
      "HLS_tindex_master.csv has the most tiles. Using this for DPS tiles list.\n",
      "5220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5241"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HLS_tindex_master_fn = \"s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/HLS/fall2022/HLS_stack_2022_v2/HLS_tindex_master.csv\".replace('s3://maap-ops-workspace/shared/nathanmthomas', '/projects/my-public-bucket')\n",
    "Topo_tindex_master_fn = \"s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv\" #.replace('s3://maap-ops-workspace/shared/nathanmthomas', '/projects/my-public-bucket')\n",
    "\n",
    "# Spring 2022 c2020 composite - use this as template set of tiles (5241 tiles)\n",
    "HLS_EXISTING_TINDEX_FN = 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/HLS/c2020/HLS_stack_2023_v1/HLS_tindex_master.csv'\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "DPS_INPUT_TILE_NUM_LIST = []\n",
    "for tindex_fn in [HLS_EXISTING_TINDEX_FN, Topo_tindex_master_fn]:\n",
    "    tindex = pd.read_csv(tindex_fn, storage_options={'anon':True})\n",
    "    \n",
    "    print(len(tindex['tile_num'].values))\n",
    "    \n",
    "    if len(tindex['tile_num'].values) > len(DPS_INPUT_TILE_NUM_LIST):\n",
    "        print(f\"{len(tindex['tile_num'].values)} tiles in {tindex_fn}\")\n",
    "        print(f'{os.path.basename(tindex_fn)} has the most tiles. Using this for DPS tiles list.')\n",
    "        DPS_INPUT_TILE_NUM_LIST = tindex['tile_num'].values\n",
    "\n",
    "len(DPS_INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8857874a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_IRREGULAR_TILES = False\n",
    "\n",
    "IRREGULAR_TILES_FN = '/projects/my-public-bucket/DPS_tile_lists/HLS_irregular_tiles.csv'\n",
    "\n",
    "if RUN_IRREGULAR_TILES:\n",
    "    DPS_INPUT_TILE_NUM_LIST = []\n",
    "    print(\"Running DPS on irregular tiles that forces a shape of 3000...\")\n",
    "    df = pd.read_csv(IRREGULAR_TILES_FN)\n",
    "    tile_num_list = []\n",
    "    for i in df[\"IrregularTiles\"]:\n",
    "        tile_num = int(i.split('_')[9])\n",
    "        DPS_INPUT_TILE_NUM_LIST.append(tile_num)\n",
    "    print(len(DPS_INPUT_TILE_NUM_LIST))\n",
    "    print(DPS_INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc5082d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5241"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DPS_INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "28d489ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0fea3b7",
   "metadata": {},
   "source": [
    "#### Note: make sure the `in_params_dict` coincides with the args of `build_stack.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65681b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#'INDEX_FN': '/projects/my-public-bucket/boreal_tiles_v003.gpkg',\n",
    "in_params_dict = {\n",
    "            'covar_tile_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
    "            'in_tile_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "            'covar_tile_fn': 'footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
    "            'in_tile_fn': 'boreal_tiles_v003.gpkg',\n",
    "            'in_tile_id_col': 'tile_num',\n",
    "            'in_tile_num':\"\",\n",
    "            'tile_buffer_m': 0,\n",
    "            'in_tile_layer': 'boreal_tiles_v003',\n",
    "            #'output_dir': 'dummy',  # a dummy dir so i dont have to change the .yaml now\n",
    "            #'topo_off': 'dummy', # functionality to turn off build a 'topo' stack is FALSE by default - so we have to set this flag (which turns OFF topo building) - this is hardcoded in the .sh now - if we want to build a topo stack with this script, need to learn how to set this flag with params dict \n",
    "            'covar_src_name': 'esa_worldcover_v100_2020',\n",
    "            'in_covar_s3_col': 's3_path',\n",
    "            'input_nodata_value': 0,\n",
    "            'shape': 3000\n",
    "            #'clip': 'dummy' # this is hardcoded in the .sh now - if we want to build a topo stack with this script, need to learn how to set this flag with params dict \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf6b76d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covar_tile_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
       " 'in_tile_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
       " 'covar_tile_fn': 'footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
       " 'in_tile_fn': 'boreal_tiles_v003.gpkg',\n",
       " 'in_tile_id_col': 'tile_num',\n",
       " 'in_tile_num': '',\n",
       " 'tile_buffer_m': 0,\n",
       " 'in_tile_layer': 'boreal_tiles_v003',\n",
       " 'covar_src_name': 'esa_worldcover_v100_2020',\n",
       " 'in_covar_s3_col': 's3_path',\n",
       " 'input_nodata_value': 0,\n",
       " 'shape': 3000}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86193dd5",
   "metadata": {},
   "source": [
    "## Run a DPS job across the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5e619703-7095-4f06-b707-9497690434ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAAP algorithm version name\n",
    "IDENTIFIER='LC_ESA_WC_2020'\n",
    "MAAP_VERSION = 'build_stack_v2023'\n",
    "ALGO_ID = \"run_build_stack\"\n",
    "USER = 'montesano'\n",
    "WORKER_TYPE = 'maap-dps-worker-8gb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "01c52cde-1d06-4007-a637-34988938b099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_NAME = IDENTIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4abfe38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input tiles for DPS: 5241\n",
      "DPS run #: 1\t| tile num: 4085\t| submit status: success\t| job id: 8d815eeb-4149-4ff9-b34f-99e1406b4d50\n",
      "DPS run #: 5\t| tile num: 1096\t| submit status: success\t| job id: f1403aa7-1ff2-42fc-af21-dbf2132a8879\n",
      "DPS run #: 10\t| tile num: 572\t| submit status: success\t| job id: ab711ff7-5cc5-4143-b754-14cd2b911298\n",
      "DPS run #: 50\t| tile num: 3096\t| submit status: success\t| job id: 939f3537-9a0c-4f72-8da0-14a32d1edefa\n",
      "DPS run #: 100\t| tile num: 2795\t| submit status: success\t| job id: d012f0eb-96e5-4756-9b93-fc1592c987df\n",
      "DPS run #: 250\t| tile num: 196\t| submit status: success\t| job id: 06b5177c-1c44-4d6d-a406-6ff23f88b4c1\n",
      "DPS run #: 500\t| tile num: 3618\t| submit status: success\t| job id: e84e422b-fe8a-4ac7-92fc-f01c38fc8861\n",
      "DPS run #: 750\t| tile num: 2007\t| submit status: success\t| job id: 38141138-a319-4345-ba80-6736190b711f\n",
      "DPS run #: 1000\t| tile num: 2742\t| submit status: success\t| job id: 110d176f-2e0b-4d72-946f-7817b2cba169\n",
      "DPS run #: 1500\t| tile num: 4017\t| submit status: success\t| job id: 82fb6f40-2e2b-4778-ab57-7c7098756898\n",
      "DPS run #: 2000\t| tile num: 1612\t| submit status: success\t| job id: 8d81c57b-7c91-4a96-bbba-635f262886fb\n",
      "DPS run #: 2500\t| tile num: 892\t| submit status: success\t| job id: 425a049e-cfb5-4cac-9433-5c9ca1cb6dba\n",
      "DPS run #: 3000\t| tile num: 27374\t| submit status: success\t| job id: 39d31edd-1faf-4105-b930-325e54543881\n",
      "DPS run #: 3500\t| tile num: 3893\t| submit status: success\t| job id: b9cb8236-d578-4940-89ce-b3bdd11552b2\n",
      "DPS run #: 4000\t| tile num: 58\t| submit status: success\t| job id: fbfdacf9-0fff-4744-95f2-dd20732b0a87\n",
      "DPS run #: 4500\t| tile num: 4361\t| submit status: success\t| job id: 9c6f4804-9174-4442-aee6-fb1ade918a33\n",
      "DPS run #: 5000\t| tile num: 3752\t| submit status: success\t| job id: 22c94c1c-779c-4cda-b376-de8106f49572\n",
      "DPS run #: 5241\t| tile num: 3160\t| submit status: success\t| job id: 2febe5e6-bab8-4f26-9e97-c0708c762b08\n",
      "Current time:\t202312070618\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5241 entries, 0 to 0\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   dps_num        5241 non-null   int64 \n",
      " 1   tile_num       5241 non-null   int64 \n",
      " 2   submit_time    5241 non-null   object\n",
      " 3   dbs_job_hour   5241 non-null   int64 \n",
      " 4   algo_id        5241 non-null   object\n",
      " 5   user           5241 non-null   object\n",
      " 6   worker_type    5241 non-null   object\n",
      " 7   job_id         5241 non-null   object\n",
      " 8   submit_status  5241 non-null   object\n",
      " 9   run_name       5241 non-null   object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 450.4+ KB\n",
      "CPU times: user 1min 49s, sys: 4.78 s, total: 1min 54s\n",
      "Wall time: 7min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(f\"# of input tiles for DPS: {len_input_list}\")\n",
    "\n",
    "for i, INPUT_TILE_NUM in enumerate(DPS_INPUT_TILE_NUM_LIST):\n",
    "    \n",
    "    DPS_num = i+1\n",
    "    \n",
    "    # Update the in_params_dict with th current INPUT_TILE_NUM\n",
    "    in_params_dict['in_tile_num'] = INPUT_TILE_NUM\n",
    "    \n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version=MAAP_VERSION,\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_params_dict\n",
    "        )\n",
    "    \n",
    "    #print(submit_result)\n",
    "    #break\n",
    "    \n",
    "    # Build a dataframe of submission details\n",
    "    submit_result_df = pd.DataFrame( \n",
    "        {\n",
    "                'dps_num':[DPS_num],\n",
    "                'tile_num':[INPUT_TILE_NUM],\n",
    "                'submit_time':[datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%s')],\n",
    "                'dbs_job_hour': [datetime.datetime.now().hour],\n",
    "                'algo_id': [ALGO_ID],\n",
    "                'user': [USER],\n",
    "                'worker_type': [WORKER_TYPE],\n",
    "                'job_id': [submit_result.id],\n",
    "                'submit_status': [submit_result.status],\n",
    "            \n",
    "        } \n",
    "    )\n",
    "    \n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(submit_result_df)\n",
    "    \n",
    "    if DPS_num in [1, 5, 10, 50, 100, 250, 500, 750, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000, len_input_list]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| tile num: {INPUT_TILE_NUM}\\t| submit status: {submit_result.status}\\t| job id: {submit_result.id}\") \n",
    "        \n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "submit_results_df['run_name'] = RUN_NAME\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "print(f\"Current time:\\t{nowtime}\")\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{ALGO_ID}_{RUN_NAME}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af6d0a",
   "metadata": {},
   "source": [
    "After almost any DPS job, you have to assess what succeeded and failed. This involves:\n",
    "1. building a table of job status based on job ids captured in the job_results_df from the DPS run chunk (this takes 40 mins for ~47k jobs) --> this tells you how many jobs failed\n",
    "2. merging the job status table with the job results df --> this tells you which specific granules (or tile nums) failed\n",
    "3. building another input list of granules for a follow-up DPS\n",
    "## Assess DPS results\n",
    "Build a table of job status based on job id - how many jobs failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4ccd5836-5a5d-44b9-a56c-57ff1c2da30f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA MAAP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'ExtractUtils' from '/projects/code/icesat2_boreal/lib/ExtractUtils.py'>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import ExtractUtils\n",
    "importlib.reload(ExtractUtils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c865412e-b4fe-4b10-99ed-a566e0e60c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "54dfdfd6-45cc-408b-9920-f61979661d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DPSJob.retrieve_status of {'job_id': 'd21b4131-f199-4a3f-922f-a51ed6a7ceca', 'status': None, 'machine_type': None, 'architecture': None, 'machine_memory_size': None, 'directory_size': None, 'operating_system': None, 'job_start_time': None, 'job_end_time': None, 'job_duration_seconds': None, 'cpu_usage': None, 'cache_usage': None, 'mem_usage': None, 'max_mem_usage': None, 'swap_usage': None, 'read_io_stats': None, 'write_io_stats': None, 'sync_io_stats': None, 'async_io_stats': None, 'total_io_stats': None, 'error_details': None, 'response_code': None, 'outputs': []}>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from maap.dps.dps_job import DPSJob\n",
    "job = DPSJob()\n",
    "job.id = submit_result.id #'d21b4131-f199-4a3f-922f-a51ed6a7ceca'\n",
    "job.retrieve_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "78fab3f3-f45a-43c4-a3a9-b09ac152ec19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job_id': 'cdc12d74-48ae-48d7-a8b8-541d90e35e0b', 'status': 'success', 'machine_type': None, 'architecture': None, 'machine_memory_size': None, 'directory_size': None, 'operating_system': None, 'job_start_time': None, 'job_end_time': None, 'job_duration_seconds': None, 'cpu_usage': None, 'cache_usage': None, 'mem_usage': None, 'max_mem_usage': None, 'swap_usage': None, 'read_io_stats': None, 'write_io_stats': None, 'sync_io_stats': None, 'async_io_stats': None, 'total_io_stats': None, 'error_details': None, 'response_code': 200, 'outputs': []}\n"
     ]
    }
   ],
   "source": [
    "print(submit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f6c2022c-c89e-4995-a011-95bd5a8c7ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#maap.getJobStatus(submit_result.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6f8194d3-233e-403e-979c-d269812941ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:27\u001b[0m\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/pandas/core/reshape/concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "running_list = []\n",
    "fails_list = []\n",
    "success_list = []\n",
    "for DPS_DATETIME in [nowtime]:\n",
    "    for fn in LIST_SUBMISSIONS:\n",
    "        if DPS_DATETIME in fn and not 'job_status' in fn:\n",
    "            \n",
    "            DPS_alg_id = os.path.basename(fn.split('_submission_results_')[0].replace('DPS_',''))\n",
    "            thentime = fn.split('_')[-1].replace('.csv','')\n",
    "            print(f'DPS alg:\\t\\t{DPS_alg_id}')\n",
    "            print(f'DPS run name:\\t\\t{RUN_NAME}')\n",
    "            print(f'DPS launch time:\\t{thentime}')\n",
    "            \n",
    "            # Build job status table\n",
    "            df_jstatus = ExtractUtils.BUILD_TABLE_JOBSTATUS(pd.read_csv(fn))\n",
    "            \n",
    "            # Save job status table\n",
    "            df_jstatus.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{RUN_NAME}_submission_results_job_status_{len(df_jstatus)}_{thentime}.csv')\n",
    "\n",
    "            # Get current fails df and append to list\n",
    "            df_jstatus['run_type'] = RUN_NAME\n",
    "\n",
    "            running_list.append(df_jstatus[ (df_jstatus['status'] == 'Running') ] )\n",
    "            fails_list.append(  df_jstatus[ (df_jstatus['status'] == 'Failed') ] )\n",
    "            success_list.append(df_jstatus[ (df_jstatus['status'] == 'Succeeded') ] )\n",
    "            \n",
    "df_all_running = pd.concat(running_list)          \n",
    "df_all_fails =   pd.concat(fails_list)\n",
    "df_all_success = pd.concat(success_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4a3c91b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPS alg:\t\trun_build_stack_LC_build_stack_LC\n",
      "DPS launch time:\t202312061353\n"
     ]
    },
    {
     "ename": "InvalidURL",
     "evalue": "Invalid URL 'https:///api/dps/job/9c7f52e7-3033-4c0c-bd30-8372b330a11d/status': No host supplied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:9\u001b[0m\n",
      "File \u001b[0;32m~/code/icesat2_boreal/lib/ExtractUtils.py:344\u001b[0m, in \u001b[0;36mBUILD_TABLE_JOBSTATUS\u001b[0;34m(submit_results_df, status_col)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# If jobs failed to submit, then they have a NaN for jobid, which makes the merge (join) fail\u001b[39;00m\n\u001b[1;32m    342\u001b[0m submit_results_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 344\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [job_id], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m:[maap\u001b[38;5;241m.\u001b[39mgetJobStatus(job_id)]}) \u001b[38;5;28;01mfor\u001b[39;00m job_id \u001b[38;5;129;01min\u001b[39;00m submit_results_df\u001b[38;5;241m.\u001b[39mjob_id\u001b[38;5;241m.\u001b[39mto_list()])\n\u001b[1;32m    345\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mmerge(job_status_df, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m,  right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount total jobs:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(job_status_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/code/icesat2_boreal/lib/ExtractUtils.py:344\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# If jobs failed to submit, then they have a NaN for jobid, which makes the merge (join) fail\u001b[39;00m\n\u001b[1;32m    342\u001b[0m submit_results_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 344\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [job_id], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[43mmaap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetJobStatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m)\u001b[49m]}) \u001b[38;5;28;01mfor\u001b[39;00m job_id \u001b[38;5;129;01min\u001b[39;00m submit_results_df\u001b[38;5;241m.\u001b[39mjob_id\u001b[38;5;241m.\u001b[39mto_list()])\n\u001b[1;32m    345\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mmerge(job_status_df, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m,  right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount total jobs:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(job_status_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/maap/maap.py:314\u001b[0m, in \u001b[0;36mMAAP.getJobStatus\u001b[0;34m(self, jobid)\u001b[0m\n\u001b[1;32m    312\u001b[0m job \u001b[38;5;241m=\u001b[39m DPSJob()\n\u001b[1;32m    313\u001b[0m job\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m jobid\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/maap/dps/dps_job.py:71\u001b[0m, in \u001b[0;36mDPSJob.retrieve_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     70\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(headers)\n\u001b[0;32m---> 71\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__not_self_signed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_job_status_result(RequestsUtils\u001b[38;5;241m.\u001b[39mcheck_response(response))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[1;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[1;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[1;32m    581\u001b[0m )\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/sessions.py:486\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    483\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    485\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[0;32m--> 486\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCaseInsensitiveDict\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_method(method)\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/models.py:445\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No scheme supplied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No host supplied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# In general, we want to try IDNA encoding the hostname if the string contains\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# non-ASCII characters. This allows users to automatically get the correct IDNA\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# behaviour. For strings containing only ASCII characters, we need to also verify\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# it doesn't start with a wildcard (*), before allowing the unencoded hostname.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unicode_is_ascii(host):\n",
      "\u001b[0;31mInvalidURL\u001b[0m: Invalid URL 'https:///api/dps/job/9c7f52e7-3033-4c0c-bd30-8372b330a11d/status': No host supplied"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LIST_SUBMISSIONS = sorted(glob.glob(f'/projects/my-public-bucket/dps_submission_results/DPS_{ALGO_ID}_{RUN_NAME}_submission_results_*.csv'),key=ExtractUtils.func, reverse=True)\n",
    "for DPS_DATETIME in [nowtime]:\n",
    "    for fn in LIST_SUBMISSIONS:\n",
    "        if DPS_DATETIME in fn and not 'job_status' in fn:\n",
    "            DPS_alg_id = os.path.basename(fn.split('_submission_results_')[0].replace('DPS_',''))\n",
    "            thentime = fn.split('_')[-1].replace('.csv','')\n",
    "            print(f'DPS alg:\\t\\t{DPS_alg_id}')\n",
    "            print(f'DPS launch time:\\t{thentime}')\n",
    "            z = ExtractUtils.BUILD_TABLE_JOBSTATUS(pd.read_csv(fn))\n",
    "            # Save job status table\n",
    "            z.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_job_status_{len(z)}_{thentime}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fb9ff4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24389]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[z['wps:Status'] =='Succeeded'].tile_num.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73b24071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wps:Result': {'@xmlns:ows': 'http://www.opengis.net/ows/2.0',\n",
       "  '@xmlns:schemaLocation': 'http://schemas.opengis.net/wps/2.0/wps.xsd',\n",
       "  '@xmlns:wps': 'http://www.opengis.net/wps/2.0',\n",
       "  '@xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
       "  'wps:JobID': '20d88b45-eb25-48e6-b6f0-477ca59f3bb6',\n",
       "  'wps:Output': {'@id': 'traceback',\n",
       "   'wps:Data': \"+ export HOME=/root\\n+ HOME=/root\\n+ exec /docker-stats-on-exit-shim _docker_stats.json /app/dps_wrapper.sh /app/icesat2_boreal/dps/alg_3-1-3/run.sh footprints_v100_2020_v100_2020_map-s3.gpkg boreal_tiles_v003.gpkg tile_num 2223 0 boreal_tiles_v003 esa_worldcover_v100_2020 s3_path 0 3000\\n+ /app/icesat2_boreal/dps/alg_3-1-3/run.sh footprints_v100_2020_v100_2020_map-s3.gpkg boreal_tiles_v003.gpkg tile_num 2223 0 boreal_tiles_v003 esa_worldcover_v100_2020 s3_path 0 3000\\n+ unset PROJ_LIB\\n+ mkdir output\\n+++ dirname /app/icesat2_boreal/dps/alg_3-1-3/run.sh\\n++ cd /app/icesat2_boreal/dps/alg_3-1-3\\n++ pwd -P\\n+ basedir=/app/icesat2_boreal/dps/alg_3-1-3\\n+ FILENAMELIST=($(ls -d input/*))\\n++ ls -d input/boreal_tiles_v003.gpkg input/footprints_v100_2020_v100_2020_map-s3.gpkg\\n+ COVAR_TILE_FN=/data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/input/footprints_v100_2020_v100_2020_map-s3.gpkg\\n+ IN_TILE_FN=/data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/input/boreal_tiles_v003.gpkg\\n+ OUTPUTDIR=/data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/output\\n+ python /app/icesat2_boreal/dps/alg_3-1-3/../../lib/build_stack.py --covar_tile_fn /data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/input/footprints_v100_2020_v100_2020_map-s3.gpkg --in_tile_fn /data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/input/boreal_tiles_v003.gpkg --in_tile_id_col tile_num --in_tile_num 2223 --tile_buffer_m 0 --in_tile_layer boreal_tiles_v003 --output_dir /data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/output --topo_off --covar_src_name esa_worldcover_v100_2020 --in_covar_s3_col s3_path --input_nodata_value 0 --shape footprints_v100_2020_v100_2020_map-s3.gpkg0 --clip\\nERROR 1: PROJ: proj_create_from_database: Open of /opt/conda/envs/icesat2_boreal/share/proj failed\\nusage: build_stack.py [-h] [-i IN_TILE_FN] [-n IN_TILE_NUM] [-b TILE_BUFFER_M]\\n                      [--in_tile_id_col IN_TILE_ID_COL] [-l IN_TILE_LAYER]\\n                      [-o OUTPUT_DIR] [-r RES] [--shape SHAPE]\\n                      [-covar COVAR_TILE_FN]\\n                      [--input_nodata_value INPUT_NODATA_VALUE]\\n                      [--in_covar_s3_col IN_COVAR_S3_COL] [-tmp TMP_OUT_PATH]\\n                      [-name COVAR_SRC_NAME] [--topo_off] [--clip]\\nbuild_stack.py: error: argument --shape: invalid int value: 'footprints_v100_2020_v100_2020_map-s3.gpkg0'\\n+ cp _stderr.txt _alt_traceback.txt\"}}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Failed'].iloc[43].job_id).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec1852",
   "metadata": {},
   "source": [
    "## Update the DPS input tiles list with only the tiles that failed - then run the DPS submit chunk above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ab7ab0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2917, 980, 25175, 25740, 2981, 3879]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPS_INPUT_TILE_NUM_LIST = z[z['wps:Status'] =='Failed'].tile_num.to_list()\n",
    "len(DPS_INPUT_TILE_NUM_LIST)\n",
    "DPS_INPUT_TILE_NUM_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d1e0fd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STRAGGLER_TILE_LIST = [3269, 3088, 2917, 2513, 2228, 1554, 1274, 1200, 620, 980, 790, 23828, 24389, 24108, 23501, 23830, 108, 25175, 25458, 25740, 163, 28967, 1172, 1431, 2981, 3531, 4463, 3879, 4207, 4445, 2888, 3790]\n",
    "DPS_INPUT_TILE_NUM_LIST = STRAGGLER_TILE_LIST\n",
    "len(DPS_INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0952e739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINISHED_LIST = z[z['wps:Status'] =='Succeeded'].tile_num.to_list()\n",
    "DPS_INPUT_TILE_NUM_LIST = set(STRAGGLER_TILE_LIST) - set(FINISHED_LIST)\n",
    "len(DPS_INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "868245e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRAGGLER_TILE_LIST2 = [3855,24389]\n",
    "DPS_INPUT_TILE_NUM_LIST = STRAGGLER_TILE_LIST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd440449-d3e8-4315-bd66-41187a65e2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7e16d9c1-f63f-4241-a63f-2b6d475d6752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /projects/code/icesat2_boreal/lib/build_tindex_master.py --RETURN_DUPS --user montesano --dps_identifier build_stack_v2023/LC_ESA_WC_2020 -alg_name run_build_stack -t LC -y 2023 --dps_month_list 01 02 03 04 05 06 07 08 09 10 11 12 -d_min 1 --outdir /projects/my-public-bucket/DPS_tile_lists/build_stack_v2023/LC_ESA_WC_2020\n",
      "\n",
      "real\t0m0.000s\n",
      "user\t0m0.000s\n",
      "sys\t0m0.000s\n"
     ]
    }
   ],
   "source": [
    "USER = 'montesano'\n",
    "\n",
    "DPS_IDENTIFIER = f'{MAAP_VERSION}/{IDENTIFIER}'\n",
    "DPS_MONTH_LIST = '01 02 03 04 05 06 07 08 09 10 11 12'        \n",
    "TYPE = 'LC'\n",
    "\n",
    "DPS_DAY_MIN = 1\n",
    "OUTDIR = f'/projects/my-public-bucket/DPS_tile_lists/{ALGO_ID}/{DPS_IDENTIFIER}'\n",
    "!mkdir -p $OUTDIR\n",
    "\n",
    "# Build tindex\n",
    "args = f\"--RETURN_DUPS \\\n",
    "--user {USER} \\\n",
    "--dps_identifier {DPS_IDENTIFIER} \\\n",
    "-alg_name {ALGO_ID} \\\n",
    "-t {TYPE} \\\n",
    "-y 2023 \\\n",
    "--dps_month_list {DPS_MONTH_LIST} \\\n",
    "-d_min {DPS_DAY_MIN} \\\n",
    "--outdir {OUTDIR}\"\n",
    "\n",
    "!time echo python /projects/code/icesat2_boreal/lib/build_tindex_master.py $args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ab0003f5-ee7a-4cb5-a93c-69adee50b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/projects/my-public-bucket/DPS_tile_lists/build_stack_v2023/LC_ESA_WC_2020/LC_tindex_master_duplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d507c520-c267-42f0-b379-1fa5620d8432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>s3_path</th>\n",
       "      <th>local_path</th>\n",
       "      <th>file</th>\n",
       "      <th>tile_num</th>\n",
       "      <th>creation time</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2826</td>\n",
       "      <td>s3://maap-ops-workspace/montesano/dps_output/r...</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/run_bui...</td>\n",
       "      <td>esa_worldcover_v100_2020_874_cog.tif</td>\n",
       "      <td>874</td>\n",
       "      <td>2023-12-07 06:39:10</td>\n",
       "      <td>dropped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2825</td>\n",
       "      <td>s3://maap-ops-workspace/montesano/dps_output/r...</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/run_bui...</td>\n",
       "      <td>esa_worldcover_v100_2020_939_cog.tif</td>\n",
       "      <td>939</td>\n",
       "      <td>2023-12-07 06:39:09</td>\n",
       "      <td>dropped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2824</td>\n",
       "      <td>s3://maap-ops-workspace/montesano/dps_output/r...</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/run_bui...</td>\n",
       "      <td>esa_worldcover_v100_2020_30505_cog.tif</td>\n",
       "      <td>30505</td>\n",
       "      <td>2023-12-07 06:39:09</td>\n",
       "      <td>dropped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2823</td>\n",
       "      <td>s3://maap-ops-workspace/montesano/dps_output/r...</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/run_bui...</td>\n",
       "      <td>esa_worldcover_v100_2020_1191_cog.tif</td>\n",
       "      <td>1191</td>\n",
       "      <td>2023-12-07 06:39:09</td>\n",
       "      <td>dropped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2822</td>\n",
       "      <td>s3://maap-ops-workspace/montesano/dps_output/r...</td>\n",
       "      <td>/projects/my-private-bucket/dps_output/run_bui...</td>\n",
       "      <td>esa_worldcover_v100_2020_26826_cog.tif</td>\n",
       "      <td>26826</td>\n",
       "      <td>2023-12-07 06:39:09</td>\n",
       "      <td>dropped</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            s3_path  \\\n",
       "0        2826  s3://maap-ops-workspace/montesano/dps_output/r...   \n",
       "1        2825  s3://maap-ops-workspace/montesano/dps_output/r...   \n",
       "2        2824  s3://maap-ops-workspace/montesano/dps_output/r...   \n",
       "3        2823  s3://maap-ops-workspace/montesano/dps_output/r...   \n",
       "4        2822  s3://maap-ops-workspace/montesano/dps_output/r...   \n",
       "\n",
       "                                          local_path  \\\n",
       "0  /projects/my-private-bucket/dps_output/run_bui...   \n",
       "1  /projects/my-private-bucket/dps_output/run_bui...   \n",
       "2  /projects/my-private-bucket/dps_output/run_bui...   \n",
       "3  /projects/my-private-bucket/dps_output/run_bui...   \n",
       "4  /projects/my-private-bucket/dps_output/run_bui...   \n",
       "\n",
       "                                     file tile_num        creation time  \\\n",
       "0    esa_worldcover_v100_2020_874_cog.tif      874  2023-12-07 06:39:10   \n",
       "1    esa_worldcover_v100_2020_939_cog.tif      939  2023-12-07 06:39:09   \n",
       "2  esa_worldcover_v100_2020_30505_cog.tif    30505  2023-12-07 06:39:09   \n",
       "3   esa_worldcover_v100_2020_1191_cog.tif     1191  2023-12-07 06:39:09   \n",
       "4  esa_worldcover_v100_2020_26826_cog.tif    26826  2023-12-07 06:39:09   \n",
       "\n",
       "    status  \n",
       "0  dropped  \n",
       "1  dropped  \n",
       "2  dropped  \n",
       "3  dropped  \n",
       "4  dropped  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['tile_num'] = df['file'].str.split('_', expand=True)[4].str.strip('_cog.tif')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cdf1d-c589-42d6-b3a1-697914d4fe46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "/projects/env/above",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
