{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7bcf9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from maap.maap import MAAP\n",
    "#maap = MAAP(maap_host='api.ops.maap-project.org')\n",
    "maap = MAAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "be655aaf-644c-4041-8d04-e1237a50a7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'api.maap-project.org'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maap._MAAP_HOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c8ff57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U -r /projects/Developer/icesat2_boreal/dps/requirements_main.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e3ace",
   "metadata": {},
   "source": [
    "# Launch DPS for build_stack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7041b2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For some reason this is needed to get s3fs to work in ExtractUtils\n",
    "# this upgrades to 0.3.4 even though we already specify this version in requirements_main...\n",
    "#!pip install s3fs --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c541eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.10/site-packages (0.13.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "!pip install xmltodict\n",
    "import xmltodict\n",
    "import sys\n",
    "sys.path.append('/projects/code/icesat2_boreal/lib')\n",
    "import ExtractUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b52264b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023.12.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "s3fs.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e42d",
   "metadata": {},
   "source": [
    "# Test (locally) the script for DPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4283ab1",
   "metadata": {},
   "source": [
    "##### To run build_stack.py across a tiled raster dataset you need a bunch of args that we'll gather into a dictionary\n",
    "\n",
    "s3 you need to have a vector footprint of that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1147899c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TILE_NUM = 1274"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df593ad",
   "metadata": {},
   "source": [
    "#### Dictionary preparation makes this script very flexible and transferable to another s3 dataset\n",
    "This dictionary is specific to the ESA Worldcover dataset.  \n",
    "To run '`build_stack.py` across another dataset, just prepare another dictionary here and everything below should be exactly the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f57e6230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ESA Worldcover 2020\n",
    "BUILD_STACK_DICT = {\n",
    "            #'INDEX_FN': '/projects/my-public-bucket/boreal_tiles_v003.gpkg',\n",
    "            'INDEX_FN': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "            'ID_COL_NAME': 'tile_num',\n",
    "            'TILE_NUM':TILE_NUM,\n",
    "            'INDEX_LYR': 'boreal_tiles_v003',\n",
    "            # Worldcover data is accessed via its footprint, with a 's3_path' col identifying the s3 locations of each tile\n",
    "            'RASTER_NAME': 'esa_worldcover_v100_2020',\n",
    "            #'COVAR_TILE_FN': '/projects/my-public-bucket/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
    "            'COVAR_TILE_FN': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
    "            'IN_COVAR_S3_COL': 's3_path',\n",
    "            'OUTDIR': '/projects/my-public-bucket/DPS_ESA_LC',\n",
    "            'NODATA_VAL': 0,\n",
    "            'OUTPUT_CLIP_COG_FN':'',\n",
    "            'CREDENTIALS_FN': None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ae6e066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INDEX clip shapes should be in equal area\n",
    "# COVAR_TILE_FN tiles (of raster COGs) should be in 4326\n",
    "args = f\"\\\n",
    "    --in_tile_fn {BUILD_STACK_DICT['INDEX_FN']} \\\n",
    "    --in_tile_id_col {BUILD_STACK_DICT['ID_COL_NAME']} \\\n",
    "    --in_tile_num {BUILD_STACK_DICT['TILE_NUM']} \\\n",
    "    --tile_buffer_m 0 \\\n",
    "    --in_tile_layer {BUILD_STACK_DICT['INDEX_LYR']} \\\n",
    "    -o {BUILD_STACK_DICT['OUTDIR']} \\\n",
    "    --topo_off \\\n",
    "    --covar_src_name {BUILD_STACK_DICT['RASTER_NAME']} \\\n",
    "    --covar_tile_fn {BUILD_STACK_DICT['COVAR_TILE_FN']} \\\n",
    "    --in_covar_s3_col {BUILD_STACK_DICT['IN_COVAR_S3_COL']} \\\n",
    "    --input_nodata_value {BUILD_STACK_DICT['NODATA_VAL']} \\\n",
    "    --shape 3000 \\\n",
    "    --clip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a27d919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /projects/code/icesat2_boreal/lib/build_stack.py --in_tile_fn https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg --in_tile_id_col tile_num --in_tile_num 1274 --tile_buffer_m 0 --in_tile_layer boreal_tiles_v003 -o /projects/my-public-bucket/DPS_ESA_LC --topo_off --covar_src_name esa_worldcover_v100_2020 --covar_tile_fn https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg --in_covar_s3_col s3_path --input_nodata_value 0 --shape 3000 --clip\n"
     ]
    }
   ],
   "source": [
    "!echo python /projects/code/icesat2_boreal/lib/build_stack.py $args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b16d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DPS_INPUT_TILE_NUM_LIST = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a058f23-c0a1-4445-9656-70eb7489441b",
   "metadata": {},
   "source": [
    "### Use MAAP Registration call in notebook chunk to register DPS algorithm\n",
    " - We need to register a DPS algorithm called `run_build_stack_LC` before proceeding to the chunks below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7810c9e6-5dc8-4969-b1f4-beb3d06e9d96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"code\": 200, \"message\": {\"id\": \"3ded06e3a34ac17bc8dea976020371f0ca306a1e\", \"short_id\": \"3ded06e3\", \"created_at\": \"2023-12-06T20:19:52.000+00:00\", \"parent_ids\": [\"d64a4daa9f01801c7495e7ba8b12065c3bb5023e\"], \"title\": \"Registering algorithm: run_build_stack_LC\", \"message\": \"Registering algorithm: run_build_stack_LC\", \"author_name\": \"root\", \"author_email\": \"root@e49d5cea4b76\", \"authored_date\": \"2023-12-06T20:19:52.000+00:00\", \"committer_name\": \"root\", \"committer_email\": \"root@e49d5cea4b76\", \"committed_date\": \"2023-12-06T20:19:52.000+00:00\", \"trailers\": {}, \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/commit/3ded06e3a34ac17bc8dea976020371f0ca306a1e\", \"stats\": {\"additions\": 0, \"deletions\": 0, \"total\": 0}, \"status\": \"pending\", \"project_id\": 3, \"last_pipeline\": {\"id\": 10336, \"iid\": 612, \"project_id\": 3, \"sha\": \"3ded06e3a34ac17bc8dea976020371f0ca306a1e\", \"ref\": \"main\", \"status\": \"pending\", \"source\": \"push\", \"created_at\": \"2023-12-06T20:19:54.306Z\", \"updated_at\": \"2023-12-06T20:19:54.914Z\", \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/pipelines/10336\"}, \"job_web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/10588\", \"job_log_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/10588/raw\"}}\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maap.register_algorithm_from_yaml_file(\"/projects/code/icesat2_boreal/dps/registered/run_build_stack_LC.yml\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836409b4",
   "metadata": {},
   "source": [
    "# Build a DPS list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d40e2d35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5241\n",
      "5241 tiles in s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/HLS/c2020/HLS_stack_2023_v1/HLS_tindex_master.csv\n",
      "HLS_tindex_master.csv has the most tiles. Using this for DPS tiles list.\n",
      "5220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5241"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HLS_tindex_master_fn = \"s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/HLS/fall2022/HLS_stack_2022_v2/HLS_tindex_master.csv\".replace('s3://maap-ops-workspace/shared/nathanmthomas', '/projects/my-public-bucket')\n",
    "Topo_tindex_master_fn = \"s3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/Topo_tindex_master.csv\" #.replace('s3://maap-ops-workspace/shared/nathanmthomas', '/projects/my-public-bucket')\n",
    "\n",
    "# Spring 2022 c2020 composite - use this as template set of tiles (5241 tiles)\n",
    "HLS_EXISTING_TINDEX_FN = 's3://maap-ops-workspace/shared/nathanmthomas/DPS_tile_lists/HLS/c2020/HLS_stack_2023_v1/HLS_tindex_master.csv'\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "DPS_INPUT_TILE_NUM_LIST = []\n",
    "for tindex_fn in [HLS_EXISTING_TINDEX_FN, Topo_tindex_master_fn]:\n",
    "    tindex = pd.read_csv(tindex_fn, storage_options={'anon':True})\n",
    "    \n",
    "    print(len(tindex['tile_num'].values))\n",
    "    \n",
    "    if len(tindex['tile_num'].values) > len(DPS_INPUT_TILE_NUM_LIST):\n",
    "        print(f\"{len(tindex['tile_num'].values)} tiles in {tindex_fn}\")\n",
    "        print(f'{os.path.basename(tindex_fn)} has the most tiles. Using this for DPS tiles list.')\n",
    "        DPS_INPUT_TILE_NUM_LIST = tindex['tile_num'].values\n",
    "\n",
    "len(DPS_INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8857874a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_IRREGULAR_TILES = False\n",
    "\n",
    "IRREGULAR_TILES_FN = '/projects/my-public-bucket/DPS_tile_lists/HLS_irregular_tiles.csv'\n",
    "\n",
    "if RUN_IRREGULAR_TILES:\n",
    "    DPS_INPUT_TILE_NUM_LIST = []\n",
    "    print(\"Running DPS on irregular tiles that forces a shape of 3000...\")\n",
    "    df = pd.read_csv(IRREGULAR_TILES_FN)\n",
    "    tile_num_list = []\n",
    "    for i in df[\"IrregularTiles\"]:\n",
    "        tile_num = int(i.split('_')[9])\n",
    "        DPS_INPUT_TILE_NUM_LIST.append(tile_num)\n",
    "    print(len(DPS_INPUT_TILE_NUM_LIST))\n",
    "    print(DPS_INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc5082d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5241"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DPS_INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28d489ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_NAME = 'build_stack_LC'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fea3b7",
   "metadata": {},
   "source": [
    "#### Note: make sure the `in_params_dict` coincides with the args of `build_stack.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65681b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#'INDEX_FN': '/projects/my-public-bucket/boreal_tiles_v003.gpkg',\n",
    "in_params_dict = {\n",
    "            'covar_tile_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
    "            'in_tile_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
    "            'covar_tile_fn': 'footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
    "            'in_tile_fn': 'boreal_tiles_v003.gpkg',\n",
    "            'in_tile_id_col': 'tile_num',\n",
    "            'in_tile_num':\"\",\n",
    "            'tile_buffer_m': 0,\n",
    "            'in_tile_layer': 'boreal_tiles_v003',\n",
    "            #'output_dir': 'dummy',  # a dummy dir so i dont have to change the .yaml now\n",
    "            #'topo_off': 'dummy', # functionality to turn off build a 'topo' stack is FALSE by default - so we have to set this flag (which turns OFF topo building) - this is hardcoded in the .sh now - if we want to build a topo stack with this script, need to learn how to set this flag with params dict \n",
    "            'covar_src_name': 'esa_worldcover_v100_2020',\n",
    "            'in_covar_s3_col': 's3_path',\n",
    "            'input_nodata_value': 0,\n",
    "            'shape': 3000\n",
    "            #'clip': 'dummy' # this is hardcoded in the .sh now - if we want to build a topo stack with this script, need to learn how to set this flag with params dict \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf6b76d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covar_tile_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/analyze_agb/footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
       " 'in_tile_url': 'https://maap-ops-workspace.s3.amazonaws.com/shared/nathanmthomas/boreal_tiles_v003.gpkg',\n",
       " 'covar_tile_fn': 'footprints_v100_2020_v100_2020_map-s3.gpkg',\n",
       " 'in_tile_fn': 'boreal_tiles_v003.gpkg',\n",
       " 'in_tile_id_col': 'tile_num',\n",
       " 'in_tile_num': '',\n",
       " 'tile_buffer_m': 0,\n",
       " 'in_tile_layer': 'boreal_tiles_v003',\n",
       " 'covar_src_name': 'esa_worldcover_v100_2020',\n",
       " 'in_covar_s3_col': 's3_path',\n",
       " 'input_nodata_value': 0,\n",
       " 'shape': 3000}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86193dd5",
   "metadata": {},
   "source": [
    "## Run a DPS job across the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e619703-7095-4f06-b707-9497690434ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAAP algorithm version name\n",
    "IDENTIFIER='LC_ESA_2020'\n",
    "MAAP_VERSION = \"master\"\n",
    "ALGO_ID = \"run_build_stack_LC\"\n",
    "USER = 'montesano'\n",
    "WORKER_TYPE = 'maap-dps-worker-8gb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4abfe38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input tiles for DPS: 5241\n",
      "DPS run #: 1\t| tile num: 4085\t| submit status: success\t| job id: 9c7f52e7-3033-4c0c-bd30-8372b330a11d\n",
      "DPS run #: 5\t| tile num: 1096\t| submit status: success\t| job id: d21b4131-f199-4a3f-922f-a51ed6a7ceca\n",
      "Current time:\t202312061353\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5 entries, 0 to 0\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   dps_num        5 non-null      int64 \n",
      " 1   tile_num       5 non-null      int64 \n",
      " 2   submit_time    5 non-null      object\n",
      " 3   dbs_job_hour   5 non-null      int64 \n",
      " 4   algo_id        5 non-null      object\n",
      " 5   user           5 non-null      object\n",
      " 6   worker_type    5 non-null      object\n",
      " 7   job_id         5 non-null      object\n",
      " 8   submit_status  5 non-null      object\n",
      " 9   run_name       5 non-null      object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 440.0+ bytes\n",
      "CPU times: user 115 ms, sys: 0 ns, total: 115 ms\n",
      "Wall time: 727 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "submit_results_df_list = []\n",
    "len_input_list = len(DPS_INPUT_TILE_NUM_LIST)\n",
    "print(f\"# of input tiles for DPS: {len_input_list}\")\n",
    "\n",
    "for i, INPUT_TILE_NUM in enumerate(DPS_INPUT_TILE_NUM_LIST[0:5]):\n",
    "    \n",
    "    DPS_num = i+1\n",
    "    \n",
    "    # Update the in_params_dict with th current INPUT_TILE_NUM\n",
    "    in_params_dict['in_tile_num'] = INPUT_TILE_NUM\n",
    "    \n",
    "    submit_result = maap.submitJob(\n",
    "            identifier=IDENTIFIER,\n",
    "            algo_id=ALGO_ID,\n",
    "            version=MAAP_VERSION,\n",
    "            username=USER, # username needs to be the same as whoever created the workspace\n",
    "            queue=WORKER_TYPE,\n",
    "            **in_params_dict\n",
    "        )\n",
    "    \n",
    "    #print(submit_result)\n",
    "    #break\n",
    "    \n",
    "    # Build a dataframe of submission details\n",
    "    submit_result_df = pd.DataFrame( \n",
    "        {\n",
    "                'dps_num':[DPS_num],\n",
    "                'tile_num':[INPUT_TILE_NUM],\n",
    "                'submit_time':[datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%s')],\n",
    "                'dbs_job_hour': [datetime.datetime.now().hour],\n",
    "                'algo_id': [ALGO_ID],\n",
    "                'user': [USER],\n",
    "                'worker_type': [WORKER_TYPE],\n",
    "                'job_id': [submit_result.id],\n",
    "                'submit_status': [submit_result.status],\n",
    "            \n",
    "        } \n",
    "    )\n",
    "    \n",
    "    # Append to a list of data frames of submission results\n",
    "    submit_results_df_list.append(submit_result_df)\n",
    "    \n",
    "    if DPS_num in [1, 5, 10, 50, 100, 250, 500, 750, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 7000, 9000, 11000, 13000, 15000, 17000, 19000, 21000, 24000, len_input_list]:\n",
    "        print(f\"DPS run #: {DPS_num}\\t| tile num: {INPUT_TILE_NUM}\\t| submit status: {submit_result.status}\\t| job id: {submit_result.id}\") \n",
    "        \n",
    "# Build a final submission results df and save\n",
    "submit_results_df = pd.concat(submit_results_df_list)\n",
    "submit_results_df['run_name'] = RUN_NAME\n",
    "nowtime = pd.Timestamp.now().strftime('%Y%m%d%H%M')\n",
    "print(f\"Current time:\\t{nowtime}\")\n",
    "submit_results_df.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{ALGO_ID}_{RUN_NAME}_submission_results_{len_input_list}_{nowtime}.csv')\n",
    "submit_results_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af6d0a",
   "metadata": {},
   "source": [
    "After almost any DPS job, you have to assess what succeeded and failed. This involves:\n",
    "1. building a table of job status based on job ids captured in the job_results_df from the DPS run chunk (this takes 40 mins for ~47k jobs) --> this tells you how many jobs failed\n",
    "2. merging the job status table with the job results df --> this tells you which specific granules (or tile nums) failed\n",
    "3. building another input list of granules for a follow-up DPS\n",
    "## Assess DPS results\n",
    "Build a table of job status based on job id - how many jobs failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4ccd5836-5a5d-44b9-a56c-57ff1c2da30f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA MAAP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'ExtractUtils' from '/projects/code/icesat2_boreal/lib/ExtractUtils.py'>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import ExtractUtils\n",
    "importlib.reload(ExtractUtils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c865412e-b4fe-4b10-99ed-a566e0e60c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "54dfdfd6-45cc-408b-9920-f61979661d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DPSJob.retrieve_status of {'job_id': 'd21b4131-f199-4a3f-922f-a51ed6a7ceca', 'status': None, 'machine_type': None, 'architecture': None, 'machine_memory_size': None, 'directory_size': None, 'operating_system': None, 'job_start_time': None, 'job_end_time': None, 'job_duration_seconds': None, 'cpu_usage': None, 'cache_usage': None, 'mem_usage': None, 'max_mem_usage': None, 'swap_usage': None, 'read_io_stats': None, 'write_io_stats': None, 'sync_io_stats': None, 'async_io_stats': None, 'total_io_stats': None, 'error_details': None, 'response_code': None, 'outputs': []}>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from maap.dps.dps_job import DPSJob\n",
    "job = DPSJob()\n",
    "job.id = submit_result.id #'d21b4131-f199-4a3f-922f-a51ed6a7ceca'\n",
    "job.retrieve_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "78fab3f3-f45a-43c4-a3a9-b09ac152ec19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job_id': 'd21b4131-f199-4a3f-922f-a51ed6a7ceca', 'status': 'success', 'machine_type': None, 'architecture': None, 'machine_memory_size': None, 'directory_size': None, 'operating_system': None, 'job_start_time': None, 'job_end_time': None, 'job_duration_seconds': None, 'cpu_usage': None, 'cache_usage': None, 'mem_usage': None, 'max_mem_usage': None, 'swap_usage': None, 'read_io_stats': None, 'write_io_stats': None, 'sync_io_stats': None, 'async_io_stats': None, 'total_io_stats': None, 'error_details': None, 'response_code': 200, 'outputs': []}\n"
     ]
    }
   ],
   "source": [
    "print(submit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f6c2022c-c89e-4995-a011-95bd5a8c7ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#maap.getJobStatus('d21b4131-f199-4a3f-922f-a51ed6a7ceca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6f8194d3-233e-403e-979c-d269812941ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPS alg:\t\trun_build_stack_LC_build_stack_LC\n",
      "DPS run name:\t\tbuild_stack_LC\n",
      "DPS launch time:\t202312061353\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'submit_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:15\u001b[0m\n",
      "File \u001b[0;32m~/code/icesat2_boreal/lib/ExtractUtils.py:345\u001b[0m, in \u001b[0;36mBUILD_TABLE_JOBSTATUS\u001b[0;34m(submit_results_df, status_col)\u001b[0m\n\u001b[1;32m    342\u001b[0m submit_results_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m#job_status_df = pd.concat([pd.DataFrame({'job_id': [job_id], 'status':[maap.getJobStatus(job_id)]}) for job_id in submit_results_df.job_id.to_list()])\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [job_id], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m:[submit_result\u001b[38;5;241m.\u001b[39mstatus]}) \u001b[38;5;28;01mfor\u001b[39;00m job_id \u001b[38;5;129;01min\u001b[39;00m submit_results_df\u001b[38;5;241m.\u001b[39mjob_id\u001b[38;5;241m.\u001b[39mto_list()])\n\u001b[1;32m    346\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mmerge(job_status_df, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m,  right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount total jobs:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(job_status_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/code/icesat2_boreal/lib/ExtractUtils.py:345\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    342\u001b[0m submit_results_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m#job_status_df = pd.concat([pd.DataFrame({'job_id': [job_id], 'status':[maap.getJobStatus(job_id)]}) for job_id in submit_results_df.job_id.to_list()])\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [job_id], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[43msubmit_result\u001b[49m\u001b[38;5;241m.\u001b[39mstatus]}) \u001b[38;5;28;01mfor\u001b[39;00m job_id \u001b[38;5;129;01min\u001b[39;00m submit_results_df\u001b[38;5;241m.\u001b[39mjob_id\u001b[38;5;241m.\u001b[39mto_list()])\n\u001b[1;32m    346\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mmerge(job_status_df, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m,  right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount total jobs:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(job_status_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'submit_result' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "running_list = []\n",
    "fails_list = []\n",
    "success_list = []\n",
    "for DPS_DATETIME in [nowtime]:\n",
    "    for fn in LIST_SUBMISSIONS:\n",
    "        if DPS_DATETIME in fn and not 'job_status' in fn:\n",
    "            \n",
    "            DPS_alg_id = os.path.basename(fn.split('_submission_results_')[0].replace('DPS_',''))\n",
    "            thentime = fn.split('_')[-1].replace('.csv','')\n",
    "            print(f'DPS alg:\\t\\t{DPS_alg_id}')\n",
    "            print(f'DPS run name:\\t\\t{RUN_NAME}')\n",
    "            print(f'DPS launch time:\\t{thentime}')\n",
    "            \n",
    "            # Build job status table\n",
    "            df_jstatus = ExtractUtils.BUILD_TABLE_JOBSTATUS(pd.read_csv(fn))\n",
    "            \n",
    "            # Save job status table\n",
    "            df_jstatus.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{RUN_NAME}_submission_results_job_status_{len(df_jstatus)}_{thentime}.csv')\n",
    "\n",
    "            # Get current fails df and append to list\n",
    "            df_jstatus['run_type'] = RUN_NAME\n",
    "\n",
    "            running_list.append(df_jstatus[ (df_jstatus['status'] == 'Running') ] )\n",
    "            fails_list.append(  df_jstatus[ (df_jstatus['status'] == 'Failed') ] )\n",
    "            success_list.append(df_jstatus[ (df_jstatus['status'] == 'Succeeded') ] )\n",
    "            \n",
    "df_all_running = pd.concat(running_list)          \n",
    "df_all_fails =   pd.concat(fails_list)\n",
    "df_all_success = pd.concat(success_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4a3c91b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPS alg:\t\trun_build_stack_LC_build_stack_LC\n",
      "DPS launch time:\t202312061353\n"
     ]
    },
    {
     "ename": "InvalidURL",
     "evalue": "Invalid URL 'https:///api/dps/job/9c7f52e7-3033-4c0c-bd30-8372b330a11d/status': No host supplied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:9\u001b[0m\n",
      "File \u001b[0;32m~/code/icesat2_boreal/lib/ExtractUtils.py:344\u001b[0m, in \u001b[0;36mBUILD_TABLE_JOBSTATUS\u001b[0;34m(submit_results_df, status_col)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# If jobs failed to submit, then they have a NaN for jobid, which makes the merge (join) fail\u001b[39;00m\n\u001b[1;32m    342\u001b[0m submit_results_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 344\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [job_id], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m:[maap\u001b[38;5;241m.\u001b[39mgetJobStatus(job_id)]}) \u001b[38;5;28;01mfor\u001b[39;00m job_id \u001b[38;5;129;01min\u001b[39;00m submit_results_df\u001b[38;5;241m.\u001b[39mjob_id\u001b[38;5;241m.\u001b[39mto_list()])\n\u001b[1;32m    345\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mmerge(job_status_df, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m,  right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount total jobs:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(job_status_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/code/icesat2_boreal/lib/ExtractUtils.py:344\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# If jobs failed to submit, then they have a NaN for jobid, which makes the merge (join) fail\u001b[39;00m\n\u001b[1;32m    342\u001b[0m submit_results_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 344\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [job_id], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[43mmaap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetJobStatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m)\u001b[49m]}) \u001b[38;5;28;01mfor\u001b[39;00m job_id \u001b[38;5;129;01min\u001b[39;00m submit_results_df\u001b[38;5;241m.\u001b[39mjob_id\u001b[38;5;241m.\u001b[39mto_list()])\n\u001b[1;32m    345\u001b[0m job_status_df \u001b[38;5;241m=\u001b[39m submit_results_df\u001b[38;5;241m.\u001b[39mmerge(job_status_df, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m,  right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount total jobs:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(job_status_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/maap/maap.py:314\u001b[0m, in \u001b[0;36mMAAP.getJobStatus\u001b[0;34m(self, jobid)\u001b[0m\n\u001b[1;32m    312\u001b[0m job \u001b[38;5;241m=\u001b[39m DPSJob()\n\u001b[1;32m    313\u001b[0m job\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m jobid\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/maap/dps/dps_job.py:71\u001b[0m, in \u001b[0;36mDPSJob.retrieve_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     70\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(headers)\n\u001b[0;32m---> 71\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__not_self_signed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_job_status_result(RequestsUtils\u001b[38;5;241m.\u001b[39mcheck_response(response))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[1;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[1;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[1;32m    581\u001b[0m )\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/sessions.py:486\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    483\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    485\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[0;32m--> 486\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCaseInsensitiveDict\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_method(method)\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[0;32m~/env/above/lib/python3.9/site-packages/requests/models.py:445\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No scheme supplied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No host supplied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# In general, we want to try IDNA encoding the hostname if the string contains\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# non-ASCII characters. This allows users to automatically get the correct IDNA\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# behaviour. For strings containing only ASCII characters, we need to also verify\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# it doesn't start with a wildcard (*), before allowing the unencoded hostname.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unicode_is_ascii(host):\n",
      "\u001b[0;31mInvalidURL\u001b[0m: Invalid URL 'https:///api/dps/job/9c7f52e7-3033-4c0c-bd30-8372b330a11d/status': No host supplied"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LIST_SUBMISSIONS = sorted(glob.glob(f'/projects/my-public-bucket/dps_submission_results/DPS_{ALGO_ID}_{RUN_NAME}_submission_results_*.csv'),key=ExtractUtils.func, reverse=True)\n",
    "for DPS_DATETIME in [nowtime]:\n",
    "    for fn in LIST_SUBMISSIONS:\n",
    "        if DPS_DATETIME in fn and not 'job_status' in fn:\n",
    "            DPS_alg_id = os.path.basename(fn.split('_submission_results_')[0].replace('DPS_',''))\n",
    "            thentime = fn.split('_')[-1].replace('.csv','')\n",
    "            print(f'DPS alg:\\t\\t{DPS_alg_id}')\n",
    "            print(f'DPS launch time:\\t{thentime}')\n",
    "            z = ExtractUtils.BUILD_TABLE_JOBSTATUS(pd.read_csv(fn))\n",
    "            # Save job status table\n",
    "            z.to_csv(f'/projects/my-public-bucket/dps_submission_results/DPS_{IDENTIFIER}_submission_results_job_status_{len(z)}_{thentime}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fb9ff4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24389]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[z['wps:Status'] =='Succeeded'].tile_num.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73b24071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wps:Result': {'@xmlns:ows': 'http://www.opengis.net/ows/2.0',\n",
       "  '@xmlns:schemaLocation': 'http://schemas.opengis.net/wps/2.0/wps.xsd',\n",
       "  '@xmlns:wps': 'http://www.opengis.net/wps/2.0',\n",
       "  '@xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
       "  'wps:JobID': '20d88b45-eb25-48e6-b6f0-477ca59f3bb6',\n",
       "  'wps:Output': {'@id': 'traceback',\n",
       "   'wps:Data': \"+ export HOME=/root\\n+ HOME=/root\\n+ exec /docker-stats-on-exit-shim _docker_stats.json /app/dps_wrapper.sh /app/icesat2_boreal/dps/alg_3-1-3/run.sh footprints_v100_2020_v100_2020_map-s3.gpkg boreal_tiles_v003.gpkg tile_num 2223 0 boreal_tiles_v003 esa_worldcover_v100_2020 s3_path 0 3000\\n+ /app/icesat2_boreal/dps/alg_3-1-3/run.sh footprints_v100_2020_v100_2020_map-s3.gpkg boreal_tiles_v003.gpkg tile_num 2223 0 boreal_tiles_v003 esa_worldcover_v100_2020 s3_path 0 3000\\n+ unset PROJ_LIB\\n+ mkdir output\\n+++ dirname /app/icesat2_boreal/dps/alg_3-1-3/run.sh\\n++ cd /app/icesat2_boreal/dps/alg_3-1-3\\n++ pwd -P\\n+ basedir=/app/icesat2_boreal/dps/alg_3-1-3\\n+ FILENAMELIST=($(ls -d input/*))\\n++ ls -d input/boreal_tiles_v003.gpkg input/footprints_v100_2020_v100_2020_map-s3.gpkg\\n+ COVAR_TILE_FN=/data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/input/footprints_v100_2020_v100_2020_map-s3.gpkg\\n+ IN_TILE_FN=/data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/input/boreal_tiles_v003.gpkg\\n+ OUTPUTDIR=/data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/output\\n+ python /app/icesat2_boreal/dps/alg_3-1-3/../../lib/build_stack.py --covar_tile_fn /data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/input/footprints_v100_2020_v100_2020_map-s3.gpkg --in_tile_fn /data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/input/boreal_tiles_v003.gpkg --in_tile_id_col tile_num --in_tile_num 2223 --tile_buffer_m 0 --in_tile_layer boreal_tiles_v003 --output_dir /data/work/jobs/2022/12/18/01/06/job-run_build_stack_ubuntu__master-20221218T010032.68527Z/output --topo_off --covar_src_name esa_worldcover_v100_2020 --in_covar_s3_col s3_path --input_nodata_value 0 --shape footprints_v100_2020_v100_2020_map-s3.gpkg0 --clip\\nERROR 1: PROJ: proj_create_from_database: Open of /opt/conda/envs/icesat2_boreal/share/proj failed\\nusage: build_stack.py [-h] [-i IN_TILE_FN] [-n IN_TILE_NUM] [-b TILE_BUFFER_M]\\n                      [--in_tile_id_col IN_TILE_ID_COL] [-l IN_TILE_LAYER]\\n                      [-o OUTPUT_DIR] [-r RES] [--shape SHAPE]\\n                      [-covar COVAR_TILE_FN]\\n                      [--input_nodata_value INPUT_NODATA_VALUE]\\n                      [--in_covar_s3_col IN_COVAR_S3_COL] [-tmp TMP_OUT_PATH]\\n                      [-name COVAR_SRC_NAME] [--topo_off] [--clip]\\nbuild_stack.py: error: argument --shape: invalid int value: 'footprints_v100_2020_v100_2020_map-s3.gpkg0'\\n+ cp _stderr.txt _alt_traceback.txt\"}}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmltodict.parse(maap.getJobResult(z[z['wps:Status'] =='Failed'].iloc[43].job_id).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec1852",
   "metadata": {},
   "source": [
    "## Update the DPS input tiles list with only the tiles that failed - then run the DPS submit chunk above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ab7ab0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2917, 980, 25175, 25740, 2981, 3879]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPS_INPUT_TILE_NUM_LIST = z[z['wps:Status'] =='Failed'].tile_num.to_list()\n",
    "len(DPS_INPUT_TILE_NUM_LIST)\n",
    "DPS_INPUT_TILE_NUM_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d1e0fd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STRAGGLER_TILE_LIST = [3269, 3088, 2917, 2513, 2228, 1554, 1274, 1200, 620, 980, 790, 23828, 24389, 24108, 23501, 23830, 108, 25175, 25458, 25740, 163, 28967, 1172, 1431, 2981, 3531, 4463, 3879, 4207, 4445, 2888, 3790]\n",
    "DPS_INPUT_TILE_NUM_LIST = STRAGGLER_TILE_LIST\n",
    "len(DPS_INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0952e739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINISHED_LIST = z[z['wps:Status'] =='Succeeded'].tile_num.to_list()\n",
    "DPS_INPUT_TILE_NUM_LIST = set(STRAGGLER_TILE_LIST) - set(FINISHED_LIST)\n",
    "len(DPS_INPUT_TILE_NUM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "868245e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRAGGLER_TILE_LIST2 = [3855,24389]\n",
    "DPS_INPUT_TILE_NUM_LIST = STRAGGLER_TILE_LIST2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "/projects/env/above",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
